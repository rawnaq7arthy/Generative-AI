{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fq6vRCJtoJeE",
        "outputId": "a8f194bb-86d3-4088-a1d2-480e1755ee18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.97.1)\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Collecting pdfminer.six==20250506 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.3.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20250506 pdfplumber-0.11.7 pypdfium2-4.30.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai pdfplumber tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "nChZJqV_oD9i",
        "outputId": "67d6b4d6-e8b6-44df-d32f-d684e4e6bcf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2-4166939742.py:871: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://05ae9dfcde93cbc1f0.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://05ae9dfcde93cbc1f0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import openai\n",
        "import pdfplumber\n",
        "import re\n",
        "import numpy as np\n",
        "import json\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "# Configuration\n",
        "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
        "CHAT_MODEL = \"gpt-3.5-turbo\"\n",
        "\n",
        "# Set your OpenAI API key here\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "# Global variables to store data\n",
        "current_pdf_data = {\n",
        "    \"pages\": [],\n",
        "    \"embedded_chunks\": [],\n",
        "    \"mcqs\": [],\n",
        "    \"summary\": \"\",\n",
        "    \"full_text\": \"\",\n",
        "    \"translations\": {}\n",
        "}\n",
        "\n",
        "# Available languages for translation\n",
        "LANGUAGES = {\n",
        "    \"Arabic\": \"العربية\",\n",
        "    \"Spanish\": \"Español\",\n",
        "    \"French\": \"Français\",\n",
        "    \"German\": \"Deutsch\",\n",
        "    \"Italian\": \"Italiano\",\n",
        "    \"Portuguese\": \"Português\",\n",
        "    \"Russian\": \"Русский\",\n",
        "    \"Chinese\": \"中文\",\n",
        "    \"Japanese\": \"日本語\",\n",
        "    \"Korean\": \"한국어\",\n",
        "    \"Hindi\": \"हिन्दी\",\n",
        "    \"Urdu\": \"اردو\",\n",
        "    \"Turkish\": \"Türkçe\",\n",
        "    \"Dutch\": \"Nederlands\",\n",
        "    \"Swedish\": \"Svenska\"\n",
        "}\n",
        "\n",
        "def extract_text_by_page(file_path: str) -> List[Dict]:\n",
        "    \"\"\"Extract text from PDF by page\"\"\"\n",
        "    pages = []\n",
        "    try:\n",
        "        with pdfplumber.open(file_path) as pdf:\n",
        "            for i, page in enumerate(pdf.pages):\n",
        "                text = page.extract_text()\n",
        "                if text:\n",
        "                    pages.append({\"page\": i + 1, \"text\": text})\n",
        "        return pages\n",
        "    except Exception as e:\n",
        "        raise gr.Error(f\"Error extracting PDF: {str(e)}\")\n",
        "\n",
        "def summarize_text(text: str) -> str:\n",
        "    \"\"\"Generate summary of the document\"\"\"\n",
        "    try:\n",
        "        response = openai.chat.completions.create(\n",
        "            model=CHAT_MODEL,\n",
        "            messages=[{\"role\": \"user\", \"content\": f\"Summarize this document:\\n\\n{text[:4000]}\"}],\n",
        "            max_tokens=500,\n",
        "            temperature=0.5\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        raise gr.Error(f\"Error generating summary: {str(e)}\")\n",
        "\n",
        "def translate_text(text: str, target_language: str = \"Arabic\") -> str:\n",
        "    \"\"\"Translate text to target language\"\"\"\n",
        "    try:\n",
        "        # Limit text length for API efficiency\n",
        "        text_to_translate = text[:4000] if len(text) > 4000 else text\n",
        "\n",
        "        prompt = f\"\"\"Translate the following text into {target_language}.\n",
        "        Maintain the original formatting and structure as much as possible:\n",
        "\n",
        "        {text_to_translate}\"\"\"\n",
        "\n",
        "        response = openai.chat.completions.create(\n",
        "            model=CHAT_MODEL,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=1500,\n",
        "            temperature=0.3\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        raise gr.Error(f\"Error translating text: {str(e)}\")\n",
        "\n",
        "def translate_summary(summary: str, target_language: str) -> str:\n",
        "    \"\"\"Translate document summary\"\"\"\n",
        "    try:\n",
        "        prompt = f\"\"\"Translate this document summary into {target_language}:\n",
        "\n",
        "        {summary}\"\"\"\n",
        "\n",
        "        response = openai.chat.completions.create(\n",
        "            model=CHAT_MODEL,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=800,\n",
        "            temperature=0.3\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        raise gr.Error(f\"Error translating summary: {str(e)}\")\n",
        "\n",
        "def generate_mcqs(text: str, num_questions: int = 5) -> str:\n",
        "    \"\"\"Generate MCQ questions from text\"\"\"\n",
        "    try:\n",
        "        prompt = f\"\"\"\n",
        "Create {num_questions} multiple choice questions from the following document.\n",
        "Each question should have 4 options (A–D) and one correct answer.\n",
        "Use exactly this format:\n",
        "\n",
        "Q1: What is the capital of France?\n",
        "A. Berlin\n",
        "B. Madrid\n",
        "C. Paris\n",
        "D. Rome\n",
        "Answer: C\n",
        "\n",
        "Content:\n",
        "{text[:3000]}\n",
        "\"\"\"\n",
        "        response = openai.chat.completions.create(\n",
        "            model=CHAT_MODEL,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=800,\n",
        "            temperature=0.5\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        raise gr.Error(f\"Error generating MCQs: {str(e)}\")\n",
        "\n",
        "def parse_mcqs(raw_text: str) -> List[Dict]:\n",
        "    \"\"\"Parse MCQ text into structured format\"\"\"\n",
        "    pattern = re.compile(\n",
        "        r\"Q\\d+: (.*?)\\nA\\. (.*?)\\nB\\. (.*?)\\nC\\. (.*?)\\nD\\. (.*?)\\nAnswer: ([A-D])\",\n",
        "        re.DOTALL\n",
        "    )\n",
        "    matches = pattern.findall(raw_text)\n",
        "    questions = []\n",
        "    for match in matches:\n",
        "        question, a, b, c, d, answer = match\n",
        "        questions.append({\n",
        "            \"question\": question.strip(),\n",
        "            \"options\": {\n",
        "                \"A\": a.strip(),\n",
        "                \"B\": b.strip(),\n",
        "                \"C\": c.strip(),\n",
        "                \"D\": d.strip(),\n",
        "            },\n",
        "            \"answer\": answer.strip()\n",
        "        })\n",
        "    return questions\n",
        "\n",
        "def embed_chunks(pages: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"Create embeddings for PDF chunks\"\"\"\n",
        "    try:\n",
        "        chunks = []\n",
        "        for chunk in pages:\n",
        "            embedding = openai.embeddings.create(\n",
        "                model=EMBEDDING_MODEL,\n",
        "                input=chunk[\"text\"]\n",
        "            ).data[0].embedding\n",
        "            chunks.append({\n",
        "                \"page\": chunk[\"page\"],\n",
        "                \"text\": chunk[\"text\"],\n",
        "                \"embedding\": embedding\n",
        "            })\n",
        "        return chunks\n",
        "    except Exception as e:\n",
        "        raise gr.Error(f\"Error creating embeddings: {str(e)}\")\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
        "    a, b = np.array(a), np.array(b)\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "def retrieve_relevant_chunks(query: str, chunks: List[Dict], top_k: int = 3) -> List[Dict]:\n",
        "    \"\"\"Retrieve most relevant chunks for a query\"\"\"\n",
        "    try:\n",
        "        query_embedding = openai.embeddings.create(\n",
        "            model=EMBEDDING_MODEL,\n",
        "            input=query\n",
        "        ).data[0].embedding\n",
        "\n",
        "        ranked = sorted(\n",
        "            chunks,\n",
        "            key=lambda x: cosine_similarity(query_embedding, x[\"embedding\"]),\n",
        "            reverse=True\n",
        "        )\n",
        "        return ranked[:top_k]\n",
        "    except Exception as e:\n",
        "        raise gr.Error(f\"Error retrieving chunks: {str(e)}\")\n",
        "\n",
        "def process_pdf(file, num_questions: int) -> Tuple[str, str]:\n",
        "    \"\"\"Process uploaded PDF file\"\"\"\n",
        "    global current_pdf_data\n",
        "\n",
        "    if not file:\n",
        "        return \"❌ Please upload a PDF file\", \"\"\n",
        "\n",
        "    try:\n",
        "        # Extract text from PDF\n",
        "        pages = extract_text_by_page(file.name)\n",
        "        full_text = \"\\n\\n\".join([p['text'] for p in pages])\n",
        "\n",
        "        # Generate summary\n",
        "        summary = summarize_text(full_text)\n",
        "\n",
        "        # Generate MCQs\n",
        "        mcq_text = generate_mcqs(full_text, num_questions)\n",
        "        mcqs = parse_mcqs(mcq_text)\n",
        "\n",
        "        # Create embeddings\n",
        "        embedded_chunks = embed_chunks(pages)\n",
        "\n",
        "        # Store in global variable\n",
        "        current_pdf_data = {\n",
        "            \"pages\": pages,\n",
        "            \"embedded_chunks\": embedded_chunks,\n",
        "            \"mcqs\": mcqs,\n",
        "            \"summary\": summary,\n",
        "            \"full_text\": full_text,\n",
        "            \"translations\": {}\n",
        "        }\n",
        "\n",
        "        status = f\"✅ Successfully processed PDF with {len(pages)} pages and generated {len(mcqs)} questions\"\n",
        "\n",
        "        return status, \"PDF processed successfully! Choose an option below:\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error processing PDF: {str(e)}\", \"\"\n",
        "\n",
        "def show_summary():\n",
        "    \"\"\"Display document summary\"\"\"\n",
        "    global current_pdf_data\n",
        "    if not current_pdf_data[\"summary\"]:\n",
        "        return \"❌ Please upload and process a PDF first.\"\n",
        "    return f\"📄 **Document Summary:**\\n\\n{current_pdf_data['summary']}\"\n",
        "\n",
        "def show_mcqs():\n",
        "    \"\"\"Display interactive MCQ questions\"\"\"\n",
        "    global current_pdf_data\n",
        "    if not current_pdf_data[\"mcqs\"]:\n",
        "        return \"❌ Please upload and process a PDF first.\"\n",
        "\n",
        "    # Start with first question\n",
        "    q = current_pdf_data[\"mcqs\"][0]\n",
        "    mcq_display = f\"📝 **Question 1 of {len(current_pdf_data['mcqs'])}**\\n\\n**{q['question']}**\"\n",
        "\n",
        "    return mcq_display\n",
        "\n",
        "def show_translation(target_language: str) -> str:\n",
        "    \"\"\"Display translated document\"\"\"\n",
        "    global current_pdf_data\n",
        "\n",
        "    if not current_pdf_data[\"full_text\"]:\n",
        "        return \"❌ Please upload and process a PDF first.\"\n",
        "\n",
        "    # Check if translation already exists\n",
        "    if target_language in current_pdf_data[\"translations\"]:\n",
        "        return f\"🌍 **Document Translation ({target_language}):**\\n\\n{current_pdf_data['translations'][target_language]}\"\n",
        "\n",
        "    # Generate new translation\n",
        "    try:\n",
        "        translated_text = translate_text(current_pdf_data[\"full_text\"], target_language)\n",
        "        current_pdf_data[\"translations\"][target_language] = translated_text\n",
        "        return f\"🌍 **Document Translation ({target_language}):**\\n\\n{translated_text}\"\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error translating document: {str(e)}\"\n",
        "\n",
        "def show_translated_summary(target_language: str) -> str:\n",
        "    \"\"\"Display translated summary\"\"\"\n",
        "    global current_pdf_data\n",
        "\n",
        "    if not current_pdf_data[\"summary\"]:\n",
        "        return \"❌ Please upload and process a PDF first.\"\n",
        "\n",
        "    try:\n",
        "        translated_summary = translate_summary(current_pdf_data[\"summary\"], target_language)\n",
        "        return f\"🌍 **Translated Summary ({target_language}):**\\n\\n{translated_summary}\"\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error translating summary: {str(e)}\"\n",
        "\n",
        "def start_chatbot():\n",
        "    \"\"\"Initialize chatbot\"\"\"\n",
        "    global current_pdf_data\n",
        "    if not current_pdf_data[\"embedded_chunks\"]:\n",
        "        return \"❌ Please upload and process a PDF first.\", []\n",
        "    return \"🤖 **Chatbot Ready!** You can now access Summary, Questions, and Translation while chatting.\", []\n",
        "\n",
        "def chat_with_pdf(message: str, history: List[Tuple[str, str]]) -> Tuple[List[Tuple[str, str]], str]:\n",
        "    \"\"\"Chat with the PDF using RAG\"\"\"\n",
        "    global current_pdf_data\n",
        "\n",
        "    if not current_pdf_data[\"embedded_chunks\"]:\n",
        "        history.append((message, \"❌ Please upload and process a PDF first.\"))\n",
        "        return history, \"\"\n",
        "\n",
        "    try:\n",
        "        # Retrieve relevant chunks\n",
        "        top_chunks = retrieve_relevant_chunks(message, current_pdf_data[\"embedded_chunks\"])\n",
        "        context = \"\\n\\n\".join([f\"(Page {c['page']}):\\n{c['text']}\" for c in top_chunks])\n",
        "\n",
        "        # Generate response\n",
        "        prompt = f\"\"\"You are a helpful assistant. Use the following PDF content to answer the user's question.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {message}\n",
        "Answer:\"\"\"\n",
        "\n",
        "        response = openai.chat.completions.create(\n",
        "            model=CHAT_MODEL,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=500,\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "        answer = response.choices[0].message.content.strip()\n",
        "        history.append((message, answer))\n",
        "\n",
        "        return history, \"\"\n",
        "\n",
        "    except Exception as e:\n",
        "        history.append((message, f\"❌ Error: {str(e)}\"))\n",
        "        return history, \"\"\n",
        "\n",
        "def check_mcq_answer(selected_option: str, current_idx: int) -> Tuple[str, str, str, str, str, str, int]:\n",
        "    \"\"\"Check MCQ answer and show next question\"\"\"\n",
        "    global current_pdf_data\n",
        "\n",
        "    if not current_pdf_data[\"mcqs\"]:\n",
        "        return \"❌ No questions available\", \"\", \"\", \"\", \"\", \"\", 0\n",
        "\n",
        "    try:\n",
        "        total_questions = len(current_pdf_data[\"mcqs\"])\n",
        "\n",
        "        if current_idx >= total_questions:\n",
        "            return \"All questions completed!\", \"\", \"\", \"\", \"\", \"\", current_idx\n",
        "\n",
        "        # Check current answer\n",
        "        correct_answer = current_pdf_data[\"mcqs\"][current_idx][\"answer\"]\n",
        "        is_correct = selected_option == correct_answer\n",
        "\n",
        "        # Move to next question\n",
        "        next_idx = current_idx + 1\n",
        "\n",
        "        if next_idx >= total_questions:\n",
        "            # All questions completed\n",
        "            result = f\"{'✅ Correct!' if is_correct else f'❌ Wrong! Correct answer: {correct_answer}'}\\n\\n🎯 **All Questions Completed!**\\n\\nYou can now review the questions again or use other features.\"\n",
        "            return result, \"\", \"\", \"\", \"\", \"\", next_idx\n",
        "\n",
        "        # Show next question\n",
        "        q = current_pdf_data[\"mcqs\"][next_idx]\n",
        "        result = f\"{'✅ Correct!' if is_correct else f'❌ Wrong! Correct answer: {correct_answer}'}\\n\\n📝 **Question {next_idx + 1} of {total_questions}**\\n\\n**{q['question']}**\"\n",
        "\n",
        "        return (\n",
        "            result,\n",
        "            q['options']['A'],\n",
        "            q['options']['B'],\n",
        "            q['options']['C'],\n",
        "            q['options']['D'],\n",
        "            f\"Question {next_idx + 1} of {total_questions}\",\n",
        "            next_idx\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error: {str(e)}\", \"\", \"\", \"\", \"\", \"\", 0\n",
        "\n",
        "def reset_mcq():\n",
        "    \"\"\"Reset MCQ to first question\"\"\"\n",
        "    global current_pdf_data\n",
        "    if not current_pdf_data[\"mcqs\"]:\n",
        "        return \"❌ No questions available\", \"\", \"\", \"\", \"\", \"\", 0\n",
        "\n",
        "    q = current_pdf_data[\"mcqs\"][0]\n",
        "    return (\n",
        "        f\"📝 **Question 1 of {len(current_pdf_data['mcqs'])}**\\n\\n**{q['question']}**\",\n",
        "        q['options']['A'],\n",
        "        q['options']['B'],\n",
        "        q['options']['C'],\n",
        "        q['options']['D'],\n",
        "        f\"Question 1 of {len(current_pdf_data['mcqs'])}\",\n",
        "        0\n",
        "    )\n",
        "\n",
        "def check_answer(selected_option: str, current_idx: int, current_score: int) -> Tuple[str, str, str, str, str, str, int, int]:\n",
        "    \"\"\"Check answer and move to next question\"\"\"\n",
        "    global current_pdf_data\n",
        "\n",
        "    if not current_pdf_data[\"mcqs\"]:\n",
        "        return \"❌ No quiz available\", \"\", \"\", \"\", \"\", \"0/0\", 0, 0\n",
        "\n",
        "    try:\n",
        "        total_questions = len(current_pdf_data[\"mcqs\"])\n",
        "\n",
        "        if current_idx >= total_questions:\n",
        "            return \"Quiz completed!\", \"\", \"\", \"\", \"\", f\"Final Score: {current_score}/{total_questions}\", current_idx, current_score\n",
        "\n",
        "        # Check current answer\n",
        "        correct_answer = current_pdf_data[\"mcqs\"][current_idx][\"answer\"]\n",
        "        is_correct = selected_option == correct_answer\n",
        "        new_score = current_score + (1 if is_correct else 0)\n",
        "\n",
        "        # Move to next question\n",
        "        next_idx = current_idx + 1\n",
        "\n",
        "        if next_idx >= total_questions:\n",
        "            # Quiz finished\n",
        "            result = f\"{'✅ Correct!' if is_correct else f'❌ Wrong! Correct answer: {correct_answer}'}\\n\\n🎯 **Quiz Completed!**\\n\\n**Final Score: {new_score}/{total_questions}**\"\n",
        "            if new_score == total_questions:\n",
        "                result += \"\\n\\n🏆 **Perfect Score! Excellent work!**\"\n",
        "            elif new_score >= total_questions * 0.8:\n",
        "                result += \"\\n\\n🌟 **Great job! You did very well!**\"\n",
        "            elif new_score >= total_questions * 0.6:\n",
        "                result += \"\\n\\n👍 **Good effort! Keep learning!**\"\n",
        "            else:\n",
        "                result += \"\\n\\n📚 **Keep studying and try again!**\"\n",
        "\n",
        "            return result, \"\", \"\", \"\", \"\", f\"Final Score: {new_score}/{total_questions}\", next_idx, new_score\n",
        "\n",
        "        # Show next question\n",
        "        q = current_pdf_data[\"mcqs\"][next_idx]\n",
        "        result = f\"{'✅ Correct!' if is_correct else f'❌ Wrong! Correct answer: {correct_answer}'}\\n\\n🎯 **Question {next_idx + 1} of {total_questions}**\\n\\n**{q['question']}**\"\n",
        "\n",
        "        return (\n",
        "            result,\n",
        "            q['options']['A'],\n",
        "            q['options']['B'],\n",
        "            q['options']['C'],\n",
        "            q['options']['D'],\n",
        "            f\"Score: {new_score}/{current_idx + 1} | Progress: {next_idx + 1}/{total_questions}\",\n",
        "            next_idx,\n",
        "            new_score\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error: {str(e)}\", \"\", \"\", \"\", \"\", \"0/0\", 0, 0\n",
        "\n",
        "# Enhanced CSS with better styling\n",
        "custom_css = \"\"\"\n",
        "/* Global styling */\n",
        ".gradio-container {\n",
        "    font-family: 'Segoe UI', 'Inter', Tahoma, sans-serif !important;\n",
        "    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%) !important;\n",
        "    min-height: 100vh;\n",
        "}\n",
        "\n",
        "/* Header styling */\n",
        ".main-header {\n",
        "    text-align: center;\n",
        "    padding: 3rem;\n",
        "    background: rgba(255, 255, 255, 0.15);\n",
        "    backdrop-filter: blur(20px);\n",
        "    border-radius: 25px;\n",
        "    margin: 2rem 0;\n",
        "    border: 1px solid rgba(255, 255, 255, 0.3);\n",
        "    box-shadow: 0 20px 40px rgba(0,0,0,0.1);\n",
        "}\n",
        "\n",
        ".main-header h1 {\n",
        "    color: white;\n",
        "    font-size: 3.5rem;\n",
        "    font-weight: 800;\n",
        "    margin-bottom: 1rem;\n",
        "    text-shadow: 2px 2px 8px rgba(0,0,0,0.3);\n",
        "    background: linear-gradient(45deg, #fff, #f0f0f0);\n",
        "    -webkit-background-clip: text;\n",
        "    -webkit-text-fill-color: transparent;\n",
        "}\n",
        "\n",
        ".main-header p {\n",
        "    color: rgba(255, 255, 255, 0.95);\n",
        "    font-size: 1.3rem;\n",
        "    font-weight: 400;\n",
        "    text-shadow: 1px 1px 2px rgba(0,0,0,0.2);\n",
        "}\n",
        "\n",
        "/* Card containers */\n",
        ".upload-card, .action-card, .content-card, .translation-card {\n",
        "    background: rgba(255, 255, 255, 0.98) !important;\n",
        "    backdrop-filter: blur(15px) !important;\n",
        "    border-radius: 20px !important;\n",
        "    border: 1px solid rgba(255, 255, 255, 0.4) !important;\n",
        "    box-shadow: 0 15px 35px rgba(0,0,0,0.1) !important;\n",
        "    padding: 2.5rem !important;\n",
        "    margin: 1.5rem 0 !important;\n",
        "    transition: all 0.3s ease !important;\n",
        "}\n",
        "\n",
        ".upload-card:hover, .action-card:hover, .content-card:hover, .translation-card:hover {\n",
        "    transform: translateY(-5px) !important;\n",
        "    box-shadow: 0 25px 50px rgba(0,0,0,0.15) !important;\n",
        "}\n",
        "\n",
        "/* Section headers */\n",
        ".section-header {\n",
        "    color: #667eea !important;\n",
        "    font-size: 1.8rem !important;\n",
        "    font-weight: 700 !important;\n",
        "    margin-bottom: 1.5rem !important;\n",
        "    text-align: center !important;\n",
        "    padding: 1rem !important;\n",
        "    background: linear-gradient(45deg, #667eea, #764ba2) !important;\n",
        "    -webkit-background-clip: text !important;\n",
        "    -webkit-text-fill-color: transparent !important;\n",
        "}\n",
        "\n",
        "/* Primary buttons */\n",
        ".primary-btn {\n",
        "    background: linear-gradient(45deg, #ff6b6b, #feca57) !important;\n",
        "    color: white !important;\n",
        "    border: none !important;\n",
        "    border-radius: 15px !important;\n",
        "    padding: 15px 40px !important;\n",
        "    font-size: 1.2rem !important;\n",
        "    font-weight: 700 !important;\n",
        "    box-shadow: 0 8px 25px rgba(255, 107, 107, 0.3) !important;\n",
        "    transition: all 0.3s ease !important;\n",
        "    text-transform: uppercase !important;\n",
        "    letter-spacing: 1px !important;\n",
        "}\n",
        "\n",
        ".primary-btn:hover {\n",
        "    transform: translateY(-3px) !important;\n",
        "    box-shadow: 0 12px 35px rgba(255, 107, 107, 0.4) !important;\n",
        "}\n",
        "\n",
        "/* Action buttons */\n",
        ".action-btn {\n",
        "    background: linear-gradient(45deg, #667eea, #764ba2) !important;\n",
        "    color: white !important;\n",
        "    border: none !important;\n",
        "    border-radius: 12px !important;\n",
        "    padding: 12px 30px !important;\n",
        "    font-size: 1.1rem !important;\n",
        "    font-weight: 600 !important;\n",
        "    margin: 8px !important;\n",
        "    min-width: 200px !important;\n",
        "    box-shadow: 0 6px 20px rgba(102, 126, 234, 0.3) !important;\n",
        "    transition: all 0.3s ease !important;\n",
        "}\n",
        "\n",
        ".action-btn:hover {\n",
        "    transform: translateY(-2px) !important;\n",
        "    box-shadow: 0 10px 30px rgba(102, 126, 234, 0.4) !important;\n",
        "}\n",
        "\n",
        "/* Translation buttons */\n",
        ".translate-btn {\n",
        "    background: linear-gradient(45deg, #26d0ce, #1a2980) !important;\n",
        "    color: white !important;\n",
        "    border: none !important;\n",
        "    border-radius: 12px !important;\n",
        "    padding: 12px 30px !important;\n",
        "    font-size: 1.1rem !important;\n",
        "    font-weight: 600 !important;\n",
        "    margin: 8px !important;\n",
        "    min-width: 200px !important;\n",
        "    box-shadow: 0 6px 20px rgba(38, 208, 206, 0.3) !important;\n",
        "    transition: all 0.3s ease !important;\n",
        "}\n",
        "\n",
        ".translate-btn:hover {\n",
        "    transform: translateY(-2px) !important;\n",
        "    box-shadow: 0 10px 30px rgba(38, 208, 206, 0.4) !important;\n",
        "}\n",
        "\n",
        "/* Quiz buttons */\n",
        ".quiz-btn {\n",
        "    background: linear-gradient(45deg, #4ecdc4, #44a08d) !important;\n",
        "    color: white !important;\n",
        "    border: none !important;\n",
        "    border-radius: 15px !important;\n",
        "    padding: 18px 30px !important;\n",
        "    font-size: 1.1rem !important;\n",
        "    font-weight: 600 !important;\n",
        "    margin: 8px !important;\n",
        "    min-width: 140px !important;\n",
        "    box-shadow: 0 8px 25px rgba(78, 205, 196, 0.3) !important;\n",
        "    transition: all 0.3s ease !important;\n",
        "    text-align: left !important;\n",
        "}\n",
        "\n",
        ".quiz-btn:hover {\n",
        "    transform: translateY(-3px) scale(1.02) !important;\n",
        "    box-shadow: 0 12px 35px rgba(78, 205, 196, 0.4) !important;\n",
        "}\n",
        "\n",
        "/* Input styling */\n",
        ".gr-textbox, .gr-file, .gr-slider, .gr-dropdown {\n",
        "    border: 2px solid rgba(102, 126, 234, 0.2) !important;\n",
        "    border-radius: 15px !important;\n",
        "    padding: 15px !important;\n",
        "    font-size: 1.1rem !important;\n",
        "    transition: all 0.3s ease !important;\n",
        "    background: rgba(255, 255, 255, 0.9) !important;\n",
        "}\n",
        "\n",
        ".gr-textbox:focus, .gr-file:focus, .gr-dropdown:focus {\n",
        "    border-color: #667eea !important;\n",
        "    box-shadow: 0 0 0 4px rgba(102, 126, 234, 0.1) !important;\n",
        "    transform: translateY(-2px) !important;\n",
        "}\n",
        "\n",
        "/* Content display */\n",
        ".content-display {\n",
        "    background: rgba(255, 255, 255, 0.95) !important;\n",
        "    border-radius: 15px !important;\n",
        "    padding: 2rem !important;\n",
        "    border: 2px solid rgba(102, 126, 234, 0.1) !important;\n",
        "    box-shadow: inset 0 2px 10px rgba(0,0,0,0.05) !important;\n",
        "    line-height: 1.6 !important;\n",
        "    font-size: 1.05rem !important;\n",
        "}\n",
        "\n",
        "/* Chatbot styling */\n",
        ".chatbot-container {\n",
        "    border-radius: 20px !important;\n",
        "    border: 2px solid rgba(102, 126, 234, 0.2) !important;\n",
        "    box-shadow: 0 10px 30px rgba(0,0,0,0.1) !important;\n",
        "    background: white !important;\n",
        "}\n",
        "\n",
        "/* Progress indicators */\n",
        ".progress-indicator {\n",
        "    background: linear-gradient(45deg, #667eea, #764ba2) !important;\n",
        "    color: white !important;\n",
        "    text-align: center !important;\n",
        "    padding: 12px 20px !important;\n",
        "    border-radius: 25px !important;\n",
        "    font-weight: 600 !important;\n",
        "    font-size: 1.1rem !important;\n",
        "    box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3) !important;\n",
        "}\n",
        "\n",
        "/* Status messages */\n",
        ".status-success {\n",
        "    color: #28a745 !important;\n",
        "    font-weight: 600 !important;\n",
        "    font-size: 1.1rem !important;\n",
        "}\n",
        "\n",
        ".status-error {\n",
        "    color: #dc3545 !important;\n",
        "    font-weight: 600 !important;\n",
        "    font-size: 1.1rem !important;\n",
        "}\n",
        "\n",
        "/* Animations */\n",
        "@keyframes fadeInUp {\n",
        "    from {\n",
        "        opacity: 0;\n",
        "        transform: translateY(30px);\n",
        "    }\n",
        "    to {\n",
        "        opacity: 1;\n",
        "        transform: translateY(0);\n",
        "    }\n",
        "}\n",
        "\n",
        ".fade-in {\n",
        "    animation: fadeInUp 0.6s ease-out;\n",
        "}\n",
        "\n",
        "/* Responsive design */\n",
        "@media (max-width: 768px) {\n",
        "    .main-header h1 {\n",
        "        font-size: 2.5rem !important;\n",
        "    }\n",
        "\n",
        "    .main-header p {\n",
        "        font-size: 1.1rem !important;\n",
        "    }\n",
        "\n",
        "    .upload-card, .action-card, .content-card, .translation-card {\n",
        "        padding: 1.5rem !important;\n",
        "        margin: 1rem 0 !important;\n",
        "    }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Create the enhanced Gradio interface\n",
        "with gr.Blocks(\n",
        "    title=\"PDF Learning Assistant with Translation\",\n",
        "    theme=gr.themes.Soft(\n",
        "        primary_hue=gr.themes.colors.blue,\n",
        "        secondary_hue=gr.themes.colors.purple,\n",
        "        neutral_hue=gr.themes.colors.slate,\n",
        "        font=[gr.themes.GoogleFont(\"Inter\"), \"Arial\", \"sans-serif\"]\n",
        "    ),\n",
        "    css=custom_css\n",
        ") as app:\n",
        "\n",
        "    # Beautiful enhanced header\n",
        "    gr.HTML(\"\"\"\n",
        "    <div class=\"main-header fade-in\">\n",
        "        <h1>🎓 PDF Learning Assistant</h1>\n",
        "        <p>Transform your documents into interactive learning experiences with AI-powered insights and translation</p>\n",
        "    </div>\n",
        "    \"\"\")\n",
        "\n",
        "    # Main upload section\n",
        "    with gr.Row():\n",
        "        with gr.Column(elem_classes=\"upload-card\"):\n",
        "            gr.HTML('<h2 class=\"section-header\">📁 Upload Your Document</h2>')\n",
        "\n",
        "            file_input = gr.File(\n",
        "                label=\"📄 Choose your PDF file\",\n",
        "                file_types=[\".pdf\"],\n",
        "                elem_classes=\"file-upload\"\n",
        "            )\n",
        "\n",
        "            num_questions = gr.Slider(\n",
        "                minimum=3,\n",
        "                maximum=15,\n",
        "                value=5,\n",
        "                step=1,\n",
        "                label=\"🎯 Number of Quiz Questions\",\n",
        "                info=\"Select how many questions to generate for your quiz\"\n",
        "            )\n",
        "\n",
        "            process_btn = gr.Button(\n",
        "                \"🚀 Process Document\",\n",
        "                variant=\"primary\",\n",
        "                elem_classes=\"primary-btn\",\n",
        "                size=\"lg\"\n",
        "            )\n",
        "\n",
        "            status_output = gr.Textbox(\n",
        "                label=\"📊 Processing Status\",\n",
        "                interactive=False,\n",
        "                placeholder=\"Upload a PDF to see processing status...\",\n",
        "                elem_classes=\"content-display\"\n",
        "            )\n",
        "\n",
        "    # Action buttons section\n",
        "    with gr.Row():\n",
        "        with gr.Column(elem_classes=\"action-card\"):\n",
        "            gr.HTML('<h2 class=\"section-header\">🎯 Choose Your Learning Mode</h2>')\n",
        "\n",
        "            result_display = gr.Markdown(\n",
        "                value=\"*Select an option after processing your PDF...*\",\n",
        "                elem_classes=\"content-display\"\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                summary_btn = gr.Button(\"📄 View Summary\", elem_classes=\"action-btn\", size=\"lg\")\n",
        "                mcq_btn = gr.Button(\"📝 Interactive Questions\", elem_classes=\"action-btn\", size=\"lg\")\n",
        "                translate_btn = gr.Button(\"🌍 Translation Center\", elem_classes=\"translate-btn\", size=\"lg\")\n",
        "                chat_btn = gr.Button(\"💬 Start Chatbot\", elem_classes=\"action-btn\", size=\"lg\")\n",
        "\n",
        "    # Translation section\n",
        "    with gr.Group(visible=False) as translation_section:\n",
        "        with gr.Row():\n",
        "            with gr.Column(elem_classes=\"translation-card\"):\n",
        "                gr.HTML('<h2 class=\"section-header\">🌍 Translation Center</h2>')\n",
        "\n",
        "                language_dropdown = gr.Dropdown(\n",
        "                    choices=list(LANGUAGES.keys()),\n",
        "                    value=\"Arabic\",\n",
        "                    label=\"🗣️ Select Target Language\",\n",
        "                    info=\"Choose the language you want to translate the document to\"\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    translate_full_btn = gr.Button(\"📄 Translate Full Document\", elem_classes=\"translate-btn\", size=\"lg\")\n",
        "                    translate_summary_btn = gr.Button(\"📋 Translate Summary\", elem_classes=\"translate-btn\", size=\"lg\")\n",
        "\n",
        "                translation_display = gr.Markdown(\n",
        "                    value=\"*Select a translation option above...*\",\n",
        "                    elem_classes=\"content-display\"\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    back_to_menu_trans_btn = gr.Button(\"⬅️ Back to Menu\", variant=\"secondary\", elem_classes=\"action-btn\")\n",
        "\n",
        "    # Interactive MCQ section\n",
        "    with gr.Group(visible=False) as mcq_section:\n",
        "        with gr.Row():\n",
        "            with gr.Column(elem_classes=\"content-card\"):\n",
        "                gr.HTML('<h2 class=\"section-header\">📝 Interactive Questions</h2>')\n",
        "\n",
        "                mcq_question_display = gr.Markdown(\n",
        "                    value=\"*Questions will appear here...*\",\n",
        "                    elem_classes=\"content-display\"\n",
        "                )\n",
        "\n",
        "                gr.HTML('<h3 style=\"color: #764ba2; margin: 1.5rem 0; text-align: center;\">Choose your answer:</h3>')\n",
        "\n",
        "                with gr.Row():\n",
        "                    mcq_option_a = gr.Button(\"A. Option A\", elem_classes=\"quiz-btn\", size=\"lg\")\n",
        "                    mcq_option_b = gr.Button(\"B. Option B\", elem_classes=\"quiz-btn\", size=\"lg\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    mcq_option_c = gr.Button(\"C. Option C\", elem_classes=\"quiz-btn\", size=\"lg\")\n",
        "                    mcq_option_d = gr.Button(\"D. Option D\", elem_classes=\"quiz-btn\", size=\"lg\")\n",
        "\n",
        "                mcq_progress = gr.Textbox(\n",
        "                    label=\"📈 Progress\",\n",
        "                    interactive=False,\n",
        "                    elem_classes=\"progress-indicator\"\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    reset_mcq_btn = gr.Button(\"🔄 Start Over\", variant=\"secondary\", elem_classes=\"action-btn\")\n",
        "                    back_to_menu_btn = gr.Button(\"⬅️ Back to Menu\", variant=\"secondary\", elem_classes=\"action-btn\")\n",
        "\n",
        "    # Quiz section (initially hidden)\n",
        "    with gr.Group(visible=False) as quiz_section:\n",
        "        with gr.Row():\n",
        "            with gr.Column(elem_classes=\"content-card\"):\n",
        "                gr.HTML('<h2 class=\"section-header\">🧠 Interactive Quiz</h2>')\n",
        "\n",
        "                start_quiz_btn = gr.Button(\n",
        "                    \"🎯 Start Quiz Challenge\",\n",
        "                    variant=\"primary\",\n",
        "                    elem_classes=\"primary-btn\",\n",
        "                    size=\"lg\"\n",
        "                )\n",
        "\n",
        "                question_display = gr.Markdown(\n",
        "                    value=\"*Click 'Start Quiz Challenge' to begin!*\",\n",
        "                    elem_classes=\"content-display\"\n",
        "                )\n",
        "\n",
        "                gr.HTML('<h3 style=\"color: #764ba2; margin: 1.5rem 0; text-align: center;\">Choose your answer:</h3>')\n",
        "\n",
        "                with gr.Row():\n",
        "                    option_a = gr.Button(\"A. Option A\", elem_classes=\"quiz-btn\", size=\"lg\")\n",
        "                    option_b = gr.Button(\"B. Option B\", elem_classes=\"quiz-btn\", size=\"lg\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    option_c = gr.Button(\"C. Option C\", elem_classes=\"quiz-btn\", size=\"lg\")\n",
        "                    option_d = gr.Button(\"D. Option D\", elem_classes=\"quiz-btn\", size=\"lg\")\n",
        "\n",
        "                quiz_progress = gr.Textbox(\n",
        "                    label=\"📈 Quiz Progress\",\n",
        "                    interactive=False,\n",
        "                    elem_classes=\"progress-indicator\"\n",
        "                )\n",
        "\n",
        "    # Chat section with additional controls\n",
        "    with gr.Group(visible=False) as chat_section:\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=3, elem_classes=\"content-card\"):\n",
        "                gr.HTML('<h2 class=\"section-header\">🤖 AI Document Assistant</h2>')\n",
        "\n",
        "                # Additional controls in chat mode\n",
        "                with gr.Row():\n",
        "                    chat_summary_btn = gr.Button(\"📄 Show Summary\", elem_classes=\"action-btn\", size=\"sm\")\n",
        "                    chat_mcq_btn = gr.Button(\"📝 Show Questions\", elem_classes=\"action-btn\", size=\"sm\")\n",
        "                    chat_translate_btn = gr.Button(\"🌍 Show Translation\", elem_classes=\"translate-btn\", size=\"sm\")\n",
        "                    back_to_main_btn = gr.Button(\"⬅️ Back to Menu\", variant=\"secondary\", elem_classes=\"action-btn\", size=\"sm\")\n",
        "\n",
        "                # Chat language selector\n",
        "                chat_language_dropdown = gr.Dropdown(\n",
        "                    choices=list(LANGUAGES.keys()),\n",
        "                    value=\"Arabic\",\n",
        "                    label=\"🗣️ Translation Language\",\n",
        "                    visible=False\n",
        "                )\n",
        "\n",
        "                chatbot = gr.Chatbot(\n",
        "                    height=400,\n",
        "                    placeholder=\"Your conversation will appear here...\",\n",
        "                    elem_classes=\"chatbot-container\"\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    msg = gr.Textbox(\n",
        "                        placeholder=\"💭 Ask me anything about your document...\",\n",
        "                        label=\"Your Question\",\n",
        "                        scale=4,\n",
        "                        elem_classes=\"chat-input\"\n",
        "                    )\n",
        "                    clear = gr.Button(\"🗑️ Clear\", variant=\"secondary\", scale=1)\n",
        "\n",
        "            with gr.Column(scale=1, elem_classes=\"content-card\"):\n",
        "                gr.HTML(\"\"\"\n",
        "                <div style='background: linear-gradient(45deg, #667eea, #764ba2); color: white; padding: 2rem; border-radius: 15px; text-align: center;'>\n",
        "                    <h3>💡 Chat Tips</h3>\n",
        "                    <ul style='text-align: left; margin-top: 1rem; line-height: 1.8;'>\n",
        "                        <li>Ask specific questions</li>\n",
        "                        <li>Request explanations</li>\n",
        "                        <li>Get key insights</li>\n",
        "                        <li>Clarify concepts</li>\n",
        "                        <li>Find specific information</li>\n",
        "                        <li>Request translations</li>\n",
        "                    </ul>\n",
        "                </div>\n",
        "                \"\"\")\n",
        "\n",
        "                # Display area for summary/questions/translation in chat mode\n",
        "                chat_display_area = gr.Markdown(\n",
        "                    value=\"*Use the buttons above to show summary, questions, or translation while chatting*\",\n",
        "                    elem_classes=\"content-display\"\n",
        "                )\n",
        "\n",
        "    # Hidden states\n",
        "    current_question_idx = gr.State(0)\n",
        "    current_score = gr.State(0)\n",
        "    current_mcq_idx = gr.State(0)\n",
        "\n",
        "    # Event handlers\n",
        "    process_btn.click(\n",
        "        process_pdf,\n",
        "        inputs=[file_input, num_questions],\n",
        "        outputs=[status_output, result_display]\n",
        "    )\n",
        "\n",
        "    # Action button handlers\n",
        "    def show_summary_and_hide_others():\n",
        "        return {\n",
        "            result_display: show_summary(),\n",
        "            mcq_section: gr.Group(visible=False),\n",
        "            chat_section: gr.Group(visible=False),\n",
        "            quiz_section: gr.Group(visible=False),\n",
        "            translation_section: gr.Group(visible=False)\n",
        "        }\n",
        "\n",
        "    summary_btn.click(\n",
        "        show_summary_and_hide_others,\n",
        "        outputs=[result_display, mcq_section, chat_section, quiz_section, translation_section]\n",
        "    )\n",
        "\n",
        "    def show_translation_interface():\n",
        "        return {\n",
        "            result_display: \"🌍 Translation center activated! Select a language and translation option.\",\n",
        "            translation_section: gr.Group(visible=True),\n",
        "            mcq_section: gr.Group(visible=False),\n",
        "            chat_section: gr.Group(visible=False),\n",
        "            quiz_section: gr.Group(visible=False)\n",
        "        }\n",
        "\n",
        "    translate_btn.click(\n",
        "        show_translation_interface,\n",
        "        outputs=[result_display, translation_section, mcq_section, chat_section, quiz_section]\n",
        "    )\n",
        "\n",
        "    # Translation handlers\n",
        "    def handle_full_translation(language):\n",
        "        translation = show_translation(language)\n",
        "        return translation\n",
        "\n",
        "    def handle_summary_translation(language):\n",
        "        translation = show_translated_summary(language)\n",
        "        return translation\n",
        "\n",
        "    translate_full_btn.click(\n",
        "        handle_full_translation,\n",
        "        inputs=[language_dropdown],\n",
        "        outputs=[translation_display]\n",
        "    )\n",
        "\n",
        "    translate_summary_btn.click(\n",
        "        handle_summary_translation,\n",
        "        inputs=[language_dropdown],\n",
        "        outputs=[translation_display]\n",
        "    )\n",
        "\n",
        "    def show_mcq_interface():\n",
        "        mcq_display = show_mcqs()\n",
        "        if \"❌\" in mcq_display:\n",
        "            return {\n",
        "                result_display: mcq_display,\n",
        "                mcq_section: gr.Group(visible=False),\n",
        "                chat_section: gr.Group(visible=False),\n",
        "                quiz_section: gr.Group(visible=False),\n",
        "                translation_section: gr.Group(visible=False)\n",
        "            }\n",
        "\n",
        "        q = current_pdf_data[\"mcqs\"][0]\n",
        "        return {\n",
        "            result_display: \"📝 Interactive questions mode activated!\",\n",
        "            mcq_section: gr.Group(visible=True),\n",
        "            chat_section: gr.Group(visible=False),\n",
        "            quiz_section: gr.Group(visible=False),\n",
        "            translation_section: gr.Group(visible=False),\n",
        "            mcq_question_display: mcq_display,\n",
        "            mcq_option_a: gr.Button(f\"A. {q['options']['A']}\", elem_classes=\"quiz-btn\", size=\"lg\"),\n",
        "            mcq_option_b: gr.Button(f\"B. {q['options']['B']}\", elem_classes=\"quiz-btn\", size=\"lg\"),\n",
        "            mcq_option_c: gr.Button(f\"C. {q['options']['C']}\", elem_classes=\"quiz-btn\", size=\"lg\"),\n",
        "            mcq_option_d: gr.Button(f\"D. {q['options']['D']}\", elem_classes=\"quiz-btn\", size=\"lg\"),\n",
        "            mcq_progress: f\"Question 1 of {len(current_pdf_data['mcqs'])}\",\n",
        "            current_mcq_idx: 0\n",
        "        }\n",
        "\n",
        "    mcq_btn.click(\n",
        "        show_mcq_interface,\n",
        "        outputs=[result_display, mcq_section, chat_section, quiz_section, translation_section, mcq_question_display,\n",
        "                mcq_option_a, mcq_option_b, mcq_option_c, mcq_option_d, mcq_progress, current_mcq_idx]\n",
        "    )\n",
        "\n",
        "    def show_chat_interface():\n",
        "        message = start_chatbot()[0]\n",
        "        return {\n",
        "            result_display: message,\n",
        "            chat_section: gr.Group(visible=True),\n",
        "            quiz_section: gr.Group(visible=False),\n",
        "            mcq_section: gr.Group(visible=False),\n",
        "            translation_section: gr.Group(visible=False)\n",
        "        }\n",
        "\n",
        "    chat_btn.click(\n",
        "        show_chat_interface,\n",
        "        outputs=[result_display, chat_section, quiz_section, mcq_section, translation_section]\n",
        "    )\n",
        "\n",
        "    # MCQ answer handlers\n",
        "    for option, letter in [(mcq_option_a, \"A\"), (mcq_option_b, \"B\"), (mcq_option_c, \"C\"), (mcq_option_d, \"D\")]:\n",
        "        option.click(\n",
        "            check_mcq_answer,\n",
        "            inputs=[gr.State(letter), current_mcq_idx],\n",
        "            outputs=[mcq_question_display, mcq_option_a, mcq_option_b, mcq_option_c, mcq_option_d, mcq_progress, current_mcq_idx]\n",
        "        )\n",
        "\n",
        "    # MCQ reset and navigation\n",
        "    reset_mcq_btn.click(\n",
        "        reset_mcq,\n",
        "        outputs=[mcq_question_display, mcq_option_a, mcq_option_b, mcq_option_c, mcq_option_d, mcq_progress, current_mcq_idx]\n",
        "    )\n",
        "\n",
        "    def hide_all_sections():\n",
        "        return {\n",
        "            mcq_section: gr.Group(visible=False),\n",
        "            chat_section: gr.Group(visible=False),\n",
        "            quiz_section: gr.Group(visible=False),\n",
        "            translation_section: gr.Group(visible=False),\n",
        "            result_display: \"Select an option to continue learning!\"\n",
        "        }\n",
        "\n",
        "    back_to_menu_btn.click(\n",
        "        hide_all_sections,\n",
        "        outputs=[mcq_section, chat_section, quiz_section, translation_section, result_display]\n",
        "    )\n",
        "\n",
        "    back_to_main_btn.click(\n",
        "        hide_all_sections,\n",
        "        outputs=[mcq_section, chat_section, quiz_section, translation_section, result_display]\n",
        "    )\n",
        "\n",
        "    back_to_menu_trans_btn.click(\n",
        "        hide_all_sections,\n",
        "        outputs=[mcq_section, chat_section, quiz_section, translation_section, result_display]\n",
        "    )\n",
        "\n",
        "    # Chat section summary, MCQ, and translation buttons\n",
        "    chat_summary_btn.click(\n",
        "        show_summary,\n",
        "        outputs=[chat_display_area]\n",
        "    )\n",
        "\n",
        "    def show_mcq_in_chat():\n",
        "        mcq_display = \"📝 **All Questions:**\\n\\n\"\n",
        "        for i, q in enumerate(current_pdf_data[\"mcqs\"]):\n",
        "            mcq_display += f\"**Q{i+1}: {q['question']}**\\n\\n\"\n",
        "            for opt_key, opt_text in q['options'].items():\n",
        "                mcq_display += f\"**{opt_key}.** {opt_text}\\n\"\n",
        "            mcq_display += f\"*Correct Answer: {q['answer']}*\\n\\n---\\n\\n\"\n",
        "        return mcq_display\n",
        "\n",
        "    chat_mcq_btn.click(\n",
        "        show_mcq_in_chat,\n",
        "        outputs=[chat_display_area]\n",
        "    )\n",
        "\n",
        "    def show_translation_in_chat(language):\n",
        "        return show_translation(language)\n",
        "\n",
        "    def toggle_chat_language_dropdown():\n",
        "        return gr.Dropdown(visible=True)\n",
        "\n",
        "    chat_translate_btn.click(\n",
        "        toggle_chat_language_dropdown,\n",
        "        outputs=[chat_language_dropdown]\n",
        "    ).then(\n",
        "        show_translation_in_chat,\n",
        "        inputs=[chat_language_dropdown],\n",
        "        outputs=[chat_display_area]\n",
        "    )\n",
        "\n",
        "    # Quiz functionality (keeping the existing quiz section for backward compatibility)\n",
        "    def show_quiz_interface():\n",
        "        return {\n",
        "            quiz_section: gr.Group(visible=True),\n",
        "            chat_section: gr.Group(visible=False),\n",
        "            mcq_section: gr.Group(visible=False),\n",
        "            translation_section: gr.Group(visible=False)\n",
        "        }\n",
        "\n",
        "    start_quiz_btn.click(\n",
        "        lambda: quiz_section.update(visible=True),\n",
        "        outputs=[]\n",
        "    ).then(\n",
        "        lambda: take_quiz() if current_pdf_data[\"mcqs\"] else (\"❌ No questions available\", \"\", \"\", \"\", \"\", \"\", 0),\n",
        "        outputs=[question_display, option_a, option_b, option_c, option_d, quiz_progress, current_question_idx]\n",
        "    )\n",
        "\n",
        "    # Original quiz answer handlers (keeping for the separate quiz section)\n",
        "    for option, letter in [(option_a, \"A\"), (option_b, \"B\"), (option_c, \"C\"), (option_d, \"D\")]:\n",
        "        option.click(\n",
        "            check_answer,\n",
        "            inputs=[gr.State(letter), current_question_idx, current_score],\n",
        "            outputs=[question_display, option_a, option_b, option_c, option_d, quiz_progress, current_question_idx, current_score]\n",
        "        )\n",
        "\n",
        "    # Chat functionality\n",
        "    msg.submit(chat_with_pdf, [msg, chatbot], [chatbot, msg])\n",
        "    clear.click(lambda: ([], \"\"), outputs=[chatbot, msg])\n",
        "\n",
        "def take_quiz():\n",
        "    \"\"\"Legacy function for existing quiz section\"\"\"\n",
        "    global current_pdf_data\n",
        "    if not current_pdf_data[\"mcqs\"]:\n",
        "        return \"❌ Please upload and process a PDF first to generate questions.\", \"\", \"\", \"\", \"\", \"\", 0\n",
        "\n",
        "    q = current_pdf_data[\"mcqs\"][0]\n",
        "    return (\n",
        "        f\"🎯 **Question 1 of {len(current_pdf_data['mcqs'])}**\\n\\n**{q['question']}**\",\n",
        "        q['options']['A'],\n",
        "        q['options']['B'],\n",
        "        q['options']['C'],\n",
        "        q['options']['D'],\n",
        "        \"0/0 - Quiz Started!\",\n",
        "        0\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}