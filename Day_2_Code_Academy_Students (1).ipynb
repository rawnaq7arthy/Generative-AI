{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### üß™ Day 2: Fine-Tuning Language Models (Phi-1.5)\n",
        "\n",
        "Welcome to the second session of the Generative AI workshop!\n",
        "\n",
        "Today we'll dive into how to **fine-tune a pretrained language model** using your own dataset. We'll walk through the training pipeline using Microsoft's **Phi-1.5** model and Hugging Face tools.\n",
        "\n",
        "üéØ **Objectives**\n",
        "- Understand the difference between pretraining and fine-tuning\n",
        "- Prepare a dataset for supervised fine-tuning\n",
        "- Set up a training configuration using Hugging Face‚Äôs `transformers` and `trl`\n",
        "- Train and evaluate a fine-tuned LLM\n",
        "- Experiment with custom prompts to test your results\n"
      ],
      "metadata": {
        "id": "qeqQcjMWJIUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and import the necessary libraries\n",
        "!pip install torch\n",
        "!pip install -q -U accelerate peft bitsandbytes transformers trl einops\n",
        "!pip install fsspec==2025.3.2\n"
      ],
      "metadata": {
        "id": "BYC1Cfs3upPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚öôÔ∏è Configure environment to avoid memory fragmentation and disable Weights & Biases\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n"
      ],
      "metadata": {
        "id": "M-zY7-mgJKWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üì¶ Install and import libraries\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    prepare_model_for_kbit_training\n",
        ")\n",
        "from trl import SFTTrainer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJrO9_0nJTkm",
        "outputId": "31eed01f-4cfe-4f12-9270-91a1b41e0749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üîó Load Phi-1.5 Tokenizer\n",
        "# TODO: Set the base model name for Phi-1.5\n",
        "base_model = \"____\"  # Hint: What's the model name for microsoft/phi-1_5?\n",
        "\n",
        "# TODO: Load the tokenizer from the pretrained model\n",
        "tokenizer = AutoTokenizer.____(____, trust_remote_code=____)  # Hint: What method loads pretrained tokenizers and should we trust remote code?"
      ],
      "metadata": {
        "id": "0mKi5Y_JKiJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚öôÔ∏è Quantization Configuration for 4-bit Loading\n",
        "# TODO: Create a 4-bit quantization configuration to save memory\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "   load_in_4bit=____,  # Hint: Do we want to load the model in 4-bit precision?\n",
        "   bnb_4bit_quant_type=\"____\",  # Hint: What quantization type? (nf4 or fp4)\n",
        "   bnb_4bit_compute_dtype=torch.____,  # Hint: What data type for computation? (float16 or bfloat16)\n",
        "   bnb_4bit_use_double_quant=____,  # Hint: Should we use double quantization for better compression?\n",
        ")"
      ],
      "metadata": {
        "id": "ddH9ryG9OIob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üß† Load Model with Quantization and Offload as Needed\n",
        "# TODO: Load the model with our quantization configuration\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    ____,  # Hint: What variable contains our model name?\n",
        "    quantization_config=____,  # Hint: What variable contains our quantization config?\n",
        "    trust_remote_code=____,  # Hint: Should we trust remote code for this model?\n",
        "    low_cpu_mem_usage=____,  # Hint: Should we optimize for low CPU memory usage?\n",
        "    device_map={\"\": ____}  # Hint: What device number should we use? (0 for first GPU)\n",
        ")"
      ],
      "metadata": {
        "id": "ZN8XJsqvKxI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîß Prep Model for LoRA + Training Efficiency\n",
        "# TODO: Disable caching to save memory during training\n",
        "model.config.use_cache = ____  # Hint: Should we use cache during training? (True/False)\n",
        "\n",
        "# TODO: Prepare the model for efficient training with gradient checkpointing\n",
        "model = prepare_model_for_kbit_training(____, use_gradient_checkpointing=____)  # Hint: What model to prepare and should we use gradient checkpointing?"
      ],
      "metadata": {
        "id": "LrBqkJ6KK2SV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# üëÄ Preview the Raw Structure (before mapping)\n",
        "# Load a small sample of SQuAD v2 dataset for training\n",
        "# Note: We're using SQuAD v2 for this exercise, but feel free to experiment with your own dataset!\n",
        "\n",
        "# TODO: Load the SQuAD v2 dataset with a small sample\n",
        "dataset = load_dataset(\"____\", split=\"____\")  # Hint: Use \"squad_v2\" and \"train[:1%]\" for a 1% sample\n",
        "\n",
        "# TODO: Display the dataset structure to understand what we're working with\n",
        "print(\"üìä Dataset Structure:\")\n",
        "print(____)  # Hint: What variable contains our dataset?\n",
        "\n",
        "print(\"\\nüîç First Example:\")\n",
        "print(____[0])  # Hint: How do we access the first example in the dataset?\n",
        "\n",
        "print(\"\\nüìã Available Keys:\")\n",
        "print(\"Features:\", ____[0].keys())  # Hint: How do we see the keys of the first example?\n",
        "\n",
        "# üí° UNDERSTANDING TIP:\n",
        "# Look at the structure! You'll see 'question', 'answers', 'context', etc.\n",
        "# We need to transform this into a simple \"Question: ... Answer: ...\" format for training"
      ],
      "metadata": {
        "id": "6qvH2HRhNRT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîÑ Format the Data to Plain QA Format\n",
        "# Now that we understand the structure, let's transform it for training\n",
        "\n",
        "def format_qa(example):\n",
        "    # TODO: Extract the question from the example\n",
        "    question = example[\"____\"].strip()  # Hint: What key contains the question text?\n",
        "\n",
        "    # TODO: Get the answer text, with fallback for unanswerable questions\n",
        "    # SQuAD v2 structure: answers['text'] is a list, we want the first answer\n",
        "    answer = example[\"____\"][\"____\"][0] if example[\"____\"][\"____\"] else \"____\"  # Hint: Navigate through answers->text array, fallback to \"I don't know.\"\n",
        "\n",
        "    # TODO: Create formatted text for training in Question/Answer format\n",
        "    return {\"text\": f\"Question: {____}\\nAnswer: {____}\"}  # Hint: Use the question and answer variables you created above\n",
        "\n",
        "# TODO: Apply the formatting function to transform our dataset\n",
        "dataset = dataset.map(____)  # Hint: What function should we apply to each example in the dataset?\n",
        "\n",
        "print(\"‚úÖ Dataset formatted successfully!\")\n",
        "print(\"\\nüîç Example of formatted data:\")\n",
        "print(____[0][\"____\"])  # Hint: What dataset and what key contains our formatted text?\n",
        "\n",
        "# üí° ADVANCED TIP: Want to use your own dataset?\n",
        "# Replace \"squad_v2\" with your dataset name or create a custom dataset with this format:\n",
        "# {\"text\": \"Question: Your question here\\nAnswer: Your answer here\"}"
      ],
      "metadata": {
        "id": "oViFPVkv1T9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTConfig\n",
        "\n",
        "# ‚öôÔ∏è Training Configuration - Fine-tune Your Training Parameters\n",
        "# This configuration controls how our model will be trained\n",
        "\n",
        "training_args = SFTConfig(\n",
        "   # üìÅ Output and Logging Configuration\n",
        "   # TODO: Set the directory where training results will be saved\n",
        "   output_dir=\"____\",  # Hint: Create a descriptive folder name like \"./model-results\" to store all training outputs\n",
        "\n",
        "   # TODO: Set the directory for logging training metrics\n",
        "   logging_dir=\"____\",  # Hint: TensorBoard reads from a \"runs\" subfolder of your output directory\n",
        "\n",
        "   # TODO: Choose what to report training metrics to\n",
        "   report_to=\"____\",  # Hint: Popular options include \"tensorboard\", \"wandb\", or \"none\" for tracking progress\n",
        "\n",
        "   # üèÉ‚Äç‚ôÇÔ∏è Training Schedule Configuration\n",
        "   # TODO: Set how many complete passes through the dataset\n",
        "   num_train_epochs=____,  # Hint: Range 1-5. More epochs = longer training but potentially better results. Start small for testing!\n",
        "\n",
        "   # TODO: Set batch size per GPU device (affects memory usage)\n",
        "   per_device_train_batch_size=____,  # Hint: Range 1-8. Higher = faster training but more memory. Start low if you get memory errors\n",
        "\n",
        "   # TODO: Set gradient accumulation steps (simulates larger batch size)\n",
        "   gradient_accumulation_steps=____,  # Hint: Range 2-8. Higher = more stable gradients but slower. Multiply with batch_size for effective batch size\n",
        "\n",
        "   # üéØ Learning Rate Configuration\n",
        "   # TODO: Set the learning rate (how fast the model learns)\n",
        "   learning_rate=____,  # Hint: Range 1e-5 to 5e-4. Too high = unstable training, too low = slow learning. Scientific notation like 2e-4\n",
        "\n",
        "   # TODO: Choose learning rate scheduler type\n",
        "   lr_scheduler_type=\"____\",  # Hint: Options: \"cosine\" (gradual decrease), \"linear\" (steady decrease), \"constant\" (no change)\n",
        "\n",
        "   # TODO: Set warmup ratio (gradually increase learning rate at start)\n",
        "   warmup_ratio=____,  # Hint: Range 0.01-0.1. Prevents early training instability. Start around 3% of training\n",
        "\n",
        "   # üìä Logging and Saving Configuration\n",
        "   # TODO: Set how often to log training metrics (in steps)\n",
        "   logging_steps=____,  # Hint: Range 1-20. Lower = more frequent updates, higher = less storage. Balance monitoring vs performance\n",
        "\n",
        "   # TODO: Set when to save model checkpoints\n",
        "   save_strategy=\"____\",  # Hint: Options: \"epoch\" (after each full pass), \"steps\" (every N steps), \"no\" (never save)\n",
        "\n",
        "   # TODO: Set maximum number of checkpoints to keep\n",
        "   save_total_limit=____,  # Hint: Range 1-5. More = keep more versions but use more disk space. Consider your storage limits\n",
        "\n",
        "   # üöÄ Optimization Configuration\n",
        "   # TODO: Choose the optimizer algorithm\n",
        "   optim=\"____\",  # Hint: \"paged_adamw_32bit\" is memory-efficient, \"adamw_torch\" is standard. Choose based on memory constraints\n",
        "\n",
        "   # TODO: Set maximum gradient norm for clipping\n",
        "   max_grad_norm=____,  # Hint: Range 0.1-1.0. Prevents gradient explosion. Lower = more conservative, higher = allows bigger updates\n",
        "\n",
        "   # TODO: Enable gradient checkpointing to save memory\n",
        "   gradient_checkpointing=____,  # Hint: True/False. True saves memory but slightly slower. Essential for large models on limited GPU\n",
        "\n",
        "   # üîß Memory and Precision Configuration\n",
        "   # TODO: Enable 16-bit floating point training\n",
        "   fp16=____,  # Hint: True/False. True reduces memory and speeds up training on modern GPUs. Disable if you get numerical issues\n",
        "\n",
        "   # TODO: Enable grouping sequences by length for efficiency\n",
        "   group_by_length=____,  # Hint: True/False. True improves efficiency by batching similar-length sequences together\n",
        "\n",
        "   # üé≤ Reproducibility Configuration\n",
        "   # TODO: Set random seed for reproducible results\n",
        "   seed=____,  # Hint: Any integer. Use the same number to get identical results across runs. Popular choices: 42, 123, 2024\n",
        "\n",
        "   # üìù Text Processing Configuration\n",
        "   # TODO: Specify which field contains the training text\n",
        "   dataset_text_field=\"____\",  # Hint: Must match the field name from your formatted dataset. Check what you named it in the mapping step\n",
        "\n",
        "   # TODO: Set maximum sequence length for training\n",
        "   max_seq_length=____  # Hint: Range 256-2048. Longer = more context but more memory. Common choices: 512, 1024. Match your data needs\n",
        ")\n",
        "\n",
        "# üí° CONFIGURATION STRATEGY:\n",
        "# 1. Start with conservative settings (small epochs, batch size, learning rate)\n",
        "# 2. Monitor memory usage and training progress\n",
        "# 3. Gradually increase parameters if needed\n",
        "# 4. If you get \"CUDA out of memory\": reduce batch_size, max_seq_length, or enable gradient_checkpointing\n",
        "# 5. Use TensorBoard to visualize training: `tensorboard --logdir ./your-output-dir/runs`"
      ],
      "metadata": {
        "id": "acQTTlaXTXI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üéØ LoRA Configuration for Low-Rank Adaptation\n",
        "# LoRA allows efficient fine-tuning by only training small additional parameters\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "   # TODO: Set the rank parameter (controls model capacity vs efficiency trade-off)\n",
        "   r=____,  # Hint: Range 8-128. Higher = more parameters to train (better learning but slower). Start with powers of 2: 16, 32, 64\n",
        "\n",
        "   # TODO: Set the alpha parameter (controls adaptation strength)\n",
        "   lora_alpha=____,  # Hint: Range 8-64. Often set equal to or double the rank. Controls how much LoRA affects the original model\n",
        "\n",
        "   # TODO: Set dropout rate for regularization\n",
        "   lora_dropout=____,  # Hint: Range 0.0-0.3. Higher = more regularization but may hurt learning. 0.05-0.1 is common for preventing overfitting\n",
        "\n",
        "   # Static configurations (advanced settings - leave these as-is)\n",
        "   bias=\"none\",  # No bias training for efficiency\n",
        "   task_type=\"CAUSAL_LM\",  # Language modeling task\n",
        "   target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  # Which model parts to adapt\n",
        ")\n",
        "\n",
        "# üí° LoRA PARAMETER TIPS:\n",
        "# - r (rank): Start with 16-32. Higher if you have complex data or need better performance\n",
        "# - lora_alpha: Often set to r or 2*r. Higher values make LoRA adaptations more influential\n",
        "# - lora_dropout: Start around 0.05. Increase if you see overfitting, decrease if underfitting\n",
        "# - Total trainable parameters ‚âà 2 * r * (sum of target module dimensions)"
      ],
      "metadata": {
        "id": "MRNDSa0GTZ5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üöÄ Setup Trainer with SFT (Supervised Fine-Tuning)\n",
        "# This combines our model, dataset, and configurations into a training pipeline\n",
        "\n",
        "# TODO: Create the SFT trainer with all our configurations\n",
        "trainer = SFTTrainer(\n",
        "   model=____,  # Hint: What variable contains our loaded and prepared model?\n",
        "   train_dataset=____,  # Hint: What variable contains our formatted dataset?\n",
        "   peft_config=____,  # Hint: What variable contains our LoRA configuration?\n",
        "   args=____  # Hint: What variable contains our training arguments/configuration?\n",
        ")\n",
        "\n",
        "# üí° TRAINER SETUP EXPLANATION:\n",
        "# - model: The quantized Phi model we prepared earlier\n",
        "# - train_dataset: Our formatted SQuAD dataset with \"Question: ... Answer: ...\" format\n",
        "# - peft_config: LoRA settings that determine which parts of the model to fine-tune\n",
        "# - args: All the training hyperparameters we configured (learning rate, batch size, etc.)"
      ],
      "metadata": {
        "id": "OmY3Uv-HTbzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üî• Start Training\n",
        "# This will begin the fine-tuning process - monitor the output for progress!\n",
        "\n",
        "# TODO: Start the training process\n",
        "____.____()  # Hint: What object do we call the train method on?\n",
        "\n",
        "# üí° TRAINING TIPS:\n",
        "# - Training will show loss decreasing over time (good sign!)\n",
        "# - Watch for \"CUDA out of memory\" errors - reduce batch_size if this happens\n",
        "# - Training time depends on your settings: epochs, dataset size, and hardware\n",
        "# - You can stop training anytime with Ctrl+C and still save progress"
      ],
      "metadata": {
        "id": "uVaajRpHTdN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üíæ Save the Fine-tuned Model Components\n",
        "# After training, we need to save our work for future use\n",
        "\n",
        "# TODO: Save the LoRA adapter weights (lightweight and efficient)\n",
        "____.model.save_pretrained(\"____\")  # Hint: Use the trainer object to access the model and give it a descriptive name like \"phi15-lora-adapter\"\n",
        "\n",
        "# TODO: Save the tokenizer for future inference\n",
        "____.save_pretrained(\"____\")  # Hint: What tokenizer object should we save and what folder name should match our model?\n",
        "\n",
        "# üí° SAVING EXPLANATION:\n",
        "# - LoRA adapter: Only saves the small additional weights we trained (few MB vs GB)\n",
        "# - Tokenizer: Essential for processing text inputs during inference\n",
        "# - These files can be loaded later with the base model for predictions\n",
        "# - Much more storage-efficient than saving the entire model"
      ],
      "metadata": {
        "id": "hvBRYtBVThp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir phi15-results/runs"
      ],
      "metadata": {
        "id": "BhtQn9VdPtF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üß™ Test the Fine-tuned Model\n",
        "# Let's see how our fine-tuned model performs on a new question\n",
        "\n",
        "# TODO: Create a test prompt in the same format as training\n",
        "prompt = \"____\"  # Hint: Use the same \"Question: ... \\nAnswer: \" format we trained on\n",
        "\n",
        "# TODO: Tokenize the prompt for the model\n",
        "inputs = tokenizer(____, return_tensors=\"____\").to(model.device)  # Hint: What prompt variable and tensor format?\n",
        "\n",
        "# Set model to evaluation mode and disable cache to avoid warnings\n",
        "model.eval()\n",
        "model.config.use_cache = False\n",
        "\n",
        "# TODO: Generate a response with controlled parameters\n",
        "with torch.no_grad():  # Saves memory during inference\n",
        "    outputs = model.generate(\n",
        "        ____,  # Hint: What input variable contains our tokenized prompt?\n",
        "        max_new_tokens=____,  # Hint: Range 20-100. How many new tokens should the model generate?\n",
        "        temperature=____,  # Hint: Range 0.1-1.0. Lower = more focused, higher = more creative\n",
        "        do_sample=____,  # Hint: True/False. True for creative responses, False for deterministic\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        use_cache=False  # Prevents cache warnings\n",
        "    )\n",
        "\n",
        "# TODO: Decode and print the generated response\n",
        "response = tokenizer.decode(____[0], skip_special_tokens=____)  # Hint: What outputs to decode and should we skip special tokens?\n",
        "print(\"ü§ñ Model Response:\")\n",
        "print(____)  # Hint: What variable contains our decoded response?\n",
        "\n",
        "# üí° TESTING TIPS:\n",
        "# - Try different questions to see how well your model learned\n",
        "# - Compare responses to the base model (before fine-tuning)\n",
        "# - Lower temperature = more consistent answers\n",
        "# - Higher temperature = more creative but potentially less accurate answers"
      ],
      "metadata": {
        "id": "_6pSeldFUrA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç Let's Compare: Base vs Fine-Tuned Phi-1.5\n",
        "\n",
        "Now that we've trained our model, let's see the difference in how it responds to a prompt compared to the original (pretrained) version.\n",
        "\n",
        "We‚Äôll use the same input for both models and observe their generated responses.\n"
      ],
      "metadata": {
        "id": "ygXXUqxNzm-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# üîç Let's Compare: Base vs Fine-Tuned Phi-1.5\n",
        "# We'll test both models with the same prompt to see the difference\n",
        "\n",
        "# TODO: Load the tokenizer for comparison testing\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"____\", trust_remote_code=____)  # Hint: What's the Phi-1.5 model name and should we trust remote code?\n",
        "\n",
        "# TODO: Define a test prompt (notice we removed \"Answer:\" to let models complete naturally)\n",
        "prompt = '''\n",
        "   Question: ____\n",
        "'''  # Hint: Create an interesting question to test both models - try something practical or educational\n",
        "\n",
        "# TODO: Tokenize the prompt for both models\n",
        "inputs = tokenizer(____, return_tensors=\"____\").to(\"____\")  # Hint: What prompt variable, tensor format, and device?\n",
        "\n",
        "# üì¶ TEST BASE MODEL\n",
        "print(\"üîÑ Loading base model...\")\n",
        "# TODO: Load the original Phi-1.5 model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "   \"____\",  # Hint: Same model name as tokenizer\n",
        "   trust_remote_code=____,  # Hint: Should we trust remote code?\n",
        "   device_map=\"____\",  # Hint: What device should we use?\n",
        "   low_cpu_mem_usage=____  # Hint: Should we optimize CPU memory usage?\n",
        ")\n",
        "base_model.eval()\n",
        "\n",
        "# TODO: Generate response from base model with proper EOS handling\n",
        "with torch.no_grad():\n",
        "   base_output = base_model.generate(\n",
        "       **____,\n",
        "       max_new_tokens=____,                     # Hint: How many new tokens? (30-100)\n",
        "       eos_token_id=tokenizer.____,             # Hint: What token ID signals end of sequence?\n",
        "       pad_token_id=tokenizer.____,             # Hint: What token for padding consistency?\n",
        "       temperature=____,                        # Hint: Range 0.1-1.0 for creativity balance\n",
        "       do_sample=____                          # Hint: True/False for natural responses\n",
        "   )\n",
        "\n",
        "print(\"üì¶ Base model output:\")\n",
        "print(tokenizer.decode(____[0], skip_special_tokens=____))  # Hint: What output to decode and skip special tokens?\n",
        "\n",
        "# Clear memory before loading fine-tuned model\n",
        "del base_model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# üß† TEST FINE-TUNED MODEL\n",
        "print(\"\\nüîÑ Loading fine-tuned model...\")\n",
        "# TODO: Load your fine-tuned model\n",
        "finetuned_model = AutoModelForCausalLM.from_pretrained(\n",
        "   \"____\",  # Hint: What folder did you save your LoRA adapter to?\n",
        "   trust_remote_code=____,  # Hint: Should we trust remote code?\n",
        "   device_map=\"____\",  # Hint: What device should we use?\n",
        "   low_cpu_mem_usage=____  # Hint: Should we optimize CPU memory usage?\n",
        ")\n",
        "finetuned_model.eval()\n",
        "\n",
        "# TODO: Generate response from fine-tuned model with proper EOS handling\n",
        "with torch.no_grad():\n",
        "   tuned_output = finetuned_model.generate(\n",
        "       **____,\n",
        "       max_new_tokens=____,                     # Hint: How many new tokens? (30-100)\n",
        "       eos_token_id=tokenizer.____,             # Hint: What token ID signals end of sequence?\n",
        "       pad_token_id=tokenizer.____,             # Hint: What token for padding consistency?\n",
        "       temperature=____,                        # Hint: Range 0.1-1.0 for creativity balance\n",
        "       do_sample=____                          # Hint: True/False for natural responses\n",
        "   )\n",
        "\n",
        "print(\"üß† Fine-tuned model output:\")\n",
        "print(tokenizer.decode(____[0], skip_special_tokens=____))  # Hint: What output to decode and skip special tokens?\n",
        "\n",
        "# ü§î COMPARISON QUESTIONS:\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ü§î REFLECTION QUESTIONS:\")\n",
        "print(\"1. Which response was more helpful and detailed?\")\n",
        "print(\"2. Did the fine-tuned model follow the Q&A format better?\")\n",
        "print(\"3. Which model gave more accurate information?\")\n",
        "print(\"4. How did the training on SQuAD data affect the responses?\")\n",
        "print(\"5. Try different questions - what patterns do you notice?\")\n",
        "print(\"\\nüí° TIP: Try different questions to test various aspects:\")\n",
        "print(\"   - Factual questions: 'What is the largest ocean?'\")\n",
        "print(\"   - Reasoning questions: 'Why do seasons change?'\")\n",
        "print(\"   - Practical questions: 'How do you change a tire?'\")\n",
        "\n",
        "# üí° PROMPT STRATEGY TIP:\n",
        "# Notice we removed \"Answer:\" from the prompt to let both models complete the response naturally.\n",
        "# This shows how each model approaches question-answering differently:\n",
        "# - Base model: May provide direct answers or continue with more questions\n",
        "# - Fine-tuned model: Should provide structured Q&A format responses"
      ],
      "metadata": {
        "id": "hGUFYYU0Usu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üéâ Congratulations! Core Fine-tuning Complete\n",
        "\n",
        "You've successfully fine-tuned your first language model! You now understand:\n",
        "- How to prepare data for training\n",
        "- LoRA configurations and memory optimization  \n",
        "- Training pipelines and model evaluation\n",
        "- The difference between base and fine-tuned model performance\n",
        "\n",
        "## üöÄ Ready for More? Advanced Experiments Below!\n",
        "\n",
        "For students who finish early or want to dive deeper, try the advanced experiments below. Pick any that interest you and document your findings!"
      ],
      "metadata": {
        "id": "hd4BTda5CSZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ü§ñ EXPERIMENT: Different Model Architectures\n",
        "# Test fine-tuning on different model families\n",
        "\n",
        "models_to_try = [\n",
        "    \"microsoft/DialoGPT-small\",     # Conversational model\n",
        "    \"distilgpt2\",                   # Smaller, faster GPT-2\n",
        "    \"microsoft/phi-2\",              # Larger Phi model\n",
        "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Tiny but capable\n",
        "]\n",
        "\n",
        "# TODO: Pick a model and compare fine-tuning results\n",
        "# Which model learns your task best? Which is fastest to train?\n",
        "# Document: training time, memory usage, output quality"
      ],
      "metadata": {
        "id": "pRkKAO34CWsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üìö EXPERIMENT: Try Different Datasets\n",
        "# See how the same model learns different types of tasks\n",
        "\n",
        "datasets_to_try = [\n",
        "   \"imdb\",                    # Movie reviews (sentiment)\n",
        "   \"ag_news\",                 # News classification\n",
        "   \"eli5\",                    # Simple explanations\n",
        "   \"math_qa\"                  # Math word problems\n",
        "]\n",
        "\n",
        "# TODO: Pick a dataset and compare fine-tuning results\n",
        "# Which dataset is easiest to learn? Which gives best responses?\n",
        "# Document: training progress, final quality, task difficulty"
      ],
      "metadata": {
        "id": "-SnHrWF2Egyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "# Clear any existing model\n",
        "try:\n",
        "    del base_model\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    del finetuned_model\n",
        "except:\n",
        "    pass\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "FnAFxVwxztwa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}