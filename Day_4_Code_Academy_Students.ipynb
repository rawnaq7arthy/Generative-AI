{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìù Day 4: Document Summarization with Transformers\n",
        "\n",
        "Welcome to Day 4 of the Generative AI workshop!\n",
        "\n",
        "Today we will focus on **text summarization** using open-source transformer models. Summarization is a core capability of modern LLMs that allows us to extract concise, informative summaries from longer documents.\n",
        "\n",
        "We'll work through a complete pipeline:\n",
        "- Upload a PDF file\n",
        "- Extract and chunk the text\n",
        "- Load a summarization model\n",
        "- Run summarization at the chunk level and for the full document\n",
        "\n",
        "By the end of this lab, you‚Äôll understand how summarization works and how to apply it to real-world documents.\n"
      ],
      "metadata": {
        "id": "cSOQTLzKsBLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Step 1: Install Required Packages\n",
        "\n",
        "We'll install the following libraries:\n",
        "- `PyPDF2` to extract text from PDFs\n",
        "- `transformers` to load our summarization model\n",
        "- `sentence-transformers` to optionally embed for semantic filtering\n"
      ],
      "metadata": {
        "id": "1xuGb8XysFcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2 transformers sentence-transformers rouge-score tqdm nltk --quiet\n"
      ],
      "metadata": {
        "id": "fD4AoyJZsCEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Step 2: Import Required Libraries\n"
      ],
      "metadata": {
        "id": "ZRj1V3_WsWI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PyPDF2 import PdfReader\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "import torch"
      ],
      "metadata": {
        "id": "3bM39y9SsHSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìÑ Step 3: Upload and Extract Text from PDF\n",
        "\n",
        "In this step, we'll learn how to upload a PDF file and extract its text content using the **PyPDF2** library. This is a crucial skill for processing document-based data in machine learning and NLP projects.\n",
        "\n",
        "## What We'll Learn:\n",
        "- How to upload files in Google Colab\n",
        "- Extract text from PDF documents\n",
        "- Handle multi-page PDFs efficiently\n",
        "- Process and clean extracted text\n",
        "\n",
        "## Why This Matters:\n",
        "PDF text extraction is essential for:\n",
        "- Document analysis and summarization\n",
        "- Information retrieval systems\n",
        "- Training data preparation for NLP models\n",
        "- Automated document processing pipelines\n",
        "\n",
        "## Your Task:\n",
        "Complete the code below by filling in the blanks. Pay attention to the hints provided in the comments!"
      ],
      "metadata": {
        "id": "jMIWxtWgs8bB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from google.colab import files\n",
        "from _______ import _______  # Hint: PyPDF2, PdfReader (in that order)\n",
        "\n",
        "# Step 1: Upload PDF file\n",
        "print(\"üìÅ Please select your PDF file to upload...\")\n",
        "uploaded = files._______()  # Hint: Choose one -> upload() | download() | read()\n",
        "\n",
        "# Step 2: Get the filename and create PDF reader\n",
        "filename = next(iter(_______))  # Hint: The variable that contains uploaded files -> uploaded | files | reader\n",
        "reader = _______(filename)      # Hint: The class we imported -> PdfReader | FileReader | TextReader\n",
        "\n",
        "print(f\"\\nüìä PDF Analysis:\")\n",
        "print(f\"üìÑ Number of pages: {len(reader.______)}\")  # Hint: PDF attribute -> pages | text | content\n",
        "\n",
        "# Step 3: Extract all text from PDF\n",
        "# Hint: Fill in the blanks to extract text from each page\n",
        "# Pattern: [PAGE.extract_text() for PAGE in reader.PAGES if page.extract_text()]\n",
        "full_text = \"\\n\".join([_______.extract_text() for _______ in reader._______ if page.extract_text()])\n",
        "#                       ^page                    ^page              ^pages\n",
        "\n",
        "# Step 4: Display results\n",
        "print(f\"\\n‚úÖ PDF uploaded and processed!\")\n",
        "print(f\"üìÇ Filename: {_______}\")  # Hint: Variable storing the file name -> filename | uploaded | reader\n",
        "print(f\"üìè Total text length: {len(_______)} characters\")  # Hint: Variable with all text -> full_text | text | content\n",
        "\n",
        "# Bonus: Display a preview of the text\n",
        "print(f\"üìù First 200 characters preview:\")\n",
        "print(\"-\" * 50)\n",
        "# Hint: All three blanks should be the same variable containing our extracted text\n",
        "print(_______[:200] + \"...\" if len(_______) > 200 else _______)\n",
        "#      ^full_text                    ^full_text           ^full_text\n",
        "\n",
        "# üí° LEARNING NOTES:\n",
        "# - This step reads each page of the PDF using PyPDF2\n",
        "# - We join the extracted text into a single string with newlines\n",
        "# - The resulting full_text variable will be used for chunking and embedding in the next steps\n",
        "# - We filter out empty pages to avoid processing blank content\n",
        "# - List comprehension makes the code more efficient and readable"
      ],
      "metadata": {
        "id": "Y8OQ8Q8_sYnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üëÅÔ∏è Step 3.1: Optional - Preview Extracted Text\n",
        "\n",
        "Now that we've successfully extracted text from our PDF, let's take a look at what we've got! This optional step helps us verify that our text extraction worked properly and gives us a sense of the content we'll be working with.\n",
        "\n",
        "## Why Preview the Text?\n",
        "- **Quality Check**: Ensure the extraction captured readable text\n",
        "- **Content Understanding**: Get familiar with the document structure\n",
        "- **Debugging**: Identify any formatting issues early\n",
        "- **Data Validation**: Confirm we have meaningful content to work with\n",
        "\n",
        "## Your Task:\n",
        "Complete the code to preview and analyze your extracted text. Use the hints to fill in the blanks!\n",
        "\n",
        "*Note: We're showing the first 1000 characters to get a good preview without overwhelming the output.*"
      ],
      "metadata": {
        "id": "uNNGHwvtuQIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üîç Preview the extracted text to verify successful extraction\n",
        "print(\"üìñ EXTRACTED TEXT PREVIEW (First 1000 characters):\")\n",
        "print(\"=\" * 60)\n",
        "print(full_text[:1000])\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# üìä Additional text statistics\n",
        "print(f\"\\nüìà TEXT STATISTICS:\")\n",
        "print(f\"üìè Total characters: {len(full_text):,}\")\n",
        "print(f\"üìù Total words: {len(full_text.split()):,}\")\n",
        "print(f\"üìÑ Total lines: {len(full_text.split(chr(10))):,}\")\n",
        "\n",
        "# üéØ Quick content check\n",
        "if len(full_text) > 100:\n",
        "    print(f\"‚úÖ Text extraction successful - Ready for processing!\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Warning: Text seems too short. Check your PDF file.\")"
      ],
      "metadata": {
        "id": "D111C1KvtnJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÇÔ∏è Step 4: Chunk the Text\n",
        "\n",
        "Large documents can be overwhelming for AI models and search systems. To make our text more manageable and improve processing efficiency, we'll break it into smaller, meaningful chunks.\n",
        "\n",
        "## Why Do We Need Text Chunking?\n",
        "\n",
        "### üéØ **Performance Benefits:**\n",
        "- **Memory Efficiency**: Smaller chunks use less computational resources\n",
        "- **Better Embeddings**: Models work better with focused, coherent text segments\n",
        "- **Improved Search**: Users can find specific information more easily\n",
        "- **Parallel Processing**: Multiple chunks can be processed simultaneously\n",
        "\n",
        "### üìè **Chunking Strategy:**\n",
        "- **Sentence-Based**: We split at sentence boundaries to maintain meaning\n",
        "- **Size Control**: Each chunk stays under 2000 characters (default)\n",
        "- **Context Preservation**: We don't break sentences in the middle\n",
        "\n",
        "## Your Task:\n",
        "Complete the chunking function by filling in the blanks. Pay attention to the algorithm steps in the comments!"
      ],
      "metadata": {
        "id": "xbxU_K1zuSp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import _____  # Hint: We need the 're' module for regex operations\n",
        "\n",
        "def chunk_text(text, max_length=2000):\n",
        "    \"\"\"\n",
        "    Split text into manageable chunks while preserving sentence boundaries.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to be chunked\n",
        "        max_length (int): Maximum characters per chunk (default: 2000)\n",
        "\n",
        "    Returns:\n",
        "        list: List of text chunks\n",
        "    \"\"\"\n",
        "    # Step 1: Split text into sentences using regex\n",
        "    # Hint: Use re.split() with the pattern r'(?<=[.!?])\\s+'\n",
        "    sentences = _____.split(r'(?<=[.!?])\\s+', _____)\n",
        "    #            ^re                           ^text\n",
        "\n",
        "    # Step 2: Initialize variables for chunking\n",
        "    # Hint: We need an empty list for chunks and an empty string for current chunk\n",
        "    chunks, chunk = _____, _____\n",
        "    #                ^[]    ^\"\"\n",
        "\n",
        "    # Step 3: Build chunks by combining sentences\n",
        "    for sentence in _____:  # Hint: Loop through our sentences list\n",
        "        # Check if adding this sentence would exceed our limit\n",
        "        if len(_____) + len(_____) <= max_length:  # Hint: chunk + sentence\n",
        "            chunk += _____ + \" \"  # Hint: Add the sentence with a space\n",
        "        else:\n",
        "            # Current chunk is full, save it and start new one\n",
        "            if chunk:  # Only append if chunk has content\n",
        "                chunks._____(chunk.strip())  # Hint: Add chunk to list -> append() | add() | insert()\n",
        "            chunk = _____ + \" \"  # Hint: Start new chunk with current sentence\n",
        "\n",
        "    # Step 4: Don't forget the last chunk!\n",
        "    if _____:  # Hint: Check if chunk has content\n",
        "        chunks.append(_____.strip())  # Hint: Add the final chunk\n",
        "\n",
        "    return _____  # Hint: Return our list of chunks\n",
        "\n",
        "# Apply chunking to our extracted text\n",
        "print(\"üîÑ Chunking the extracted text...\")\n",
        "chunks = chunk_text(_____)  # Hint: Pass our extracted text -> full_text | text | content\n",
        "\n",
        "# Display results\n",
        "print(f\"‚úÖ Text successfully chunked!\")\n",
        "print(f\"üìä Created {len(_____)} chunks\")  # Hint: Count our chunks list\n",
        "print(f\"üìè Average chunk size: {sum(len(chunk) for chunk in _____) // len(_____)} characters\")\n",
        "#                                                            ^chunks        ^chunks\n",
        "\n",
        "# Preview first chunk\n",
        "print(f\"\\nüìñ Preview of first chunk:\")\n",
        "print(\"-\" * 50)\n",
        "# Hint: Show first 300 characters of the first chunk, add \"...\" if longer\n",
        "print(_____[0][:300] + \"...\" if len(_____[0]) > 300 else _____[0])\n",
        "#      ^chunks                       ^chunks              ^chunks\n",
        "\n",
        "# üí° LEARNING NOTES:\n",
        "# - Regular expressions (regex) help us split text at sentence boundaries\n",
        "# - The strip() method removes leading/trailing whitespace\n",
        "# - We use len() to check if adding a sentence would exceed our character limit\n",
        "# - List comprehension with sum() calculates the average chunk size efficiently\n",
        "# - Always handle the last chunk separately since the loop might not catch it"
      ],
      "metadata": {
        "id": "p30tG23duRsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìö Step 4.1: Optional - Inspect Chunks\n",
        "\n",
        "Now that we've chunked our text, let's examine what we've created! This inspection step helps us understand the quality and distribution of our chunks, ensuring they're suitable for downstream processing.\n",
        "\n",
        "## What We'll Analyze:\n",
        "- **üìä Statistical Overview**: Count, averages, min/max sizes\n",
        "- **üìñ Content Preview**: Look at actual chunk content\n",
        "- **üìà Distribution Analysis**: Visualize chunk size patterns\n",
        "- **üéØ Quality Assessment**: Ensure chunks are meaningful and well-sized\n",
        "\n",
        "## Your Task:\n",
        "Complete the chunk analysis code by filling in the blanks. Use the hints to calculate statistics and create visualizations!"
      ],
      "metadata": {
        "id": "1z9xiiDGuZMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced chunk inspection with comprehensive statistics\n",
        "import statistics\n",
        "\n",
        "# üìä Calculate comprehensive chunk statistics\n",
        "total_chunks = len(chunks)\n",
        "chunk_lengths = [len(chunk) for chunk in chunks]\n",
        "avg_chunk_length = statistics.mean(chunk_lengths)\n",
        "min_chunk_length = min(chunk_lengths)\n",
        "max_chunk_length = max(chunk_lengths)\n",
        "median_chunk_length = statistics.median(chunk_lengths)\n",
        "\n",
        "# üìà Display detailed statistics\n",
        "print(\"=\" * 60)\n",
        "print(\"üìä CHUNK STATISTICS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"üìù Total number of chunks: {total_chunks}\")\n",
        "print(f\"üìè Average chunk length: {avg_chunk_length:.1f} characters\")\n",
        "print(f\"üìâ Minimum chunk length: {min_chunk_length} characters\")\n",
        "print(f\"üìà Maximum chunk length: {max_chunk_length} characters\")\n",
        "print(f\"üìä Median chunk length: {median_chunk_length:.1f} characters\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "# üìñ Display sample chunks for content review\n",
        "print(\"üìñ SAMPLE CHUNKS:\")\n",
        "print(\"-\" * 40)\n",
        "for i in range(min(3, len(chunks))):\n",
        "    print(f\"--- Chunk {i+1} ({len(chunks[i])} chars) ---\")\n",
        "    print(chunks[i][:200] + \"...\" if len(chunks[i]) > 200 else chunks[i])\n",
        "    print()\n",
        "\n",
        "# üìà Visual chunk length distribution\n",
        "print(\"üìà CHUNK LENGTH DISTRIBUTION (First 10 chunks):\")\n",
        "print(\"-\" * 50)\n",
        "for i, length in enumerate(chunk_lengths[:10]):\n",
        "    # Create a simple bar chart using characters\n",
        "    bar_length = int(length / max_chunk_length * 30)  # Scale to 30 chars max\n",
        "    bar = \"‚ñà\" * bar_length\n",
        "    print(f\"Chunk {i+1:2d}: {length:4d} chars |{bar}\")\n",
        "\n",
        "if len(chunk_lengths) > 10:\n",
        "    print(f\"... and {len(chunk_lengths) - 10} more chunks\")\n",
        "\n",
        "print()\n",
        "\n",
        "# üéØ Quick summary for easy reference\n",
        "print(\"üîç QUICK SUMMARY:\")\n",
        "print(f\"   üìö {total_chunks} chunks | üìè Avg: {avg_chunk_length:.0f} chars | üìä Range: {min_chunk_length}-{max_chunk_length} chars\")\n",
        "\n",
        "# ‚úÖ Quality assessment\n",
        "print(f\"\\n‚úÖ QUALITY CHECK:\")\n",
        "if avg_chunk_length > 100 and avg_chunk_length < 2500:\n",
        "    print(\"‚úÖ Chunk sizes look good for processing!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Consider adjusting chunk size parameters.\")\n",
        "\n",
        "if max_chunk_length <= 2000:\n",
        "    print(\"‚úÖ All chunks within size limit!\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Some chunks exceed 2000 characters (max: {max_chunk_length})\")"
      ],
      "metadata": {
        "id": "M8b3LkKSuVb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ Step 5: Load Summarization Model\n",
        "\n",
        "Now we'll load a powerful AI model to summarize our text chunks. We're using **Google's FLAN-T5-Base** - a state-of-the-art language model that's been fine-tuned for instruction-following tasks like summarization.\n",
        "\n",
        "## About FLAN-T5-Base üß†\n",
        "\n",
        "### **What is FLAN-T5?**\n",
        "- **FLAN**: Fine-tuned Language Net - Google's instruction-tuned model family\n",
        "- **T5**: Text-to-Text Transfer Transformer - treats all NLP tasks as text generation\n",
        "- **Base Size**: Balanced model with ~250M parameters (good performance vs. speed)\n",
        "\n",
        "### **Why This Model?**\n",
        "- ‚úÖ **Excellent Summarization**: Specifically trained for text summarization tasks\n",
        "- ‚úÖ **Instruction Following**: Understands natural language prompts\n",
        "- ‚úÖ **Efficient Size**: Fast enough for real-time processing\n",
        "- ‚úÖ **Open Source**: Free to use and well-documented\n",
        "\n",
        "## Your Task:\n",
        "Complete the model loading code by filling in the blanks. Pay attention to the two main components we need!"
      ],
      "metadata": {
        "id": "HgUj05QJuivN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries for model loading\n",
        "from transformers import _______, _______  # Hint: AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# Check if GPU is available for faster processing\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
        "\n",
        "# Define the model name - Google's FLAN-T5-Base\n",
        "model_name = \"_______\"  # Hint: \"google/flan-t5-base\" | \"openai/gpt-3\" | \"facebook/bart-base\"\n",
        "print(f\"ü§ñ Loading model: {model_name}\")\n",
        "\n",
        "# Step 1: Load the tokenizer\n",
        "print(\"üìù Loading tokenizer...\")\n",
        "# Hint: Use AutoTokenizer.from_pretrained() with the model name\n",
        "tokenizer = _______.from_pretrained(_______)\n",
        "print(\"‚úÖ Tokenizer loaded successfully!\")\n",
        "\n",
        "# Step 2: Load the model\n",
        "print(\"üß† Loading model (this may take a moment)...\")\n",
        "# Hint: Use AutoModelForSeq2SeqLM.from_pretrained() with the model name\n",
        "model = _______.from_pretrained(_______)\n",
        "print(\"‚úÖ Model loaded successfully!\")\n",
        "\n",
        "# Step 3: Move model to appropriate device (GPU if available)\n",
        "model = model.to(_____)  # Hint: Move to our device variable\n",
        "print(f\"üìç Model moved to {device}\")\n",
        "\n",
        "# Display model information\n",
        "print(f\"\\nüìä MODEL INFORMATION:\")\n",
        "print(f\"üìù Tokenizer vocabulary size: {_______.vocab_size:,}\")  # Hint: tokenizer | model | device\n",
        "print(f\"üß† Model parameters: ~250M\")\n",
        "print(f\"üìè Max input length: {_______.model_max_length:,} tokens\")  # Hint: tokenizer | model | device\n",
        "\n",
        "# Test tokenizer with a sample\n",
        "sample_text = \"This is a test sentence to verify our tokenizer is working correctly.\"\n",
        "# Hint: Use tokenizer.encode() to convert text to tokens\n",
        "tokens = _______.encode(sample_text)\n",
        "print(f\"\\nüîç TOKENIZER TEST:\")\n",
        "print(f\"Input: {sample_text}\")\n",
        "print(f\"Tokens: {len(_____)} tokens\")  # Hint: Count the tokens we just created\n",
        "print(f\"‚úÖ Tokenizer working correctly!\")\n",
        "\n",
        "print(f\"\\nüéâ Summarization model ready for use!\")\n",
        "\n",
        "# üí° LEARNING NOTES:\n",
        "# - AutoTokenizer converts text into numerical tokens that models can understand\n",
        "# - AutoModelForSeq2SeqLM is designed for sequence-to-sequence tasks like summarization\n",
        "# - from_pretrained() downloads and loads pre-trained models from Hugging Face\n",
        "# - Moving models to GPU (if available) significantly speeds up processing\n",
        "# - Testing the tokenizer ensures everything loaded correctly before proceeding"
      ],
      "metadata": {
        "id": "PvJ4OBksugIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Step 6: Define a Summarization Function\n",
        "\n",
        "Now we'll create a function that takes a text chunk and returns a concise summary. This function will be the core of our document summarization system!\n",
        "\n",
        "## How Our Summarization Works üîÑ\n",
        "\n",
        "### **The Process:**\n",
        "1. **üìù Prompt Creation**: We give the model clear instructions\n",
        "2. **üî¢ Tokenization**: Convert text to numbers the model understands  \n",
        "3. **üß† Generation**: Model creates a summary based on our prompt\n",
        "4. **üìñ Decoding**: Convert model output back to readable text\n",
        "\n",
        "### **Key Parameters Explained:**\n",
        "\n",
        "#### **üéØ Generation Settings:**\n",
        "- **`max_new_tokens=100`**: Limits summary to ~100 tokens (roughly 75-80 words)\n",
        "- **`do_sample=True`**: Enables creative, varied outputs (vs. always picking most likely words)\n",
        "- **`temperature=0.9`**: Controls creativity (0.0=boring, 1.0=creative, 2.0=chaotic)\n",
        "- **`repetition_penalty=1.1`**: Prevents the model from repeating phrases\n",
        "\n",
        "#### **üîß Technical Settings:**\n",
        "- **`return_tensors=\"pt\"`**: Returns PyTorch tensors (compatible with our model)\n",
        "- **`truncation=True`**: Cuts off text if it's too long for the model\n",
        "- **`padding=True`**: Ensures consistent input sizes for batch processing\n",
        "- **`skip_special_tokens=True`**: Removes technical tokens from final output\n",
        "\n",
        "### **Why This Approach?**\n",
        "- ‚úÖ **Clear Instructions**: The prompt tells the model exactly what we want\n",
        "- ‚úÖ **Consistent Quality**: Same prompt ensures similar summary styles\n",
        "- ‚úÖ **Optimal Length**: 3 sentences provide good detail without being too long\n",
        "- ‚úÖ **Flexible Output**: Temperature allows for natural, varied summaries\n",
        "\n",
        "## Your Task:\n",
        "Complete the summarization function by filling in the blanks. Pay attention to the process steps!"
      ],
      "metadata": {
        "id": "06utH9_Wut1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(text):\n",
        "    \"\"\"\n",
        "    Generate a concise summary of the input text using FLAN-T5.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text chunk to summarize\n",
        "\n",
        "    Returns:\n",
        "        str: A 3-sentence summary of the input text\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Create a clear prompt for the model\n",
        "    # Hint: Tell the model to \"Summarize this concisely in 3 sentences:\" followed by the text\n",
        "    prompt = f\"_______ _______ _______ _______ _______ _______:\\n{_____}\"\n",
        "    #         \"Summarize this concisely in 3 sentences\"          text\n",
        "\n",
        "    # Step 2: Tokenize the prompt\n",
        "    # Hint: Use our tokenizer with return_tensors=\"pt\", truncation=True, padding=True\n",
        "    inputs = _______(\n",
        "        prompt,\n",
        "        return_tensors=\"_____\",     # Hint: \"pt\" | \"tf\" | \"np\"\n",
        "        truncation=_____,           # Hint: True | False\n",
        "        padding=_____,              # Hint: True | False\n",
        "        max_length=512\n",
        "    ).to(_____)  # Hint: Move to our device variable\n",
        "\n",
        "    # Step 3: Generate summary using the model\n",
        "    with torch.no_grad():  # Disable gradient calculation for faster inference\n",
        "        outputs = _______.generate(  # Hint: Use our loaded model\n",
        "            **inputs,                    # Pass our tokenized input\n",
        "            max_new_tokens=_____,       # Hint: Limit to ~100 tokens\n",
        "            do_sample=_____,            # Hint: True for variety | False for consistency\n",
        "            temperature=_____,          # Hint: 0.9 for balanced creativity\n",
        "            repetition_penalty=_____,   # Hint: 1.1 to reduce repetition\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Step 4: Decode the generated tokens back to text\n",
        "    # Hint: Use tokenizer.decode() with skip_special_tokens=True\n",
        "    summary = _______.decode(\n",
        "        outputs[0],\n",
        "        skip_special_tokens=_____    # Hint: True | False - remove technical tokens?\n",
        "    )\n",
        "\n",
        "    # Step 5: Clean up the output\n",
        "    # Remove the original prompt from the generated text if it appears\n",
        "    if \"Summarize this concisely\" in summary:\n",
        "        summary = summary.split(\"Summarize this concisely in 3 sentences:\")[-1].strip()\n",
        "\n",
        "    return _____  # Hint: Return our cleaned summary\n",
        "\n",
        "# Test the function with a sample chunk\n",
        "print(\"üß™ Testing summarization function...\")\n",
        "if len(_____) > 0:  # Hint: Check if we have chunks available\n",
        "    sample_chunk = _____[0]  # Hint: Get the first chunk\n",
        "    print(f\"\\nüìñ Original text ({len(sample_chunk)} chars):\")\n",
        "    print(\"-\" * 50)\n",
        "    print(sample_chunk[:300] + \"...\" if len(sample_chunk) > 300 else sample_chunk)\n",
        "\n",
        "    print(f\"\\nüìù Generated Summary:\")\n",
        "    print(\"-\" * 50)\n",
        "    # Hint: Call our summarize function with the sample chunk\n",
        "    sample_summary = _______(sample_chunk)\n",
        "    print(sample_summary)\n",
        "\n",
        "    print(f\"\\nüìä Compression Stats:\")\n",
        "    print(f\"üìè Original: {len(_____)} characters\")  # Hint: sample_chunk | summary | text\n",
        "    print(f\"üìè Summary: {len(_____)} characters\")   # Hint: sample_summary | chunk | output\n",
        "    # Hint: Calculate compression ratio by dividing original length by summary length\n",
        "    print(f\"üìà Compression ratio: {len(_____)/len(_____):.1f}:1\")\n",
        "    print(f\"‚úÖ Summarization function working correctly!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No chunks available for testing\")\n",
        "\n",
        "# üí° LEARNING NOTES:\n",
        "# - Function definitions use def keyword followed by function name and parameters\n",
        "# - f-strings allow us to embed variables directly in text using {variable}\n",
        "# - torch.no_grad() context manager improves performance during inference\n",
        "# - The model.generate() method is where the AI magic happens\n",
        "# - String cleaning ensures our output is properly formatted\n",
        "# - Testing functions with sample data is crucial before processing large datasets"
      ],
      "metadata": {
        "id": "hoMcf5U8ulKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚≠ê Step 7: Summarize the Full Text\n",
        "\n",
        "Now let's try something interesting - what happens when we summarize the ENTIRE document in one go? This experiment will help us understand the limitations and challenges of working with large language models.\n",
        "\n",
        "## ü§î The Big Question: Context Window Limitations\n",
        "\n",
        "### **What's a Context Window?**\n",
        "A **context window** is the maximum amount of text an AI model can process at once. Think of it like the model's \"working memory\" - it can only \"see\" and \"remember\" a limited amount of text.\n",
        "\n",
        "### **FLAN-T5-Base Specifications:**\n",
        "- **üìè Maximum Input**: ~512 tokens (roughly 400-500 words)\n",
        "- **üß† Context Limit**: Cannot process documents longer than this limit\n",
        "- **‚ö†Ô∏è Truncation Risk**: Long texts get cut off, losing important information\n",
        "\n",
        "### **What Happens When Text is Too Long?**\n",
        "\n",
        "#### **üîÑ Automatic Truncation:**\n",
        "- Model automatically cuts off text after 512 tokens\n",
        "- **Lost Information**: Important content at the end gets ignored\n",
        "- **Incomplete Context**: Model only sees the beginning of the document\n",
        "- **Biased Summaries**: Results may not represent the full document\n",
        "\n",
        "#### **üìä Quality Implications:**\n",
        "- ‚úÖ **Works Well**: Short documents, single topics\n",
        "- ‚ö†Ô∏è **Problematic**: Long documents, multiple topics, important conclusions at the end\n",
        "- ‚ùå **Fails**: Very long documents where key points are distributed throughout\n",
        "\n",
        "### **Why This Experiment Matters:**\n",
        "1. **üß™ Educational**: See firsthand how context limits affect AI performance\n",
        "2. **üìà Comparison**: Compare single-shot vs. chunk-based approaches\n",
        "3. **üéØ Understanding**: Learn when each approach is appropriate\n",
        "4. **üîç Analysis**: Observe what information gets lost in truncation\n",
        "\n",
        "### **Real-World Implications:**\n",
        "- **üìö Academic Papers**: Often too long for single-pass summarization\n",
        "- **üìÑ Legal Documents**: Critical details might be at the end\n",
        "- **üì∞ News Articles**: Lead vs. conclusion information balance\n",
        "- **üìñ Books/Reports**: Require chunk-based processing for comprehensive summaries"
      ],
      "metadata": {
        "id": "k4HderO1uyiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üß™ Experiment: Summarize the entire document at once\n",
        "print(\"üß™ EXPERIMENT: Full Document Summarization\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check the size of our full text\n",
        "print(f\"üìä Full document statistics:\")\n",
        "print(f\"üìè Total characters: {len(_____):,}\")  # Hint: Our extracted text variable\n",
        "# Hint: Estimate tokens by counting words and multiplying by 1.3\n",
        "print(f\"üìù Estimated tokens: {len(_____.split()) * 1.3:.0f}\")\n",
        "print(f\"‚ö†Ô∏è  Model limit: ~512 tokens\")\n",
        "print()\n",
        "\n",
        "# Analyze what will happen\n",
        "estimated_tokens = len(_____.split()) * 1.3  # Hint: Same text variable as above\n",
        "if estimated_tokens > 500:\n",
        "    print(\"‚ö†Ô∏è  WARNING: Document likely exceeds model's context window!\")\n",
        "    print(\"üìâ Expected behavior: Text will be truncated\")\n",
        "    print(\"üéØ This demonstrates the importance of chunking strategy\")\n",
        "else:\n",
        "    print(\"‚úÖ Document should fit within context window\")\n",
        "\n",
        "print(\"\\nüîÑ Generating full document summary...\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Generate the summary\n",
        "# Hint: Use our summarize function with the full text\n",
        "abstractive_full_summary = _____(_____)\n",
        "\n",
        "# Display the result\n",
        "print(\"üìñ FULL DOCUMENT SUMMARY:\")\n",
        "print(\"=\" * 40)\n",
        "print(_____)  # Hint: Print our summary variable\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Analysis of the result\n",
        "print(f\"\\nüìä SUMMARY ANALYSIS:\")\n",
        "print(f\"üìè Summary length: {len(_____)} characters\")  # Hint: Count summary characters\n",
        "print(f\"üìù Summary words: {len(_____.split())} words\")  # Hint: Count summary words\n",
        "# Hint: Calculate compression by dividing original length by summary length\n",
        "print(f\"üìà Compression ratio: {len(_____)/len(_____):.1f}:1\")\n",
        "\n",
        "# Critical thinking questions\n",
        "print(f\"\\nü§î CRITICAL ANALYSIS:\")\n",
        "print(\"Questions to consider:\")\n",
        "print(\"‚Ä¢ Does this summary capture the main points from throughout the document?\")\n",
        "print(\"‚Ä¢ What information might be missing from the end of the document?\")\n",
        "print(\"‚Ä¢ How does this compare to what you'd expect from reading the full text?\")\n",
        "print(\"‚Ä¢ Would a chunk-based approach provide better coverage?\")\n",
        "\n",
        "# Preview what got truncated (if anything)\n",
        "if estimated_tokens > 500:\n",
        "    print(f\"\\n‚ö†Ô∏è  TRUNCATION ANALYSIS:\")\n",
        "    print(\"The model likely only processed the first ~400-500 words.\")\n",
        "    print(\"Here's roughly where the truncation occurred:\")\n",
        "    print(\"-\" * 30)\n",
        "    truncation_point = int(len(full_text) * 0.3)  # Rough estimate\n",
        "    print(f\"Last ~100 chars the model saw: ...{full_text[truncation_point-100:truncation_point]}\")\n",
        "    print(f\"First ~100 chars it missed: {full_text[truncation_point:truncation_point+100]}...\")\n",
        "\n",
        "# üí° LEARNING NOTES:\n",
        "# - Context windows limit how much text AI models can process at once\n",
        "# - Truncation happens automatically when text exceeds the model's capacity\n",
        "# - Information at the end of long documents may be completely ignored\n",
        "# - This experiment shows why chunking is often necessary for long documents\n",
        "# - Different models have different context window sizes (512 for FLAN-T5-Base)\n",
        "# - Understanding these limitations is crucial for building effective AI applications"
      ],
      "metadata": {
        "id": "p3M8Mjc1uwji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìè ROUGE Metric Comparison\n",
        "\n",
        "To evaluate how well our generated summaries capture key content, we'll use the **ROUGE metric** (Recall-Oriented Understudy for Gisting Evaluation). This is the gold standard for evaluating automatic summarization systems!\n",
        "\n",
        "## üéØ What is ROUGE?\n",
        "\n",
        "**ROUGE** compares the overlap between a generated summary and a reference summary using different text analysis methods:\n",
        "\n",
        "### **üìä ROUGE Variants:**\n",
        "\n",
        "#### **üî§ ROUGE-1: Single Word Overlap (Unigrams)**\n",
        "- **What it measures**: How many individual words appear in both summaries\n",
        "- **Good for**: Overall content coverage and vocabulary overlap\n",
        "- **Example**: \"The cat sat\" vs \"A cat was sitting\" ‚Üí shares \"cat\"\n",
        "\n",
        "#### **üîó ROUGE-2: Word Pair Overlap (Bigrams)**\n",
        "- **What it measures**: How many 2-word sequences appear in both summaries\n",
        "- **Good for**: Phrase-level similarity and word order preservation\n",
        "- **Example**: \"machine learning\" as a complete phrase vs. separate words\n",
        "\n",
        "#### **üìè ROUGE-L: Longest Common Subsequence**\n",
        "- **What it measures**: Longest sequence of words that appear in the same order\n",
        "- **Good for**: Structural similarity and sentence flow\n",
        "- **Example**: Preserves the logical flow of ideas between summaries\n",
        "\n",
        "### **üìà ROUGE Scores Explained:**\n",
        "\n",
        "#### **üéØ Precision**:\n",
        "- What percentage of words in the generated summary appear in the reference?\n",
        "- **High Precision** = Generated summary doesn't add irrelevant content\n",
        "\n",
        "#### **üîç Recall**:\n",
        "- What percentage of words in the reference appear in the generated summary?\n",
        "- **High Recall** = Generated summary captures most important content\n",
        "\n",
        "#### **‚öñÔ∏è F1-Score**:\n",
        "- Harmonic mean of precision and recall (balanced measure)\n",
        "- **High F1** = Good balance between completeness and relevance\n",
        "\n",
        "### **üé® Why Use ROUGE for Summarization?**\n",
        "- ‚úÖ **Objective Evaluation**: Provides quantitative quality measurements\n",
        "- ‚úÖ **Industry Standard**: Used in research and production systems\n",
        "- ‚úÖ **Multi-faceted**: Captures different aspects of summary quality\n",
        "- ‚úÖ **Comparative**: Allows comparison between different summarization approaches\n",
        "- ‚úÖ **Automated**: Can evaluate large numbers of summaries quickly\n",
        "\n",
        "### **‚ö†Ô∏è ROUGE Limitations:**\n",
        "- Requires reference summaries (human-written gold standards)\n",
        "- Focuses on word overlap, not semantic meaning\n",
        "- May not capture creative or paraphrased summaries well\n",
        "- Higher scores don't always mean better human-perceived quality"
      ],
      "metadata": {
        "id": "Rk6osScMiDXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è Setup: Import and Define ROUGE Scoring Function\n",
        "\n",
        "We‚Äôll define a simple helper function to display precision, recall, and F1 for each ROUGE score.\n"
      ],
      "metadata": {
        "id": "Utpw9nrKTbSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üîß Setup: Import and Define ROUGE Scoring Function\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "def compare_rouge(hypothesis, reference):\n",
        "    \"\"\"\n",
        "    Compare a generated summary (hypothesis) with a reference summary using ROUGE metrics.\n",
        "\n",
        "    Args:\n",
        "        hypothesis (str): The AI-generated summary to evaluate\n",
        "        reference (str): The human-written reference summary (ground truth)\n",
        "\n",
        "    Returns:\n",
        "        None: Prints formatted ROUGE scores\n",
        "    \"\"\"\n",
        "    # Initialize ROUGE scorer with all three metrics and stemming\n",
        "    scorer = rouge_scorer.RougeScorer(\n",
        "        ['rouge1', 'rouge2', 'rougeL'],\n",
        "        use_stemmer=True  # Reduces words to stems (e.g., \"running\" ‚Üí \"run\")\n",
        "    )\n",
        "\n",
        "    # Calculate scores by comparing reference to hypothesis\n",
        "    # Hint: Use scorer.score() with reference first, then hypothesis\n",
        "    scores = scorer.score(_______, _______)\n",
        "\n",
        "    # Display results in a clear, formatted way\n",
        "    print(\"üìä ROUGE EVALUATION RESULTS:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for metric_name, score_values in scores.items():\n",
        "        print(f\"\\nüîç {metric_name.upper()}:\")\n",
        "        # Hint: Access precision, recall, and fmeasure from score_values\n",
        "        print(f\"   üìà Precision: {_______.precision:.4f}\")\n",
        "        print(f\"   üìâ Recall:    {_______.recall:.4f}\")\n",
        "        print(f\"   ‚öñÔ∏è  F1-Score:  {_______.fmeasure:.4f}\")\n",
        "\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Provide interpretation guidance\n",
        "    print(\"\\nüí° INTERPRETATION GUIDE:\")\n",
        "    print(\"üìà Precision: How much of the generated content is relevant?\")\n",
        "    print(\"üìâ Recall: How much of the reference content was captured?\")\n",
        "    print(\"‚öñÔ∏è  F1-Score: Overall balanced quality measure\")\n",
        "    print(\"\\nüìä Score Ranges: 0.0 (worst) ‚Üí 1.0 (perfect match)\")\n",
        "\n",
        "print(\"‚úÖ ROUGE comparison function ready for use!\")"
      ],
      "metadata": {
        "id": "wFIVStjWjw25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìÑ Reference Summary for Comparison\n",
        "\n",
        "Now we'll define a manually written summary to use as our **ground truth** and compare our AI-generated summaries against it. This reference summary represents what we consider to be a high-quality summary of the document.\n",
        "\n",
        "## üéØ Why We Need a Reference Summary:\n",
        "- **üìè Evaluation Standard**: Provides a benchmark for comparison\n",
        "- **üéì Quality Control**: Helps us assess how well our AI performs\n",
        "- **üìä Objective Metrics**: Enables quantitative evaluation using ROUGE\n",
        "- **üîç Analysis**: Allows us to identify strengths and weaknesses\n",
        "\n",
        "## üìù Creating Good Reference Summaries:\n",
        "- **Comprehensive**: Covers main points from throughout the document\n",
        "- **Concise**: Removes unnecessary details while preserving key information\n",
        "- **Accurate**: Faithfully represents the original content\n",
        "- **Well-written**: Uses clear, coherent language"
      ],
      "metadata": {
        "id": "lsiWH3pbkIGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üìù Define a manually written reference summary for comparison\n",
        "reference_summary = \"\"\"\n",
        "Write your reference summary here. You can either do it manually, or ask GPT-4 to generate a summary. (We call the latter 'LLM-as-a-Judge')\n",
        "\"\"\"\n",
        "\n",
        "print(\"üìÑ REFERENCE SUMMARY DEFINED:\")\n",
        "print(\"=\" * 50)\n",
        "print(reference_summary)\n",
        "print(\"=\" * 50)\n",
        "print(f\"üìè Reference length: {len(reference_summary)} characters\")\n",
        "print(f\"üìù Reference words: {len(reference_summary.split())} words\")\n",
        "print(\"\\n‚úÖ Reference summary ready for ROUGE comparison!\")"
      ],
      "metadata": {
        "id": "cFHUFaLjkHDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß™ Apply ROUGE Comparison\n",
        "\n",
        "Now let's put our ROUGE evaluation to work! We'll compare our AI-generated full document summary against our manually written reference summary to see how well our model performed.\n",
        "\n",
        "## üîç What We're Testing:\n",
        "- **Quality Assessment**: How well does our AI capture the key points?\n",
        "- **Content Coverage**: Does it miss important information?\n",
        "- **Precision vs Recall**: Is it accurate vs comprehensive?\n",
        "- **Overall Performance**: Should we adjust our approach?"
      ],
      "metadata": {
        "id": "fmPNPUKpxUn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üß™ Compare our AI-generated summary with the reference summary\n",
        "print(\"üß™ EVALUATING OUR AI-GENERATED SUMMARY:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"üìñ Our AI-Generated Summary:\")\n",
        "print(\"-\" * 40)\n",
        "print(abstractive_full_summary)\n",
        "\n",
        "print(\"\\nüìÑ Reference Summary (Ground Truth):\")\n",
        "print(\"-\" * 40)\n",
        "print(reference_summary)\n",
        "\n",
        "# Apply ROUGE evaluation\n",
        "# Hint: Use our compare_rouge function with AI summary first, then reference\n",
        "_______(_______, _______)\n",
        "\n",
        "# Additional analysis\n",
        "print(f\"\\nüìà SUMMARY COMPARISON:\")\n",
        "print(f\"üìè AI Summary length: {len(abstractive_full_summary)} characters\")\n",
        "print(f\"üìè Reference length: {len(reference_summary)} characters\")\n",
        "# Hint: Calculate ratio by dividing AI length by reference length\n",
        "print(f\"üìä Length ratio: {len(_____)/len(_____):.2f}:1\")\n",
        "\n",
        "# Interpretation help\n",
        "print(f\"\\nü§î ANALYSIS QUESTIONS:\")\n",
        "print(\"‚Ä¢ Which summary better captures the paper's main contributions?\")\n",
        "print(\"‚Ä¢ What key information might be missing from the AI summary?\")\n",
        "print(\"‚Ä¢ How do the ROUGE scores reflect the quality differences you observe?\")\n",
        "print(\"‚Ä¢ Would chunked summarization potentially perform better?\")"
      ],
      "metadata": {
        "id": "WSLXr6J9xY0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîÑ Let's Try Different Summarization Methods\n",
        "\n",
        "Now that we've explored abstractive summarization with our full document, let's experiment with different approaches to see how they compare! Understanding various summarization techniques will help us choose the best method for different scenarios.\n",
        "\n",
        "## üéØ Why Compare Different Methods?\n",
        "\n",
        "### **üìä Method Comparison Benefits:**\n",
        "- **üîç Understanding Strengths**: Each method excels in different scenarios\n",
        "- **üìà Performance Analysis**: Compare quality, speed, and resource usage\n",
        "- **üé® Approach Diversity**: Abstractive vs. extractive vs. hybrid methods\n",
        "- **üõ†Ô∏è Tool Selection**: Learn when to use which technique\n",
        "\n",
        "### **üß™ What We'll Explore:**\n",
        "\n",
        "#### **üìù Extractive Summarization**\n",
        "- **How it works**: Selects and combines existing sentences from the original text\n",
        "- **Advantages**: Preserves original wording, factually accurate, faster processing\n",
        "- **Best for**: News articles, formal documents, when precision is critical\n",
        "- **Limitations**: Can sound choppy, may miss nuanced connections\n",
        "\n",
        "#### **üß† Abstractive Summarization** (What we just did)\n",
        "- **How it works**: Generates new text that captures the essence of the original\n",
        "- **Advantages**: More natural language, can rephrase and synthesize\n",
        "- **Best for**: Creative content, when readability is important\n",
        "- **Limitations**: May introduce errors, requires more computational power\n",
        "\n",
        "#### **‚ö° Hybrid Approaches**\n",
        "- **How it works**: Combines extractive and abstractive techniques\n",
        "- **Advantages**: Balances accuracy with readability\n",
        "- **Best for**: Complex documents, when you need both precision and flow"
      ],
      "metadata": {
        "id": "IVgVxI6tNFis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üé® Extractive Summarization\n",
        "\n",
        "Let's implement a simple but effective extractive summarization approach that selects the most important sentences from our document.\n",
        "\n",
        "### üîç How Extractive Summarization Works\n",
        "\n",
        "Unlike abstractive summarization that generates new text, **extractive summarization** acts like a smart highlighter - it identifies and selects the most important sentences from the original document without changing them.\n",
        "\n",
        "#### **üßÆ The Algorithm Steps:**\n",
        "\n",
        "1. **üìù Sentence Tokenization**: Split the document into individual sentences\n",
        "2. **üîß Preprocessing**: Remove very short sentences (fragments) that don't contain meaningful information\n",
        "3. **üìä TF-IDF Scoring**: Calculate importance scores for each sentence\n",
        "4. **üìç Position Weighting**: Give higher importance to sentences appearing earlier in the document\n",
        "5. **üèÜ Selection**: Pick the top-scoring sentences while preserving their original order\n",
        "\n",
        "#### **üî¢ TF-IDF: The Heart of Sentence Scoring**\n",
        "\n",
        "**TF-IDF** (Term Frequency-Inverse Document Frequency) helps us identify which sentences contain the most important information:\n",
        "\n",
        "- **üìà Term Frequency (TF)**: How often important words appear in each sentence\n",
        "- **üìâ Inverse Document Frequency (IDF)**: How rare/unique those words are across all sentences\n",
        "- **üéØ Combined Score**: Sentences with frequent important words AND rare key terms score highest\n",
        "\n",
        "#### **üìç Why Position Matters**\n",
        "\n",
        "Research shows that in most documents (especially academic papers and news articles), the most important information often appears early. Our algorithm applies **position weighting**:\n",
        "\n",
        "- **First sentences**: Get full weight (1.0)\n",
        "- **Later sentences**: Get progressively lower weights (down to 0.5)\n",
        "- **Result**: Early sentences with good content beat later sentences with similar content\n",
        "\n",
        "#### **‚öñÔ∏è Final Sentence Selection**\n",
        "\n",
        "The algorithm combines:\n",
        "- **Content importance** (TF-IDF scores)\n",
        "- **Position importance** (earlier = better)\n",
        "- **Quality filtering** (removes sentence fragments)\n",
        "\n",
        "Then selects the top N sentences while **preserving their original order** for natural reading flow.\n",
        "\n",
        "### üÜö Extractive vs. Abstractive Comparison\n",
        "\n",
        "| Aspect | Extractive | Abstractive |\n",
        "|--------|------------|-------------|\n",
        "| **Accuracy** | ‚úÖ High (uses original words) | ‚ö†Ô∏è Can introduce errors |\n",
        "| **Fluency** | ‚ö†Ô∏è May sound choppy | ‚úÖ Natural, flowing text |\n",
        "| **Speed** | ‚úÖ Fast processing | ‚ö†Ô∏è Slower, more complex |\n",
        "| **Creativity** | ‚ùå Cannot rephrase | ‚úÖ Can synthesize ideas |\n",
        "| **Factual Safety** | ‚úÖ Preserves exact wording | ‚ö†Ô∏è May alter meanings |"
      ],
      "metadata": {
        "id": "H7Jce69OKeB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "def extractive_summarization(text, num_sentences=5):\n",
        "    \"\"\"\n",
        "    Extract the most important sentences from text using TF-IDF scoring\n",
        "    \"\"\"\n",
        "    # Split into sentences\n",
        "    # Hint: Use nltk.sent_tokenize() to split text into sentences\n",
        "    sentences = nltk._______(text)\n",
        "\n",
        "    # Consider adding sentence length filtering\n",
        "    # Hint: Keep sentences with more than 5 words using len(s.split()) > 5\n",
        "    sentences = [s for s in sentences if len(s._______()) > 5]  # Remove very short sentences\n",
        "\n",
        "    if len(sentences) <= num_sentences:\n",
        "        return \" \".join(sentences)\n",
        "\n",
        "    # Create TF-IDF vectorizer\n",
        "    # Hint: Use TfidfVectorizer with stop_words='english' and lowercase=True\n",
        "    vectorizer = _______(stop_words='english', lowercase=True)\n",
        "\n",
        "    # Fit and transform sentences\n",
        "    try:\n",
        "        # Hint: Use vectorizer.fit_transform() to create the TF-IDF matrix\n",
        "        tfidf_matrix = vectorizer._______(sentences)\n",
        "\n",
        "        # Calculate sentence scores (sum of TF-IDF scores)\n",
        "        # Hint: Use np.array() and .sum(axis=1) to sum TF-IDF scores for each sentence\n",
        "        sentence_scores = np.array(tfidf_matrix.sum(axis=_______)).flatten()\n",
        "\n",
        "        # Optional: Add position weighting (earlier sentences often more important)\n",
        "        # Hint: Use np.linspace() to create weights from 1.0 to 0.5\n",
        "        position_weights = np.linspace(_______, _______, len(sentences))\n",
        "        sentence_scores = sentence_scores * position_weights\n",
        "\n",
        "        # Get top sentences\n",
        "        # Hint: Use .argsort() to get indices, then [-num_sentences:] for top scores\n",
        "        top_indices = sentence_scores._______()[-num_sentences:][::-1]\n",
        "        top_indices = sorted(top_indices)  # Keep original order\n",
        "\n",
        "        # Extract top sentences\n",
        "        summary_sentences = [sentences[i] for i in top_indices]\n",
        "        return \" \".join(summary_sentences)\n",
        "\n",
        "    except ValueError:\n",
        "        # Fallback: return first few sentences if TF-IDF fails\n",
        "        return \" \".join(sentences[:num_sentences])\n",
        "\n",
        "# Generate extractive summary\n",
        "# Hint: Call our function with full_text and num_sentences=5\n",
        "full_extractive_summary = _______(_______, num_sentences=5)"
      ],
      "metadata": {
        "id": "-LBn6qgsKh7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìñ View Our Extractive Summary\n",
        "\n",
        "Now let's see what our extractive summarization algorithm produced! This summary consists of the 5 most important sentences selected from the original document, preserving their exact wording and original order.\n",
        "\n",
        "## üîç What to Look For:\n",
        "\n",
        "### **üìä Content Analysis:**\n",
        "- **Key Topics**: Does it capture the main themes from the document?\n",
        "- **Important Details**: Are critical facts and findings included?\n",
        "- **Completeness**: Does it feel like a comprehensive overview?\n",
        "\n",
        "### **üìù Quality Assessment:**\n",
        "- **Coherence**: Do the selected sentences flow well together?\n",
        "- **Factual Accuracy**: All content is guaranteed to be from the original (no hallucinations!)\n",
        "- **Readability**: Is it easy to understand and follow?\n",
        "\n",
        "### **üÜö Comparison Points:**\n",
        "- **vs. Abstractive**: How does this compare to our AI-generated summary?\n",
        "- **vs. Reference**: How well does it match our manual reference summary?\n",
        "- **Content Coverage**: What information is preserved vs. lost?\n",
        "\n",
        "## üéØ Remember:\n",
        "Extractive summaries excel at **preserving exact wording** and **maintaining factual accuracy**, but may sometimes feel less fluid than abstractive summaries that can rephrase and connect ideas more naturally."
      ],
      "metadata": {
        "id": "0Ll2ldBP1rRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkout the Extractive summary text\n",
        "print(full_extractive_summary)"
      ],
      "metadata": {
        "id": "casWEol5K6iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets check the rouge scores\n",
        "\n",
        "compare_rouge(full_extractive_summary, reference_summary)"
      ],
      "metadata": {
        "id": "FtmOirSPPjG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üèÜ Multi-Stage (Hierarchical) Summarization\n",
        "\n",
        "Think of this like a **Champions League tournament** for text summarization! We'll summarize our text in rounds until we get one final champion summary.\n",
        "\n",
        "## üéØ Why Tournament Style?\n",
        "\n",
        "- **üìè Context Problem**: Our model only handles ~500 tokens at once\n",
        "- **üìö Large Documents**: We might have 5000+ tokens across chunks\n",
        "- **üîÑ Solution**: Summarize in rounds like a sports tournament!\n",
        "\n",
        "## üèüÔ∏è How It Works:\n",
        "\n",
        "1. **üîµ Round 1**: Summarize each chunk ‚Üí Get smaller summaries\n",
        "2. **üîç Check**: Do all summaries fit in context window?\n",
        "3. **üîÑ Round 2+**: If too big, summarize the summaries again\n",
        "4. **üèÜ Victory**: Repeat until one final summary fits"
      ],
      "metadata": {
        "id": "_5NdjY6dOWMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üèÜ Multi-Stage Summarization: Tournament Style!\n",
        "\n",
        "def hierarchical_summarization(chunks, max_context_tokens=500, target_summary_tokens=100):\n",
        "    \"\"\"\n",
        "    Summarize text in tournament-style rounds until we get a final summary.\n",
        "\n",
        "    Args:\n",
        "        chunks (list): List of text chunks to summarize\n",
        "        max_context_tokens (int): Maximum tokens our model can handle\n",
        "        target_summary_tokens (int): Target length for each summary\n",
        "\n",
        "    Returns:\n",
        "        str: Final hierarchical summary\n",
        "    \"\"\"\n",
        "\n",
        "    def estimate_tokens(text):\n",
        "        \"\"\"Simple token estimation: ~1.3 tokens per word\"\"\"\n",
        "        return int(len(text.split()) * 1.3)\n",
        "\n",
        "    def can_fit_in_context(summaries, max_tokens):\n",
        "        \"\"\"Check if all summaries together fit in context window\"\"\"\n",
        "        total_text = \"\\n\".join(summaries)\n",
        "        return estimate_tokens(total_text) <= max_tokens\n",
        "\n",
        "    # üèüÔ∏è Tournament Setup\n",
        "    print(\"üèÜ STARTING SUMMARIZATION TOURNAMENT!\")\n",
        "    print(\"‚ïê\" * 50)\n",
        "\n",
        "    current_round = 1\n",
        "    current_texts = chunks.copy()\n",
        "\n",
        "    # üîÑ Tournament Loop: Keep playing until we have a champion!\n",
        "    while len(current_texts) > 1:\n",
        "        print(f\"\\nüîµ ROUND {current_round}: Processing {len(current_texts)} texts\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        round_summaries = []\n",
        "\n",
        "        # üéØ If we have too many texts, group them before summarizing\n",
        "        if len(current_texts) > 20:\n",
        "            print(f\"   üì¶ Grouping {len(current_texts)} texts into batches for efficiency...\")\n",
        "            # Group texts into batches that fit in context window\n",
        "            batch_size = 5  # Process 5 texts at a time\n",
        "            for i in range(0, len(current_texts), batch_size):\n",
        "                batch = current_texts[i:i+batch_size]\n",
        "                batch_text = \"\\n\".join(batch)\n",
        "\n",
        "                print(f\"   ‚öΩ Batch {i//batch_size + 1}: Summarizing {len(batch)} texts ({estimate_tokens(batch_text)} tokens)...\")\n",
        "\n",
        "                if estimate_tokens(batch_text) <= max_context_tokens:\n",
        "                    summary = summarize(batch_text)\n",
        "                    round_summaries.append(summary)\n",
        "                    print(f\"   ‚úÖ Result: {estimate_tokens(summary)} tokens\")\n",
        "                else:\n",
        "                    # If batch is still too big, process individually\n",
        "                    for j, text in enumerate(batch):\n",
        "                        summary = summarize(text)\n",
        "                        round_summaries.append(summary)\n",
        "        else:\n",
        "            # üìä Regular processing for smaller numbers\n",
        "            for i, text in enumerate(current_texts, 1):\n",
        "                print(f\"   ‚öΩ Match {i}: Summarizing text {i} ({estimate_tokens(text)} tokens)...\")\n",
        "\n",
        "                summary = summarize(text)\n",
        "                round_summaries.append(summary)\n",
        "\n",
        "                print(f\"   ‚úÖ Result: {estimate_tokens(summary)} tokens\")\n",
        "\n",
        "        # üîç Check tournament status\n",
        "        if len(round_summaries) == 1:\n",
        "            # üèÜ We have our champion!\n",
        "            print(f\"\\nüèÜ TOURNAMENT COMPLETE!\")\n",
        "            print(f\"üéâ Champion Summary: {estimate_tokens(round_summaries[0])} tokens\")\n",
        "            return round_summaries[0]\n",
        "        elif can_fit_in_context(round_summaries, max_context_tokens):\n",
        "            # üîÑ Final round: combine all summaries\n",
        "            print(f\"\\nüèÅ FINAL ROUND: Combining {len(round_summaries)} summaries\")\n",
        "            combined_text = \"\\n\".join(round_summaries)\n",
        "            print(f\"   üìä Combined length: {estimate_tokens(combined_text)} tokens\")\n",
        "\n",
        "            final_summary = summarize(combined_text)\n",
        "            print(f\"\\nüèÜ TOURNAMENT COMPLETE!\")\n",
        "            print(f\"üéâ Champion Summary: {estimate_tokens(final_summary)} tokens\")\n",
        "            return final_summary\n",
        "        else:\n",
        "            # üîÑ Need another round\n",
        "            total_tokens = sum(estimate_tokens(s) for s in round_summaries)\n",
        "            print(f\"   ‚ö†Ô∏è  Combined summaries: {total_tokens} tokens (limit: {max_context_tokens})\")\n",
        "            print(f\"   üîÑ Advancing to next round with {len(round_summaries)} texts...\")\n",
        "            current_texts = round_summaries\n",
        "            current_round += 1\n",
        "\n",
        "            # Safety check to prevent infinite loops\n",
        "            if current_round > 10:\n",
        "                print(\"‚ö†Ô∏è  Maximum rounds reached, returning best available summary\")\n",
        "                return \"\\n\".join(round_summaries[:3])  # Return first 3 summaries\n",
        "\n",
        "    # üèÜ Single text case\n",
        "    return current_texts[0]\n",
        "\n",
        "# üéÆ Start the Tournament!\n",
        "print(\"üéÆ LAUNCHING HIERARCHICAL SUMMARIZATION TOURNAMENT!\")\n",
        "print(\"‚öΩ Let's see how many rounds our document needs...\\n\")\n",
        "\n",
        "# üìä First, let's check our chunks and create appropriate sized chunks for the tournament\n",
        "print(\"üìä PREPARING FOR TOURNAMENT:\")\n",
        "print(f\"üìö Current chunks: {len(chunks)}\")\n",
        "print(f\"üìè Average chunk size: {sum(len(chunk) for chunk in chunks) // len(chunks)} characters\")\n",
        "\n",
        "# üîß Create smaller chunks if needed (aiming for ~400 characters ‚âà 300 tokens)\n",
        "tournament_chunks = []\n",
        "for i, chunk in enumerate(chunks):\n",
        "    estimated_tokens = int(len(chunk.split()) * 1.3)\n",
        "    if estimated_tokens > 400:  # If chunk is too big, split it further\n",
        "        # Split large chunk into smaller pieces\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', chunk)\n",
        "        small_chunk = \"\"\n",
        "        for sentence in sentences:\n",
        "            if len(small_chunk) + len(sentence) <= 400:\n",
        "                small_chunk += sentence + \" \"\n",
        "            else:\n",
        "                if small_chunk:\n",
        "                    tournament_chunks.append(small_chunk.strip())\n",
        "                small_chunk = sentence + \" \"\n",
        "        if small_chunk:\n",
        "            tournament_chunks.append(small_chunk.strip())\n",
        "    else:\n",
        "        tournament_chunks.append(chunk)\n",
        "\n",
        "print(f\"üèüÔ∏è Tournament-ready chunks: {len(tournament_chunks)}\")\n",
        "for i, chunk in enumerate(tournament_chunks[:3]):  # Show first 3\n",
        "    tokens = int(len(chunk.split()) * 1.3)\n",
        "    print(f\"   Chunk {i+1}: {tokens} estimated tokens\")\n",
        "\n",
        "hierarchical_summary = hierarchical_summarization(tournament_chunks, max_context_tokens=500, target_summary_tokens=100)\n",
        "\n",
        "# üìä Tournament Results\n",
        "print(\"\\n\" + \"‚ïê\" * 60)\n",
        "print(\"üìä FINAL TOURNAMENT RESULTS:\")\n",
        "print(\"‚ïê\" * 60)\n",
        "print(hierarchical_summary)\n",
        "print(\"‚ïê\" * 60)\n",
        "\n",
        "# üèÜ Victory Statistics\n",
        "print(f\"\\nüèÜ VICTORY STATISTICS:\")\n",
        "print(f\"üìè Final summary length: {len(hierarchical_summary)} characters\")\n",
        "print(f\"üéØ Estimated tokens: {int(len(hierarchical_summary.split()) * 1.3)}\")\n",
        "print(f\"üìö Original chunks processed: {len(chunks)}\")\n",
        "print(f\"‚úÖ Tournament summarization complete!\")"
      ],
      "metadata": {
        "id": "T55Pr7fYNS9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now assess the Rouge results\n",
        "compare_rouge(chunk_summaries, reference_summary)"
      ],
      "metadata": {
        "id": "Saxe_so1thkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Challenge Tracks: Take Your Summarization to the Next Level!\n",
        "\n",
        "Congratulations! You've built a complete document summarization system. Now it's time to push the boundaries and explore advanced techniques. Choose one or more tracks below to enhance your skills and improve results.\n",
        "\n",
        "## üéØ Track 1: Multi-Model Comparison Arena\n",
        "**Description**: Test different language models to find the best summarizer for your use case.\n",
        "\n",
        "### üîç What You'll Learn:\n",
        "- How different models handle the same content\n",
        "- Performance vs. quality trade-offs\n",
        "- Model selection strategies for production systems\n",
        "\n",
        "### üìã Implementation Outline:\n",
        "1. **Model Selection**: Choose 2-3 models (T5-small, BART, Pegasus, or GPT-based models)\n",
        "2. **Standardized Testing**: Run the same chunks through each model\n",
        "3. **ROUGE Comparison**: Evaluate all models against your reference summary\n",
        "4. **Speed Benchmarking**: Time each model's processing speed\n",
        "5. **Quality Analysis**: Compare output readability and accuracy\n",
        "6. **Recommendation**: Document which model works best for which scenarios\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Track 2: Hybrid Extractive-Abstractive Pipeline\n",
        "**Description**: Combine the best of both worlds - use extractive summarization to select important content, then abstractive to make it flow naturally.\n",
        "\n",
        "### üîç What You'll Learn:\n",
        "- How to chain different summarization approaches\n",
        "- When extraction vs. abstraction is more appropriate\n",
        "- Pipeline optimization techniques\n",
        "\n",
        "### üìã Implementation Outline:\n",
        "1. **Stage 1**: Use extractive summarization to identify top sentences\n",
        "2. **Content Filtering**: Remove redundant or low-quality extractions\n",
        "3. **Stage 2**: Apply abstractive summarization to extracted content\n",
        "4. **Quality Control**: Compare hybrid results vs. pure approaches\n",
        "5. **Parameter Tuning**: Experiment with extraction ratios (how much to extract before abstracting)\n",
        "6. **Evaluation**: Test hybrid approach against both pure methods using ROUGE\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Track 3: Domain-Specific Optimization\n",
        "**Description**: Customize your summarization system for specific document types (academic papers, news, legal documents, etc.).\n",
        "\n",
        "### üîç What You'll Learn:\n",
        "- How document structure affects summarization quality\n",
        "- Domain-specific prompt engineering\n",
        "- Specialized evaluation metrics\n",
        "\n",
        "### üìã Implementation Outline:\n",
        "1. **Document Analysis**: Identify unique features of your chosen domain\n",
        "2. **Custom Chunking**: Adapt chunking strategy for document structure (abstracts, conclusions, etc.)\n",
        "3. **Specialized Prompts**: Create domain-specific prompts for your model\n",
        "4. **Position Weighting**: Adjust importance of different document sections\n",
        "5. **Domain Metrics**: Develop evaluation criteria beyond ROUGE (factual accuracy, terminology preservation)\n",
        "6. **Validation**: Test with multiple documents from your chosen domain\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Track 4: Intelligent Chunk Clustering for Better Summarization\n",
        "**Description**: Use machine learning to group similar chunks together before summarization, ensuring your final summary covers all major topics without redundancy.\n",
        "\n",
        "### üîç What You'll Learn:\n",
        "- How to convert text into numerical embeddings\n",
        "- Unsupervised learning with clustering algorithms\n",
        "- Topic modeling and content organization\n",
        "- How to balance topic coverage in summaries\n",
        "\n",
        "### üìã Implementation Outline:\n",
        "1. **Generate Embeddings**: Use SentenceTransformers to convert each chunk into vector embeddings\n",
        "2. **Apply Clustering**: Use K-means or DBSCAN to group semantically similar chunks\n",
        "3. **Analyze Clusters**: Visualize clusters and identify what topics each represents\n",
        "4. **Smart Selection**: Choose representative chunks from each cluster for summarization\n",
        "5. **Topic-Balanced Summary**: Ensure final summary covers all major topic clusters\n",
        "6. **Evaluation**: Compare cluster-based vs. sequential summarization for topic diversity\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Track 5: Advanced Evaluation & Quality Metrics\n",
        "**Description**: Go beyond ROUGE to develop comprehensive quality assessment using multiple evaluation approaches.\n",
        "\n",
        "### üîç What You'll Learn:\n",
        "- Limitations of current evaluation metrics\n",
        "- Multi-dimensional quality assessment\n",
        "- How to build robust evaluation pipelines\n",
        "\n",
        "### üìã Implementation Outline:\n",
        "1. **Semantic Similarity**: Implement sentence embedding-based similarity (using models like SentenceTransformers)\n",
        "2. **Factual Accuracy**: Develop methods to check if key facts are preserved\n",
        "3. **Readability Analysis**: Measure text complexity and flow (using libraries like textstat)\n",
        "4. **Coverage Analysis**: Ensure summaries represent content from throughout the document\n",
        "5. **Human Evaluation**: Design surveys to collect human quality ratings\n",
        "6. **Comprehensive Dashboard**: Create visualization showing all quality dimensions\n",
        "7. **Quality Predictor**: Build a model that predicts summary quality without reference summaries\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Getting Started Tips:\n",
        "\n",
        "### üìö **Research Phase** (for all tracks):\n",
        "- Read recent papers on your chosen topic\n",
        "- Look for existing implementations on GitHub\n",
        "- Check Hugging Face model hub for relevant models\n",
        "\n",
        "### üõ†Ô∏è **Implementation Phase**:\n",
        "- Start with small experiments before building the full system\n",
        "- Document your findings and compare results systematically\n",
        "- Use version control to track different approaches\n",
        "\n",
        "### üìä **Evaluation Phase**:\n",
        "- Always compare against your baseline system\n",
        "- Use multiple documents for testing\n",
        "- Consider both quantitative metrics and qualitative analysis\n",
        "\n",
        "### üöÄ **Presentation Phase**:\n",
        "- Document your methodology clearly\n",
        "- Include visualizations of your results\n",
        "- Discuss limitations and future improvements\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Pro Tips:\n",
        "- **Start Simple**: Pick one track and master it before combining approaches\n",
        "- **Document Everything**: Keep detailed notes on what works and what doesn't\n",
        "- **Share Results**: Consider writing a blog post or creating a demo\n",
        "- **Think Production**: How would you deploy this in a real-world system?\n",
        "\n",
        "Choose your adventure and push the boundaries of text summarization! üöÄ"
      ],
      "metadata": {
        "id": "uB7BOIqOGVg4"
      }
    }
  ]
}