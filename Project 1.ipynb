{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ec4d71c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ec4d71c",
        "outputId": "7d78be01-1338-4b1e-e226-9b04c7d3dd15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.97.1)\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Collecting pdfminer.six==20250506 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.3.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20250506 pdfplumber-0.11.7 pypdfium2-4.30.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai pdfplumber tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acc1ea2b",
      "metadata": {
        "id": "acc1ea2b"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import openai\n",
        "import pdfplumber\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "openai.api_key = \"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\"  # Replace with your OpenAI API key\n",
        "\n",
        "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
        "CHAT_MODEL = \"gpt-3.5-turbo\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b12635c2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "b12635c2",
        "outputId": "8686a773-81ea-4033-fa52-6326b4455f66"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f19ef028-2aa6-44ba-9ac3-1d55f7c3a9f1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f19ef028-2aa6-44ba-9ac3-1d55f7c3a9f1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving MindSnaps 1.pdf to MindSnaps 1.pdf\n",
            "✅ Extracted 1 pages.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "pdf_path = list(uploaded.keys())[0]\n",
        "\n",
        "def extract_text_by_page(file_path):\n",
        "    pages = []\n",
        "    with pdfplumber.open(file_path) as pdf:\n",
        "        for i, page in enumerate(pdf.pages):\n",
        "            text = page.extract_text()\n",
        "            if text:\n",
        "                pages.append({\"page\": i + 1, \"text\": text})\n",
        "    return pages\n",
        "\n",
        "pages = extract_text_by_page(pdf_path)\n",
        "full_text = \"\\n\\n\".join([p['text'] for p in pages])\n",
        "print(f\"✅ Extracted {len(pages)} pages.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d97496c1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d97496c1",
        "outputId": "cd7ffa87-7625-403f-adb8-0f1a0ba8529c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📄 Summary:\n",
            " The team MindSnaps aims to address the lack of personalized learning in traditional education by creating an AI-powered assistant that transforms educational material into personalized, engaging, and interactive content. This solution is expected to improve education quality, student development, and support Oman Vision 2040's priority of \"Education, Learning, and Scientific Research\".\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def summarize_text(text):\n",
        "    response = openai.chat.completions.create(\n",
        "        model=CHAT_MODEL,\n",
        "        messages=[{\"role\": \"user\", \"content\": f\"Summarize this document:\\n\\n{text[:4000]}\"}],\n",
        "        max_tokens=500,\n",
        "        temperature=0.5\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "summary = summarize_text(full_text)\n",
        "print(\"📄 Summary:\\n\", summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_mcqs(text):\n",
        "    prompt = f\"\"\"\n",
        "Create 5 multiple choice questions from the following document.\n",
        "Each question should have 4 options (A–D) and one correct answer.\n",
        "Use exactly this format:\n",
        "\n",
        "Q1: What is the capital of France?\n",
        "A. Berlin\n",
        "B. Madrid\n",
        "C. Paris\n",
        "D. Rome\n",
        "Answer: C\n",
        "\n",
        "Content:\n",
        "{text[:3000]}\n",
        "\"\"\"\n",
        "    response = openai.chat.completions.create(\n",
        "        model=CHAT_MODEL,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=800,\n",
        "        temperature=0.5\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "mcq_text = generate_mcqs(full_text)\n"
      ],
      "metadata": {
        "id": "YMOAdipxGEGM"
      },
      "id": "YMOAdipxGEGM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def parse_mcqs(raw_text):\n",
        "    pattern = re.compile(\n",
        "        r\"Q\\d+: (.*?)\\nA\\. (.*?)\\nB\\. (.*?)\\nC\\. (.*?)\\nD\\. (.*?)\\nAnswer: ([A-D])\",\n",
        "        re.DOTALL\n",
        "    )\n",
        "    matches = pattern.findall(raw_text)\n",
        "    questions = []\n",
        "    for match in matches:\n",
        "        question, a, b, c, d, answer = match\n",
        "        questions.append({\n",
        "            \"question\": question.strip(),\n",
        "            \"options\": {\n",
        "                \"A\": a.strip(),\n",
        "                \"B\": b.strip(),\n",
        "                \"C\": c.strip(),\n",
        "                \"D\": d.strip(),\n",
        "            },\n",
        "            \"answer\": answer.strip()\n",
        "        })\n",
        "    return questions\n",
        "\n",
        "mcqs = parse_mcqs(mcq_text)\n",
        "print(f\"✅ Parsed {len(mcqs)} MCQs\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOvJVBJDFgCY",
        "outputId": "d2868f9d-ac98-4bf6-8002-410db6a4e893"
      },
      "id": "aOvJVBJDFgCY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Parsed 5 MCQs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fff0e3e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fff0e3e0",
        "outputId": "4e9c1f73-7061-473c-8746-ee6ce649831c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. What is the proposed solution to the lack of personalized learning in traditional education?\n",
            "   A. Hiring more teachers\n",
            "   B. Implementing standardized testing\n",
            "   C. An AI-powered assistant\n",
            "   D. Increasing class sizes\n",
            "Your answer (A/B/C/D): a\n",
            "❌ Incorrect. The correct answer is C\n",
            "\n",
            "2. What positive effect does personalized content have on education quality?\n",
            "   A. Decreases motivation\n",
            "   B. Reduces comprehension\n",
            "   C. Improves comprehension and motivation\n",
            "   D. Has no effect on education quality\n",
            "Your answer (A/B/C/D): b\n",
            "❌ Incorrect. The correct answer is C\n",
            "\n",
            "3. How does the AI-powered assistant help students study more effectively?\n",
            "   A. By assigning more homework\n",
            "   B. By generating summaries, quizzes, and language support\n",
            "   C. By increasing class sizes\n",
            "   D. By decreasing student engagement\n",
            "Your answer (A/B/C/D): c\n",
            "❌ Incorrect. The correct answer is B\n",
            "\n",
            "4. What does the lack of personalized learning in traditional education limit?\n",
            "   A. Student engagement\n",
            "   B. Development of deep understanding\n",
            "   C. Critical thinking\n",
            "   D. All of the above\n",
            "Your answer (A/B/C/D): d\n",
            "✅ Correct!\n",
            "\n",
            "5. Which priority of Oman Vision 2040 does the proposed solution directly support?\n",
            "   A. Health and Wellness\n",
            "   B. Economic Diversification\n",
            "   C. Education, Learning, and Scientific Research\n",
            "   D. Infrastructure Development\n",
            "Your answer (A/B/C/D): a\n",
            "❌ Incorrect. The correct answer is C\n",
            "\n",
            "🎯 Final Score: 1/5\n"
          ]
        }
      ],
      "source": [
        "score = 0\n",
        "for i, q in enumerate(mcqs):\n",
        "    print(f\"\\n{i+1}. {q['question']}\")\n",
        "    for opt_key, opt_text in q['options'].items():\n",
        "        print(f\"   {opt_key}. {opt_text}\")\n",
        "    user_input = input(\"Your answer (A/B/C/D): \").strip().upper()\n",
        "    if user_input == q['answer']:\n",
        "        print(\"✅ Correct!\")\n",
        "        score += 1\n",
        "    else:\n",
        "        print(f\"❌ Incorrect. The correct answer is {q['answer']}\")\n",
        "print(f\"\\n🎯 Final Score: {score}/{len(mcqs)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2350012b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2350012b",
        "outputId": "34a18b46-a4ad-4f4f-b241-da4fb18d791d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ PDF embedded.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def embed_chunks(pages):\n",
        "    chunks = []\n",
        "    for chunk in pages:\n",
        "        embedding = openai.embeddings.create(\n",
        "            model=EMBEDDING_MODEL,\n",
        "            input=chunk[\"text\"]\n",
        "        ).data[0].embedding\n",
        "        chunks.append({\n",
        "            \"page\": chunk[\"page\"],\n",
        "            \"text\": chunk[\"text\"],\n",
        "            \"embedding\": embedding\n",
        "        })\n",
        "    return chunks\n",
        "\n",
        "embedded_chunks = embed_chunks(pages)\n",
        "print(\"✅ PDF embedded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8db2245",
      "metadata": {
        "id": "f8db2245"
      },
      "outputs": [],
      "source": [
        "\n",
        "def cosine_similarity(a, b):\n",
        "    a, b = np.array(a), np.array(b)\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "def retrieve_relevant_chunks(query, chunks, top_k=3):\n",
        "    query_embedding = openai.embeddings.create(\n",
        "        model=EMBEDDING_MODEL,\n",
        "        input=query\n",
        "    ).data[0].embedding\n",
        "\n",
        "    ranked = sorted(\n",
        "        chunks,\n",
        "        key=lambda x: cosine_similarity(query_embedding, x[\"embedding\"]),\n",
        "        reverse=True\n",
        "    )\n",
        "    return ranked[:top_k]\n",
        "\n",
        "def chat_with_pdf(query, chunks):\n",
        "    top_chunks = retrieve_relevant_chunks(query, chunks)\n",
        "    context = \"\\n\\n\".join([f\"(Page {c['page']}):\\n{c['text']}\" for c in top_chunks])\n",
        "    prompt = f\"\"\"You are a helpful assistant. Use the following PDF content to answer the user's question.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "Answer:\"\"\"\n",
        "    response = openai.chat.completions.create(\n",
        "        model=CHAT_MODEL,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=500,\n",
        "        temperature=0.3\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_text(text, target_language=\"Arabic\"):\n",
        "    prompt = f\"\"\"Translate the following text into {target_language}:\\n\\n{text[:4000]}\"\"\"\n",
        "    response = openai.chat.completions.create(\n",
        "        model=CHAT_MODEL,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=1000,\n",
        "        temperature=0.3\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "translated_text = translate_text(full_text)\n",
        "print(\"🌍 Translated Text:\\n\", translated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgKLrY_iL5CW",
        "outputId": "d1b4e926-8c8b-4e8c-b981-8ae7f22ff997"
      },
      "id": "kgKLrY_iL5CW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌍 Translated Text:\n",
            " اسم الفريق: مايند سنابس\n",
            "أعضاء الفريق:\n",
            "1- إيمان البتاشي\n",
            "2- روناق\n",
            "3- إسراء\n",
            "المشكلة التي تقومون بحلها:\n",
            "نقص التعلم الشخصي في التعليم التقليدي يقلل من انخراط الطلاب ويحد من تطوير الفهم العميق والتفكير النقدي.\n",
            "غالبًا ما يواجه الطلاب صعوبة في العثور على محتوى مخصص يتناسب مع مستوى تعلمهم الفردي، خاصة في تعلم اللغات والاستعداد للاختبارات.\n",
            "الحل المقترح:\n",
            "مساعد مدعوم بالذكاء الاصطناعي يحول المواد التعليمية التقليدية إلى محتوى شخصي وجذاب وتفاعلي باستخدام أدوات الذكاء الاصطناعي الإنشائي. يساعد الطلاب على الدراسة بشكل أكثر فعالية من خلال إنشاء ملخصات واختبارات ودعم لغوي تلقائيًا.\n",
            "التأثيرات الإيجابية المتوقعة:\n",
            "جودة التعليم: يحسن المحتوى الشخصي الفهم والدافع\n",
            "تطوير الطلاب: يشجع على التفكير النقدي والتعلم المستقل\n",
            "رؤية عمان 2040: يدعم مباشرة أولوية \"التعليم والتعلم والبحث العلمي\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9698b52",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9698b52",
        "outputId": "6293e217-3075-4842-da93-9b3ea7332ebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "💬 Ask something from the PDF (or type 'exit'): hi\n",
            "\n",
            "🤖 Answer:\n",
            " Hello! How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "\n",
        "while True:\n",
        "    question = input(\"\\n💬 Ask something from the PDF (or type 'exit'): \")\n",
        "    if question.lower() == \"exit\":\n",
        "        break\n",
        "    print(\"\\n🤖 Answer:\\n\", chat_with_pdf(question, embedded_chunks))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}