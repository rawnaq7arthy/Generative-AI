{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDe7DsPWmEBV"
      },
      "source": [
        "# üß† Day 1: Hands-on with Language Models using Phi-3\n",
        "Welcome to the first session of the Generative AI workshop!\n",
        "\n",
        "Today we'll explore the basics of large language models (LLMs) and use Microsoft's **Phi-3 Mini** model.\n",
        "\n",
        "### üéØ Objectives\n",
        "- Understand what a language model is\n",
        "- Load and run a small LLM\n",
        "- Generate text from prompts\n",
        "- Modify prompts and analyze outputs\n",
        "- Reflect on tokenization and model behavior"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Setup the Environment"
      ],
      "metadata": {
        "id": "jYzN4Inc1N2Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rtcrbs7tiTgb"
      },
      "outputs": [],
      "source": [
        " %%capture\n",
        " !pip install transformers accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXp09JFsFBXi"
      },
      "source": [
        "## üì¶ Load the Phi-3 Model\n",
        "Fill in the missing arguments to complete the model and tokenizer loading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSNalRXZyTTk"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# TODO: Load the model here\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"_____\",               # Hint: it's a Phi-3 variant, look for the 4k instruct model\n",
        "    device_map=\"_____\",   # Hint: 'cuda' or 'auto'\n",
        "    torch_dtype=\"_____\",  # Hint: dtype hint\n",
        "    trust_remote_code=False\n",
        ")\n",
        "\n",
        "# TODO: Load the tokenizer here\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"_____\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdyYYS0E5fEU"
      },
      "source": [
        "## üì¶ Create a Text Generation Pipeline\n",
        "\n",
        "Wrap the model and tokenizer into a convenient `pipeline` object for easy inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiUi4Wu1FCyN"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# TODO: Create a pipeline for text-generation using the model and tokenizer\n",
        "generator = pipeline(\n",
        "    \"_____\",               # Hint: task type\n",
        "    model=_____,\n",
        "    tokenizer=_____,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=_____,  # Hint: token cap\n",
        "    do_sample=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD49kysT5mMY"
      },
      "source": [
        "## üí¨ Create and Send a Prompt\n",
        "\n",
        "Finally, we create our prompt as a user and give it to the model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkR7LBmiyXmY"
      },
      "outputs": [],
      "source": [
        "# TODO: Define a user message that asks for a joke\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"_____\"}  # Hint: Something humorous\n",
        "]\n",
        "\n",
        "# Generate output\n",
        "output = generator(messages)\n",
        "\n",
        "# TODO: Extract and print the model‚Äôs response\n",
        "print(_____)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úâÔ∏è Generate a Custom Output from a Prompt\n",
        "\n",
        "Now let‚Äôs directly tokenize a prompt and run it through the model to generate a complete response.\n"
      ],
      "metadata": {
        "id": "0vVxd2VdCG45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Write a detailed prompt that includes \"<|assistant|>\" at the end\n",
        "prompt = \"_____\"\n",
        "\n",
        "# Tokenize the prompt\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "# TODO: Use the model to generate output from input_ids\n",
        "generation_output = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_new_tokens=_____  # Hint: max tokens for generation, somewhere along the range of 100 to 1000\n",
        ")\n",
        "\n",
        "# TODO: Decode and print the output\n",
        "print(_____)\n"
      ],
      "metadata": {
        "id": "WO3e63-cr-Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úçÔ∏è Reflection Prompt\n",
        "## Try changing the prompt to a sarcastic tone or use specific instructions.\n",
        "## What do you observe in the outputs? Discuss with your team.\n"
      ],
      "metadata": {
        "id": "Bsv3Yb2ySFfq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£ View the Token IDs\n",
        "\n",
        "After tokenizing the prompt, we can print out the list of token IDs generated by the tokenizer.\n"
      ],
      "metadata": {
        "id": "qmHQAVHNHc5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Print the token IDs from our tokenized input\n",
        "print(____)  # Hint: What variable stores our tokenized input?\n",
        "\n",
        "# Expected output format:\n",
        "# tensor([[14359, 385, 4376, 27746, 5281, 394, 19235, 363, 278, 25305, ...]])"
      ],
      "metadata": {
        "id": "Z1htGdKMzaWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ Decode Each Token\n",
        "\n",
        "We can decode each token ID individually to better understand how the model splits the input into subwords.\n"
      ],
      "metadata": {
        "id": "_ofB5CSiHkxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Create a loop to decode each token individually\n",
        "for ____ in input_ids[0]:  # Hint: What should we iterate over?\n",
        "    print(tokenizer.____(____.item()))  # Hint: What method converts token IDs back to text?"
      ],
      "metadata": {
        "id": "uKEhBTLzzeb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß¨ Inspect the Raw Model Output\n",
        "\n",
        "The model returns a tensor of token IDs as its output. These represent the full generated sequence (input + new tokens).\n"
      ],
      "metadata": {
        "id": "yhiTG2pOJswf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generation_output"
      ],
      "metadata": {
        "id": "069G_3wkze4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üî° Combine Tokens into Words\n",
        "\n",
        "Subword tokenizers may split a word into multiple pieces. You can decode them individually or as a group to see how they combine into meaningful text.\n"
      ],
      "metadata": {
        "id": "CbYkdV0oJ8Ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Decode individual tokens to see subword splitting\n",
        "print(tokenizer.decode([____]))   # Hint: Try token ID 3323\n",
        "print(tokenizer.decode([____]))   # Hint: Try token ID 622\n",
        "print(tokenizer.decode([____, ____]))  # Hint: Combine tokens\n",
        "print(tokenizer.decode([____]))   # Hint: Try token ID 29901\n",
        "\n",
        "# Expected output pattern:\n",
        "# Sub\n",
        "# ject\n",
        "# Subject\n",
        "# ."
      ],
      "metadata": {
        "id": "Kn4DQZHlzkJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###üåà Token Coloring Function\n",
        "\n",
        "This function uses ANSI escape codes to highlight each token in a different background color. It helps visualize how text is broken down into subword units.\n"
      ],
      "metadata": {
        "id": "VKd7KgQ2KRRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "colors_list = [\n",
        "    '102;194;165', '252;141;98', '141;160;203',\n",
        "    '231;138;195', '166;216;84', '255;217;47'\n",
        "]\n",
        "\n",
        "def show_tokens(sentence, tokenizer_name):\n",
        "   # TODO: Load the tokenizer from the pretrained model\n",
        "   tokenizer = AutoTokenizer.from_pretrained(____)  # Hint: What variable contains the model name?\n",
        "\n",
        "   # TODO: Tokenize the sentence to get token IDs\n",
        "   token_ids = tokenizer(____).input_ids  # Hint: What text should we tokenize?\n",
        "\n",
        "   for idx, t in enumerate(token_ids):\n",
        "       print(\n",
        "           f'\\033[38;2;0;{colors_list[idx % len(colors_list)]}m' +\n",
        "           tokenizer.____(__) +  # Hint: What method converts token ID back to text?\n",
        "           '\\033[0m',\n",
        "           end='' # Hint: What should this end with?\n",
        "       )"
      ],
      "metadata": {
        "id": "n28Wbn-dztNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ Try It on a Complex Sentence\n",
        "\n",
        "Now test the tokenizer on a sentence with mixed content: capital letters, emojis, symbols, numbers, and spacing.\n"
      ],
      "metadata": {
        "id": "8cK5ae6pKXyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Create a test sentence with mixed content\n",
        "text = \"\"\"\n",
        "____  # Hint: Add text with capital letters, emojis, symbols, numbers, and spacing\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "UuuHVfMIz8IZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üÜö `bert-base-uncased` Tokenizer\n",
        "\n",
        "This tokenizer lowercases all input and splits words into WordPiece subwords. Notice how it handles casing and unknown characters.\n"
      ],
      "metadata": {
        "id": "LQeQfLmhKuim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_tokens(____, \"____\")  # Hint: What text and tokenizer name?"
      ],
      "metadata": {
        "id": "z-V8Nnfwz8ZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üÜö `bert-base-cased` Tokenizer\n",
        "\n",
        "Unlike the uncased version, this tokenizer preserves capitalization. Compare the tokenization output to see how casing influences token splitting.\n"
      ],
      "metadata": {
        "id": "FEGS10kiK7MG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_tokens(____, \"____\")  # Hint: What text and tokenizer name?"
      ],
      "metadata": {
        "id": "wfQ9MZnmz91W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üÜö `gpt2` Tokenizer\n",
        "\n",
        "GPT-2 uses Byte-Pair Encoding (BPE), which often results in different token splits, especially with punctuation, emojis, or spacing.\n"
      ],
      "metadata": {
        "id": "-CkJVIQqLMCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_tokens(____, \"____\")  # Hint: What text and tokenizer name?"
      ],
      "metadata": {
        "id": "GsnbxFecz_2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üÜö `google/flan-t5-small` Tokenizer\n",
        "\n",
        "This model uses SentencePiece, which breaks down text in a more language-agnostic way. Observe how it segments common phrases and subwords.\n"
      ],
      "metadata": {
        "id": "tOHL0Y3rLoaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_tokens(____, \"____\")  # Hint: What text and tokenizer name?"
      ],
      "metadata": {
        "id": "Bw-sDPIp0Chx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üÜö `Xenova/gpt-4` Tokenizer\n",
        "\n",
        "This Hugging Face-hosted tokenizer mirrors OpenAI's `tiktoken`. It uses Byte-Pair Encoding and handles punctuation, numbers, and special symbols distinctly.\n"
      ],
      "metadata": {
        "id": "42YIbUzWL7Ba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The official is `tiktoken` but this the same tokenizer on the HF platform\n",
        "show_tokens(____, \"____\")  # Hint: What text and tokenizer name?"
      ],
      "metadata": {
        "id": "SBWcQnBw0Erg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üÜö `bigcode/starcoder2-15b` Tokenizer\n",
        "\n",
        "You need access to use the actual model, but the tokenizer is available. It's optimized for code and performs differently on natural language and structured inputs.\n"
      ],
      "metadata": {
        "id": "3k2dffPGMZkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You need to request access before being able to use this tokenizer\n",
        "show_tokens(____, \"____\")  # Hint: What text and tokenizer name?"
      ],
      "metadata": {
        "id": "9QDVadeJ0Ibv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üÜö `microsoft/Phi-3-mini-4k-instruct` Tokenizer\n",
        "\n",
        "This tokenizer is designed for compact, efficient language modeling. Notice how it splits and groups tokens differently than BERT or GPT models.\n"
      ],
      "metadata": {
        "id": "GINWypXqMdyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_tokens(____, \"____\")  # Hint: What text and tokenizer name?"
      ],
      "metadata": {
        "id": "AQTHFZeY0KcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Load a Model to Extract Embeddings\n",
        "\n",
        "We can use a pretrained transformer (like DeBERTa) to convert text into token-level embeddings.\n"
      ],
      "metadata": {
        "id": "FMpNsH-NN7OL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# TODO: Load a tokenizer for embeddings extraction\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"____\")  # Hint: What model name for DeBERTa-base?\n",
        "\n",
        "# TODO: Load a language model for embeddings\n",
        "model = AutoModel.from_pretrained(\"____\")  # Hint: Same model name as tokenizer\n",
        "\n",
        "# TODO: Tokenize the sentence with proper tensor format\n",
        "tokens = tokenizer('____', return_tensors='____')  # Hint: What text and tensor format?\n",
        "\n",
        "# TODO: Process the tokens through the model to get embeddings\n",
        "output = model(**____)[]  # Hint: What variable contains our tokenized input?"
      ],
      "metadata": {
        "id": "DUsKv4tw0Mnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üî¢ View Embedding Dimensions\n",
        "\n",
        "Each token is mapped to a high-dimensional vector. Let‚Äôs inspect the shape of the output to understand the model‚Äôs internal representation.\n"
      ],
      "metadata": {
        "id": "j8Lo16KvOAA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Check the shape of the embedding output\n",
        "____.shape  # Hint: What variable contains our model output?\n",
        "\n",
        "# Expected output: torch.Size([1, 4, 384])"
      ],
      "metadata": {
        "id": "ZHFa0Om00PmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç Decode Input Tokens\n",
        "\n",
        "We can decode the tokens back to their original text to understand which words each embedding vector corresponds to.\n"
      ],
      "metadata": {
        "id": "JE537k9fOER8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Decode the input tokens back to text\n",
        "for token in tokens['____'][0]:  # Hint: What key contains the input IDs?\n",
        "    print(tokenizer.____(____.item()))  # Hint: What method decodes tokens?\n",
        "\n",
        "# Expected output:\n",
        "# [CLS]\n",
        "# Hello\n",
        "# world\n",
        "# [SEP]"
      ],
      "metadata": {
        "id": "0BcradOt0TPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß¨ View Token-Level Embeddings\n",
        "\n",
        "Each row in the tensor represents a single token‚Äôs embedding ‚Äî a high-dimensional numerical representation that captures meaning and context.\n"
      ],
      "metadata": {
        "id": "zYmANkMeOVHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: View the token-level embeddings tensor\n",
        "____  # Hint: What variable contains our model output?\n",
        "\n",
        "# Each row represents a single token's embedding - a high-dimensional numerical representation"
      ],
      "metadata": {
        "id": "G7eQqHYz0USF",
        "outputId": "307f3d88-06e9-4a51-e900-949fd7fc9ea6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-3.4816,  0.0861, -0.1819,  ..., -0.0612, -0.3911,  0.3017],\n",
              "         [ 0.1898,  0.3208, -0.2315,  ...,  0.3714,  0.2478,  0.8048],\n",
              "         [ 0.2071,  0.5036, -0.0485,  ...,  1.2175, -0.2292,  0.8582],\n",
              "         [-3.4278,  0.0645, -0.1427,  ...,  0.0658, -0.4367,  0.3834]]],\n",
              "       grad_fn=<NativeLayerNormBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Encode a Sentence into a Vector\n",
        "\n",
        "We can convert an entire sentence into a fixed-size embedding vector using a pretrained model. This helps machines understand and compare sentences by their meaning.\n"
      ],
      "metadata": {
        "id": "uEOUCMk5QCh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# TODO: Load a sentence transformer model\n",
        "model = SentenceTransformer('____')  # Hint: What's the model name for all-mpnet-base-v2?\n",
        "\n",
        "# TODO: Convert text to sentence embeddings\n",
        "vector = model.encode(\"____\")  # Hint: What text should we encode, like \"Best movie ever!\"?"
      ],
      "metadata": {
        "id": "L1EyDe1y0Vmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìè Check the Sentence Embedding Size\n",
        "\n",
        "Let‚Äôs inspect the dimensions of the sentence vector. This tells us how many numerical features are used to represent the meaning of the sentence.\n"
      ],
      "metadata": {
        "id": "Zi7Y8bbpQiIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Check the dimensions of the sentence embedding vector\n",
        "____.shape  # Hint: What variable contains our sentence vector?\n",
        "\n",
        "# Expected output: (768,)"
      ],
      "metadata": {
        "id": "qTpujUqx0W91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üåê Load Pretrained GloVe Embeddings\n",
        "\n",
        "We can use GloVe (trained on Wikipedia and Gigaword) to explore classic word embeddings. These models capture word meaning based on co-occurrence patterns.\n"
      ],
      "metadata": {
        "id": "47J72H1EQnts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "import gensim.downloader as api\n",
        "\n",
        "# TODO: Download GloVe embeddings (50MB, trained on Wikipedia, vector size: 50)\n",
        "model = api.load('____')  # Hint: What's the model name for glove-wiki-gigaword-50?"
      ],
      "metadata": {
        "id": "DiilH-2G0eGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç Find Similar Words in Embedding Space\n",
        "\n",
        "We can now explore semantic similarity using vector distance. The model returns words that are closest to `\"king\"` in embedding space.\n"
      ],
      "metadata": {
        "id": "teMkmmtDQsRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Find words most similar to \"king\" in embedding space\n",
        "model.most_similar([____], top=____)  # Hint: What word to search for and how many results?\n",
        "\n",
        "# Expected output: List of (word, similarity_score) tuples"
      ],
      "metadata": {
        "id": "AJ2Wv72U0iKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìÇ Load and Parse Playlist Data\n",
        "\n",
        "We begin by downloading a playlist dataset and parsing it into a usable format. Each playlist is represented as a sequence of song IDs.\n"
      ],
      "metadata": {
        "id": "D_ym0tCuRDkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from urllib import request\n",
        "\n",
        "# TODO: Get the playlist dataset file\n",
        "data = request.urlopen('https://storage.googleapis.com/maps-premium/dataset/yes_complete/train.txt')\n",
        "\n",
        "# TODO: Parse the playlist dataset file, skipping metadata lines\n",
        "lines = data.read().decode('____').split('____')[2:]  # Hint: What encoding and split character?\n",
        "\n",
        "# TODO: Remove playlists with only one song\n",
        "playlists = [s.strip().split() for s in lines if len(s.split()) > ____]  # Hint: Minimum songs per playlist?\n",
        "\n",
        "# TODO: Load song metadata\n",
        "songs_file = request.urlopen('https://storage.googleapis.com/maps-premium/dataset/yes_complete/song_hash.txt')\n",
        "songs_file = songs_file.read().decode('____').split('____')  # Hint: Encoding and split character?\n",
        "songs = [s.strip().split('\\t') for s in songs_file]\n",
        "\n",
        "# TODO: Create a DataFrame with song information\n",
        "songs_df = pd.DataFrame(data=____, columns=['____', '____', '____'])  # Hint: What data and column names?\n",
        "songs_df = songs_df.set_index('____')  # Hint: What column to use as index?"
      ],
      "metadata": {
        "id": "V_W3IFi71nOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéß Display Sample Playlists\n",
        "\n",
        "Let‚Äôs preview a couple of playlists to understand the structure. Each ID corresponds to a specific song.\n"
      ],
      "metadata": {
        "id": "VoZXeoikRPot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Display sample playlists to understand the structure\n",
        "print('Playlist #1:\\n ', ____[____], '\\n')  # Hint: Which playlist list and index?\n",
        "print('Playlist #2:\\n ', ____[____])        # Hint: Which playlist list and index?"
      ],
      "metadata": {
        "id": "SaN64tdU1nwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Train a Word2Vec Model on Playlists\n",
        "\n",
        "We treat playlists like sentences and songs like words. Training Word2Vec on this lets us learn embeddings that capture song co-occurrence patterns.\n"
      ],
      "metadata": {
        "id": "REy8phl4Rbv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# TODO: Train our Word2Vec model on playlist data\n",
        "model = Word2Vec(\n",
        "   ____, vector_size=____, window=____, negative=____, min_count=____, workers=____\n",
        ")\n",
        "\n",
        "# Hint: What data to train on and what are reasonable parameter values?"
      ],
      "metadata": {
        "id": "vdVz79F71uIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç Find Similar Songs by ID\n",
        "\n",
        "Using our trained model, we can now retrieve songs that are most similar to a given song based on their playlist co-occurrence.\n"
      ],
      "metadata": {
        "id": "PXuPPOdqSAv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "song_id = 2172\n",
        "\n",
        "# TODO: Ask the model for songs similar to song #2172\n",
        "model.wv.most_similar(positive=[str(____)])  # Hint: What song ID should we convert to string?"
      ],
      "metadata": {
        "id": "iuGF0Enp1uhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéµ Look Up the Original Song\n",
        "\n",
        "Let‚Äôs look up the details (title and artist) of the query song to better understand the recommendations.\n"
      ],
      "metadata": {
        "id": "R7wRIfb7SHU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Look up the song details in our songs DataFrame\n",
        "print(songs_df.iloc[____])  # Hint: What song ID should we look up?"
      ],
      "metadata": {
        "id": "sxXNC8fy1wv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Define a Function to Print Song Recommendations\n",
        "\n",
        "This helper function prints the top 5 recommended songs for any song ID by mapping the result back to human-readable titles and artists.\n"
      ],
      "metadata": {
        "id": "6TgwvjZBSJ_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def print_recommendations(song_id):\n",
        "   # TODO: Get similar songs from the Word2Vec model\n",
        "   similar_songs = np.array(\n",
        "       model.wv.most_similar(positive=[str(____), top=____])  # Hint: What song ID and how many recommendations?\n",
        "   )[:,0]\n",
        "\n",
        "   # TODO: Return the song details from our DataFrame\n",
        "   return songs_df.iloc[____]  # Hint: What variable contains the similar song IDs?\n",
        "\n",
        "# TODO: Extract recommendations for song 2172\n",
        "print_recommendations(____)  # Hint: What song ID should we test?"
      ],
      "metadata": {
        "id": "Bt68Yy5R1ysX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéß View Song Recommendations\n",
        "\n",
        "Use the function to explore which songs are most similar to any track based on the playlist embedding model. Try it with different song IDs to see how recommendations vary across genres.\n"
      ],
      "metadata": {
        "id": "EtHmwZnEV22N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Use the function to explore recommendations for different songs\n",
        "print_recommendations(____)  # Hint: Try a different song ID to see how recommendations vary"
      ],
      "metadata": {
        "id": "iR_JENfJ1z8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ü§ñ Load AI Models for Prompt Engineering\n",
        "# Run this cell ONCE at the beginning - models will stay loaded\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "print(\"üîß Loading Language Models...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Load Phi-3 model and tokenizer\n",
        "print(\"Loading Phi-3 model and tokenizer...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "\n",
        "print(\"‚úÖ Models loaded successfully!\")\n",
        "print(\"üéØ Models are now available for all prompt engineering experiments!\")\n",
        "print(\"=\" * 50)"
      ],
      "metadata": {
        "id": "4cF-3FE4lSri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üõ†Ô∏è Setup Generation Function\n",
        "# Run this cell after loading the models above\n",
        "\n",
        "def generate_response(messages, max_tokens=200):\n",
        "    # Simple prompt formatting\n",
        "    if isinstance(messages, list) and len(messages) > 0:\n",
        "        prompt = messages[0]['content']\n",
        "    else:\n",
        "        prompt = str(messages)\n",
        "\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate response with minimal settings to avoid cache issues\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            use_cache=False  # Disable cache to avoid version issues\n",
        "        )\n",
        "\n",
        "    # Decode only the new tokens\n",
        "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "print(\"‚úÖ Generation function ready!\")\n",
        "print(\"üöÄ Ready for prompt engineering experiments!\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Test the generation function\n",
        "test_messages = [{\"role\": \"user\", \"content\": \"Hello! Can you introduce yourself briefly?\"}]\n",
        "test_output = generate_response(test_messages)\n",
        "print(\"üß™ Test Output:\")\n",
        "print(test_output)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üéØ Now let's explore how different prompts affect AI responses!\")"
      ],
      "metadata": {
        "id": "VQbWydWzlY61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üéØ EXPERIMENT 1: Basic vs Advanced Prompting\n",
        "# Compare how prompt quality affects AI responses\n",
        "\n",
        "print(\"üîç BASIC vs ADVANCED PROMPTING EXPERIMENT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Basic prompt (often gives generic responses)\n",
        "basic_prompt = \"Write about artificial intelligence\"\n",
        "\n",
        "print(\"‚ùå BASIC PROMPT:\")\n",
        "print(f\"Prompt: '{basic_prompt}'\")\n",
        "print(\"-\" * 40)\n",
        "basic_messages = [{\"role\": \"user\", \"content\": basic_prompt}]\n",
        "basic_output = generate_response(basic_messages)\n",
        "print(\"Result:\")\n",
        "print(basic_output)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Advanced prompt (specific, structured, with context)\n",
        "advanced_prompt = \"\"\"You are an AI researcher writing for a tech magazine. Write a 200-word article about how artificial intelligence is transforming healthcare. Include:\n",
        "- One specific real-world example\n",
        "- One challenge that still needs solving\n",
        "- A prediction for the next 5 years\n",
        "Use an engaging, accessible tone for general readers.\"\"\"\n",
        "\n",
        "print(\"‚úÖ ADVANCED PROMPT:\")\n",
        "print(f\"Prompt: '{advanced_prompt}'\")\n",
        "print(\"-\" * 40)\n",
        "advanced_messages = [{\"role\": \"user\", \"content\": advanced_prompt}]\n",
        "advanced_output = generate_response(advanced_messages)\n",
        "print(\"Result:\")\n",
        "print(advanced_output)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "print(\"ü§î REFLECTION QUESTIONS:\")\n",
        "print(\"1. Which response was more useful and specific?\")\n",
        "print(\"2. What elements made the advanced prompt more effective?\")\n",
        "print(\"3. How did structure and context change the output quality?\")"
      ],
      "metadata": {
        "id": "2lJfrlF3qNH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üéØ YOUR TURN: Improve This Code Prompt\n",
        "# Try to get better, more complete code from the AI\n",
        "\n",
        "print(\"üöÄ STUDENT EXPERIMENT: Code Generation\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Weak starting prompt\n",
        "weak_prompt = \"Write a function to sort numbers\"\n",
        "\n",
        "print(\"üòê STARTING PROMPT (Needs Improvement):\")\n",
        "print(f\"'{weak_prompt}'\")\n",
        "weak_messages = [{\"role\": \"user\", \"content\": weak_prompt}]\n",
        "weak_output = generate_response(weak_messages)\n",
        "print(\"Result:\")\n",
        "print(weak_output)\n",
        "print(\"\\n\" + \"-\"*50)\n",
        "\n",
        "# YOUR IMPROVED PROMPT - Edit this!\n",
        "your_improved_prompt = \"\"\"YOUR IMPROVED PROMPT HERE - Try to get:\n",
        "- Better documentation\n",
        "- Error handling\n",
        "- Examples\n",
        "- Type hints\n",
        "- Clear variable names\"\"\"\n",
        "\n",
        "print(\"üîß YOUR IMPROVED PROMPT:\")\n",
        "print(f\"'{your_improved_prompt}'\")\n",
        "# Uncomment the lines below when you're ready to test your improved prompt:\n",
        "# improved_messages = [{\"role\": \"user\", \"content\": your_improved_prompt}]\n",
        "# improved_output = generate_response(improved_messages)\n",
        "# print(\"Your Result:\")\n",
        "# print(improved_output)\n",
        "\n",
        "print(\"\\nüìù CHALLENGE: Rewrite 'your_improved_prompt' to get the best possible code!\")\n",
        "print(\"üí° TIPS: Be specific about documentation, error handling, examples, etc.\")"
      ],
      "metadata": {
        "id": "2WCGQDGeqsFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üéØ YOUR TURN: Creative Writing Challenge\n",
        "# Transform a boring prompt into something that generates amazing stories\n",
        "\n",
        "print(\"‚ú® STUDENT EXPERIMENT: Creative Writing\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Generic starting prompt\n",
        "boring_prompt = \"Tell me a story\"\n",
        "\n",
        "print(\"üò¥ BORING PROMPT:\")\n",
        "print(f\"'{boring_prompt}'\")\n",
        "boring_messages = [{\"role\": \"user\", \"content\": boring_prompt}]\n",
        "boring_output = generate_response(boring_messages)\n",
        "print(\"Result:\")\n",
        "print(boring_output)\n",
        "print(\"\\n\" + \"-\"*50)\n",
        "\n",
        "# YOUR CREATIVE PROMPT - Make it amazing!\n",
        "your_creative_prompt = \"\"\"YOUR CREATIVE PROMPT HERE - Try to include:\n",
        "- Specific setting/genre\n",
        "- Character details\n",
        "- Constraints (word count, style)\n",
        "- Sensory details\n",
        "- Plot elements\"\"\"\n",
        "\n",
        "print(\"üé® YOUR CREATIVE PROMPT:\")\n",
        "print(f\"'{your_creative_prompt}'\")\n",
        "# Uncomment when ready to test:\n",
        "# creative_messages = [{\"role\": \"user\", \"content\": your_creative_prompt}]\n",
        "# creative_output = generate_response(creative_messages)\n",
        "# print(\"Your Result:\")\n",
        "# print(creative_output)\n",
        "\n",
        "print(\"\\nüìù CHALLENGE: Create a prompt that generates a compelling, specific story!\")\n",
        "print(\"üí° TIPS: Add constraints, vivid details, specific genres, character traits, etc.\")"
      ],
      "metadata": {
        "id": "MVqkBLnHrUMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üéØ YOUR TURN: Problem Solving Challenge\n",
        "# Get structured, actionable solutions instead of vague advice\n",
        "\n",
        "print(\"üí° STUDENT EXPERIMENT: Problem Solving\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Vague starting prompt\n",
        "vague_prompt = \"How can schools reduce student stress?\"\n",
        "\n",
        "print(\"ü§î VAGUE PROMPT:\")\n",
        "print(f\"'{vague_prompt}'\")\n",
        "vague_messages = [{\"role\": \"user\", \"content\": vague_prompt}]\n",
        "vague_output = generate_response(vague_messages)\n",
        "print(\"Result:\")\n",
        "print(vague_output)\n",
        "print(\"\\n\" + \"-\"*50)\n",
        "\n",
        "# YOUR STRUCTURED PROMPT - Make it actionable!\n",
        "your_structured_prompt = \"\"\"YOUR STRUCTURED PROMPT HERE - Try to get:\n",
        "- Specific, measurable solutions\n",
        "- Implementation steps\n",
        "- Success metrics\n",
        "- Different time frames (short/long term)\n",
        "- Consideration of constraints\"\"\"\n",
        "\n",
        "print(\"üìä YOUR STRUCTURED PROMPT:\")\n",
        "print(f\"'{your_structured_prompt}'\")\n",
        "# Uncomment when ready to test:\n",
        "# structured_messages = [{\"role\": \"user\", \"content\": your_structured_prompt}]\n",
        "# structured_output = generate_response(structured_messages)\n",
        "# print(\"Your Result:\")\n",
        "# print(structured_output)\n",
        "\n",
        "print(\"\\nüìù CHALLENGE: Get specific, actionable solutions with clear steps!\")\n",
        "print(\"üí° TIPS: Ask for formats, timelines, metrics, specific constraints, etc.\")"
      ],
      "metadata": {
        "id": "G91pFagtrksS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚è±Ô∏è EXPERIMENT: Speed vs Quality Trade-offs\n",
        "# Time how long different prompts take and compare efficiency\n",
        "\n",
        "import time\n",
        "\n",
        "print(\"‚è±Ô∏è SPEED & EFFICIENCY EXPERIMENT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def timed_generation(messages, label, max_tokens=150):\n",
        "    print(f\"\\nüîÑ Generating: {label}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    result = generate_response(messages, max_tokens=max_tokens)\n",
        "\n",
        "    end_time = time.time()\n",
        "    duration = end_time - start_time\n",
        "\n",
        "    print(f\"‚è±Ô∏è  Time taken: {duration:.2f} seconds\")\n",
        "    print(f\"üìù Word count: ~{len(result.split())} words\")\n",
        "    print(f\"‚ö° Words per second: {len(result.split())/duration:.1f}\")\n",
        "    print(f\"üìÑ Result:\\n{result}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    return result, duration\n",
        "\n",
        "# Test different prompt lengths and complexity\n",
        "prompts_to_test = [\n",
        "    (\"Short & Simple\", \"Explain AI in one sentence\"),\n",
        "    (\"Medium & Specific\", \"Explain AI to a 12-year-old using simple examples\"),\n",
        "    (\"Long & Detailed\", \"\"\"You are a teacher explaining artificial intelligence to middle school students.\n",
        "    Create a 100-word explanation that includes:\n",
        "    - What AI means in simple terms\n",
        "    - One everyday example they know\n",
        "    - Why it's useful\n",
        "    - One limitation or concern\n",
        "    Use conversational tone and avoid technical jargon.\"\"\"),\n",
        "    (\"Your Custom Prompt\", \"ADD YOUR OWN PROMPT HERE TO TEST!\")\n",
        "]\n",
        "\n",
        "results = {}\n",
        "for label, prompt in prompts_to_test:\n",
        "    if prompt != \"ADD YOUR OWN PROMPT HERE TO TEST!\":  # Skip placeholder\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        result, duration = timed_generation(messages, label)\n",
        "        results[label] = {\"duration\": duration, \"result\": result}\n",
        "\n",
        "print(\"\\nüìä SPEED COMPARISON SUMMARY:\")\n",
        "print(\"=\" * 50)\n",
        "for label, data in results.items():\n",
        "    print(f\"{label}: {data['duration']:.2f}s\")\n",
        "\n",
        "print(\"\\nü§î REFLECTION QUESTIONS:\")\n",
        "print(\"- Did more detailed prompts take longer?\")\n",
        "print(\"- Which prompt gave the best quality/speed ratio?\")\n",
        "print(\"- When might you prefer speed vs. detailed prompts?\")"
      ],
      "metadata": {
        "id": "cdBBATmfvFZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ü§ñ EXPERIMENT: Multiple Models Comparison\n",
        "# Load different models and compare their responses to the same prompt\n",
        "\n",
        "print(\"ü§ñ MULTIPLE MODELS EXPERIMENT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Load a second, smaller model for comparison\n",
        "print(\"Loading GPT-2 for comparison...\")\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "try:\n",
        "    gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "    gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
        "\n",
        "    def generate_gpt2_response(prompt, max_tokens=100):\n",
        "        inputs = gpt2_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            outputs = gpt2_model.generate(\n",
        "                inputs,\n",
        "                max_new_tokens=max_tokens,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=gpt2_tokenizer.eos_token_id\n",
        "            )\n",
        "        response = gpt2_tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
        "        return response\n",
        "\n",
        "    print(\"‚úÖ GPT-2 loaded successfully!\")\n",
        "    gpt2_available = True\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Could not load GPT-2: {e}\")\n",
        "    print(\"Continuing with Phi-3 only...\")\n",
        "    gpt2_available = False\n",
        "\n",
        "print(\"=\" * 50)"
      ],
      "metadata": {
        "id": "seleYisfwf1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test prompt for comparison\n",
        "test_prompt = \"Write a creative opening sentence for a mystery novel.\"\n",
        "\n",
        "print(f\"üéØ TEST PROMPT: '{test_prompt}'\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Phi-3 Response\n",
        "print(\"üî∑ PHI-3 RESPONSE:\")\n",
        "phi3_start = time.time()\n",
        "phi3_messages = [{\"role\": \"user\", \"content\": test_prompt}]\n",
        "phi3_response = generate_response(phi3_messages, max_tokens=100)\n",
        "phi3_time = time.time() - phi3_start\n",
        "print(f\"‚è±Ô∏è  Time: {phi3_time:.2f}s\")\n",
        "print(f\"üìù Response: {phi3_response}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# GPT-2 Response (if available)\n",
        "if gpt2_available:\n",
        "    print(\"üî∂ GPT-2 RESPONSE:\")\n",
        "    gpt2_start = time.time()\n",
        "    gpt2_response = generate_gpt2_response(test_prompt, max_tokens=100)\n",
        "    gpt2_time = time.time() - gpt2_start\n",
        "    print(f\"‚è±Ô∏è  Time: {gpt2_time:.2f}s\")\n",
        "    print(f\"üìù Response: {gpt2_response}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    print(\"üìä MODEL COMPARISON:\")\n",
        "    print(f\"Phi-3 speed: {phi3_time:.2f}s\")\n",
        "    print(f\"GPT-2 speed: {gpt2_time:.2f}s\")\n",
        "    print(f\"Speed winner: {'GPT-2' if gpt2_time < phi3_time else 'Phi-3'}\")\n",
        "\n",
        "print(\"\\nü§î REFLECTION QUESTIONS:\")\n",
        "print(\"- Which model gave more creative responses?\")\n",
        "print(\"- Which was faster?\")\n",
        "print(\"- How did response quality differ?\")\n",
        "print(\"- Which would you choose for different tasks?\")"
      ],
      "metadata": {
        "id": "DkSzyAb9xho0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üå°Ô∏è EXPERIMENT: Temperature Settings\n",
        "# See how creativity settings affect output consistency and variety\n",
        "\n",
        "print(\"üå°Ô∏è TEMPERATURE & CREATIVITY EXPERIMENT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def generate_with_temperature(messages, temp, max_tokens=80):\n",
        "    prompt = messages[0]['content']\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=temp,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            use_cache=False\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Test prompt\n",
        "creativity_prompt = \"Come up with a unique name for a coffee shop and explain the concept.\"\n",
        "messages = [{\"role\": \"user\", \"content\": creativity_prompt}]\n",
        "\n",
        "temperatures = [0.1, 0.5, 0.9, 1.2]\n",
        "\n",
        "print(f\"üéØ TEST PROMPT: '{creativity_prompt}'\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for temp in temperatures:\n",
        "    print(f\"üå°Ô∏è  TEMPERATURE: {temp}\")\n",
        "    print(\"üîÑ Generating 3 responses to show variety...\")\n",
        "\n",
        "    for i in range(3):\n",
        "        start_time = time.time()\n",
        "        response = generate_with_temperature(messages, temp)\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "        print(f\"  Response {i+1} ({duration:.2f}s): {response[:100]}...\")\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "print(\"\\nü§î ANALYSIS QUESTIONS:\")\n",
        "print(\"- Which temperature gave the most creative responses?\")\n",
        "print(\"- Which was most consistent?\")\n",
        "print(\"- Which would you use for:\")\n",
        "print(\"  ‚Ä¢ Creative writing?\")\n",
        "print(\"  ‚Ä¢ Technical documentation?\")\n",
        "print(\"  ‚Ä¢ Factual answers?\")"
      ],
      "metadata": {
        "id": "9AMNDc8mxklK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üéì ADVANCED PROMPT ENGINEERING TECHNIQUES\n",
        "# Experiment with different prompting strategies\n",
        "\n",
        "print(\"üéì ADVANCED PROMPTING TECHNIQUES\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "base_question = \"How can a small business increase customer loyalty?\"\n",
        "\n",
        "techniques = {\n",
        "    \"Zero-Shot\": base_question,\n",
        "\n",
        "    \"Few-Shot\": \"\"\"Here are examples of business loyalty strategies:\n",
        "Example 1: Restaurant - Loyalty card with free meal after 10 visits\n",
        "Example 2: Bookstore - Monthly book club with member discounts\n",
        "Example 3: Gym - Referral bonus for bringing friends\n",
        "\n",
        "Now answer: How can a small business increase customer loyalty?\"\"\",\n",
        "\n",
        "    \"Chain-of-Thought\": \"\"\"Think step by step about how a small business can increase customer loyalty:\n",
        "\n",
        "Step 1: First, identify what makes customers loyal\n",
        "Step 2: Then, consider what small businesses can realistically implement\n",
        "Step 3: Finally, suggest specific actionable strategies\n",
        "\n",
        "How can a small business increase customer loyalty?\"\"\",\n",
        "\n",
        "    \"Role-Playing\": \"\"\"You are a successful small business consultant with 15 years of experience helping local shops and services grow their customer base. You've seen what works and what doesn't.\n",
        "\n",
        "A new small business owner asks: How can I increase customer loyalty?\"\"\",\n",
        "\n",
        "    \"Structured Output\": \"\"\"Provide strategies for small business customer loyalty in this format:\n",
        "\n",
        "IMMEDIATE ACTIONS (0-30 days):\n",
        "- [Strategy 1]: [Expected impact]\n",
        "- [Strategy 2]: [Expected impact]\n",
        "\n",
        "MEDIUM TERM (1-6 months):\n",
        "- [Strategy 1]: [Expected impact]\n",
        "- [Strategy 2]: [Expected impact]\n",
        "\n",
        "LONG TERM (6+ months):\n",
        "- [Strategy 1]: [Expected impact]\n",
        "\n",
        "How can a small business increase customer loyalty?\"\"\"\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for technique, prompt in techniques.items():\n",
        "    print(f\"üîç TECHNIQUE: {technique}\")\n",
        "    print(f\"Prompt: {prompt[:100]}...\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = generate_response(messages, max_tokens=150)\n",
        "    duration = time.time() - start_time\n",
        "\n",
        "    print(f\"‚è±Ô∏è  Time: {duration:.2f}s\")\n",
        "    print(f\"üìù Response: {response}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    results[technique] = {\"response\": response, \"time\": duration}\n",
        "\n",
        "print(\"üìä TECHNIQUE COMPARISON:\")\n",
        "print(\"=\" * 30)\n",
        "for technique, data in results.items():\n",
        "    print(f\"{technique}: {data['time']:.2f}s\")\n",
        "\n",
        "print(\"\\nüéØ YOUR EXPERIMENT:\")\n",
        "print(\"Try creating your own advanced prompt using multiple techniques!\")\n",
        "your_advanced_prompt = \"\"\"YOUR COMBINATION PROMPT HERE - Try mixing:\n",
        "- Role-playing + Structured output\n",
        "- Few-shot + Chain-of-thought\n",
        "- Your own creative combination!\"\"\"\n",
        "\n",
        "print(f\"Your prompt: {your_advanced_prompt}\")\n",
        "# Uncomment to test:\n",
        "# messages = [{\"role\": \"user\", \"content\": your_advanced_prompt}]\n",
        "# your_result = generate_response(messages)\n",
        "# print(f\"Your result: {your_result}\")"
      ],
      "metadata": {
        "id": "VIF-Bkkxx4uE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}