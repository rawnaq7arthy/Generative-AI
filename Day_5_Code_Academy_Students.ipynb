{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üåç Day 5: Document Translation with Transformers\n",
        "\n",
        "Welcome to Day 5 of the Generative AI workshop!\n",
        "\n",
        "Today we'll focus on **machine translation** ‚Äî the task of automatically converting text from one language to another using transformer models. This is one of the most impactful applications of modern NLP.\n",
        "\n",
        "We'll walk through a practical pipeline:\n",
        "- Upload a PDF file in English\n",
        "- Extract and chunk the text\n",
        "- Use a pre-trained multilingual model to translate it (e.g., to Arabic)\n",
        "- View translated output for each chunk and compare results\n",
        "\n",
        "By the end of this session, you'll be able to build a simple document translator using open-source tools.\n"
      ],
      "metadata": {
        "id": "ReHSXj9xXqPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Install Required Packages\n",
        "\n",
        "## What We'll Install:\n",
        "\n",
        "- **`PyPDF2`** - For extracting text from PDF documents\n",
        "- **`transformers`** - Hugging Face library for loading pre-trained translation models\n",
        "- **`tqdm`** - Creates progress bars to track translation progress\n",
        "- **`nltk`** - Natural Language Toolkit for text processing and BLEU score calculation\n",
        "- **`matplotlib`** - For creating visualizations of our results"
      ],
      "metadata": {
        "id": "3j6xltDiX-TT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2 transformers tqdm nltk --quiet"
      ],
      "metadata": {
        "id": "-wXgLALnX-Dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll import all the libraries we need for our machine translation pipeline. Each import serves a specific role in our workflow.\n",
        "\n",
        "## Core Libraries Explained:\n",
        "\n",
        "- **`PdfReader`** - Extracts text from PDF files (we'll use a test story instead)\n",
        "- **`AutoTokenizer`** - Converts text into tokens that neural networks can understand\n",
        "- **`AutoModelForSeq2SeqLM`** - Loads pre-trained sequence-to-sequence translation models\n",
        "- **`tqdm`** - Displays progress bars during translation processes\n",
        "- **`torch`** - PyTorch framework that powers our neural translation models"
      ],
      "metadata": {
        "id": "hy5hUQA7YEs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from tqdm import tqdm\n",
        "import torch"
      ],
      "metadata": {
        "id": "QUbQZavXX8pA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üåê Load Translation Model\n",
        "\n",
        "We'll use a pre-trained model from Hugging Face to translate English to Arabic.\n",
        "\n",
        "**Model**: `Helsinki-NLP/opus-mt-en-ar`\n",
        "- **Helsinki-NLP**: Research group specializing in machine translation\n",
        "- **opus-mt**: OPUS Machine Translation project with 1000+ language pairs\n",
        "- **en-ar**: English ‚Üí Arabic translation direction\n",
        "\n",
        "This model was trained on millions of parallel English-Arabic sentence pairs and can handle various text types, though it struggles with idioms and cultural expressions."
      ],
      "metadata": {
        "id": "JmFJh5UdRIwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained English ‚Üí Arabic translation model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-ar\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-ar\")"
      ],
      "metadata": {
        "id": "XKiHQ9aURQQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìñ Load Test Story with Translation Challenges\n",
        "\n",
        "We'll translate a story containing challenging elements that will test our machine translation system:\n",
        "\n",
        "- **Idioms** (break a leg, piece of cake) - Often translate literally instead of meaningfully\n",
        "- **Numbers and dates** (March 15th, 2024, $95,750) - Formatting and cultural conventions\n",
        "- **Proper names** (Sarah Johnson, Robert Chen III) - Should remain unchanged\n",
        "- **Technical terms and percentages** - Domain-specific vocabulary challenges\n",
        "\n",
        "This story will help us understand both the **strengths** and **limitations** of modern machine translation."
      ],
      "metadata": {
        "id": "IjOABtm_YNcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple test story with token counting\n",
        "test_story = \"\"\"On March 15th, 2024, Sarah Johnson received an email that would change her life forever. The subject line read: \"Congratulations! You've been selected for TechVenture Accelerator 2024.\"\n",
        "\n",
        "Sarah couldn't believe her eyes. After 3.5 years of working as a software engineer at DataCorp Inc., earning $95,750 annually, she was finally getting her shot at the big leagues. Her startup idea, \"SmartHome AI,\" had caught the attention of investors.\n",
        "\n",
        "\"Break a leg at the presentation!\" texted her best friend Mike at 9:30 AM.\n",
        "\n",
        "The accelerator program was no joke. Sarah had to pitch to 50+ investors, including billionaire tech mogul Robert Chen III, who was known for being tougher than nails. The stakes were high: $2.5 million in seed funding was on the line.\n",
        "\n",
        "Sarah's presentation went like this: \"Ladies and gentlemen, imagine a world where your home knows you better than you know yourself. Our AI can predict when you'll arrive home with 97.3% accuracy, adjust the temperature to exactly 72.5¬∞F, and even order groceries before you run out.\"\n",
        "\n",
        "The room was dead silent. Then Robert Chen stood up. \"That's a piece of cake for any decent ML engineer,\" he said. \"What makes you different?\"\n",
        "\n",
        "Sarah's heart sank. She felt like she was between a rock and a hard place. But then she remembered her secret weapon.\n",
        "\n",
        "\"Mr. Chen,\" she said, \"our AI doesn't just learn patterns. It understands context. When my grandmother, who's 84 years old, visits every Sunday at 2:00 PM, the system knows to prepare her favorite chamomile tea and lower the music volume. It's not just smart - it's thoughtful.\"\n",
        "\n",
        "Robert's expression softened. \"Now you're cooking with gas,\" he said. \"Tell me more.\"\n",
        "\"\"\"\n",
        "\n",
        "# Count tokens using the tokenizer\n",
        "tokens = tokenizer.encode(test_story)\n",
        "num_tokens = len(tokens)\n",
        "\n",
        "print(f\"Test story loaded: {len(test_story)} characters\")\n",
        "print(f\"Number of tokens: {num_tokens}\")\n",
        "print(f\"Average characters per token: {len(test_story)/num_tokens:.1f}\")"
      ],
      "metadata": {
        "id": "Cfdt2iv4YMcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÇÔ∏è Chunk the Text\n",
        "\n",
        "Translation models work best with short segments. We'll break the document into smaller sentence-based chunks.\n",
        "\n",
        "**Why sentence-level chunking?**\n",
        "- **Better quality**: Each sentence gets focused translation attention\n",
        "- **Easier evaluation**: Compare translations sentence by sentence\n",
        "- **No cutoffs**: Complete thoughts translate more accurately than partial ones\n",
        "- **Educational value**: We can see exactly where translation succeeds or fails"
      ],
      "metadata": {
        "id": "d0VmmtpcYaF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple sentence-based chunking for better translation quality\n",
        "import re\n",
        "\n",
        "def split_by_sentences(text):\n",
        "    # Split on sentence endings, keep the punctuation\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
        "    # Return only non-empty sentences\n",
        "    return [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "# Create sentence-level chunks instead of arbitrary length chunks\n",
        "sentence_chunks = split_by_sentences(test_story)\n",
        "\n",
        "print(f\"Created {len(sentence_chunks)} sentences\")\n",
        "# Preview first 5 sentences to see our chunking results\n",
        "for i, sentence in enumerate(sentence_chunks[:5]):\n",
        "    print(f\"Sentence {i+1}: {sentence}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "Q2T8zGogYVSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† Define Translation Function\n",
        "\n",
        "This function will translate an **English sentence into Arabic** using a pre-trained transformer model (such as `MarianMT`, `M2M100`, or similar).\n",
        "\n",
        "---\n",
        "\n",
        "#### üîç Function Description:\n",
        "\n",
        "The `translate` function performs the following steps:\n",
        "\n",
        "1. **Tokenizes** the input English text using the model's tokenizer.\n",
        "2. **Generates** an Arabic translation using the `generate()` method.\n",
        "3. **Decodes** the model output token IDs into human-readable Arabic text.\n",
        "\n",
        "---\n",
        "\n",
        "#### üõ†Ô∏è Generation Parameters Explained:\n",
        "\n",
        "- `max_new_tokens=300`: Limits the number of tokens to generate.\n",
        "- `num_beams=5`: Uses beam search with 5 beams to enhance translation quality.\n",
        "- `do_sample=True`: Enables sampling instead of greedy decoding.\n",
        "- `top_k=50`: Limits sampling to the top 50 token options at each step.\n",
        "- `top_p=0.95`: Applies nucleus sampling (only sample from the smallest set whose cumulative probability ‚â• 0.95).\n",
        "- `temperature=0.8`: Controls randomness in predictions (lower is more conservative).\n",
        "- `early_stopping=True`: Stops generation when the best output is likely reached.\n",
        "\n",
        "These parameters can be tuned for a balance between **translation accuracy** and **generation diversity**.\n"
      ],
      "metadata": {
        "id": "f2xK4i-PYv3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(text):\n",
        "    # Tokenize the input English sentence\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    # Generate Arabic translation with advanced decoding strategies\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=300,      # Limit on new tokens to generate\n",
        "        num_beams=5,             # Beam search with 5 beams for better translation\n",
        "        do_sample=True,          # Enable sampling (useful for diversity)\n",
        "        top_k=50,                # Top-k sampling\n",
        "        top_p=0.95,              # Nucleus sampling\n",
        "        temperature=0.8,         # Lower temp = more deterministic output\n",
        "        early_stopping=True      # Stop early when best translation is likely found\n",
        "    )\n",
        "\n",
        "    # Decode output token IDs into Arabic text\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "9SfVoQqyYj8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîÑ Translate All Chunks\n",
        "\n",
        "We‚Äôll translate each **text chunk** (from English to Arabic) and collect the results in a list.\n",
        "\n",
        "This is useful when working with large inputs that have been pre-split into smaller segments (e.g., paragraphs, sentences, or token chunks). Each chunk will be translated individually, and a progress bar will show the translation progress using `tqdm`.\n",
        "\n",
        "---\n",
        "\n",
        "#### üß† Why Chunking?\n",
        "\n",
        "Translation models often have a token limit (e.g., 512 tokens), so chunking helps:\n",
        "- Avoid truncation or cutoff.\n",
        "- Improve translation quality.\n",
        "- Enable batch processing and progress tracking."
      ],
      "metadata": {
        "id": "mlUf_Sb6Yy2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Translate each chunk and show progress bar\n",
        "translations = [translate(chunk) for chunk in tqdm(sentence_chunks, desc=\"Translating chunks\")]"
      ],
      "metadata": {
        "id": "VGNEfHLQYxV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìÑ Translated Document (Arabic)\n",
        "\n",
        "Let‚Äôs view the translated version of the full document. This cell prints each original English chunk alongside its Arabic translation in a readable format.\n",
        "\n",
        "---\n",
        "\n",
        "#### üìù What This Does:\n",
        "\n",
        "- Neatly formats the output for each translated chunk.\n",
        "- Labels each section to show which chunk is being printed.\n",
        "- Helps with debugging or manual inspection of translations.\n",
        "- Shows both original and translated texts for easy comparison.\n",
        "\n",
        "---\n",
        "\n",
        "#### üìå Example Output:\n",
        "\n",
        "```\n",
        "==================================================\n",
        "TRANSLATION RESULTS\n",
        "==================================================\n",
        "\n",
        "--- Chunk 1 ---\n",
        "Original: This is an example.\n",
        "Arabic: Ÿáÿ∞ÿß ŸÖÿ´ÿßŸÑ.\n",
        "------------------------------\n",
        "```\n"
      ],
      "metadata": {
        "id": "r7LcBFeiY58m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print formatted translation results for the full document\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TRANSLATION RESULTS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for i, (original, translation) in enumerate(zip(sentence_chunks, translations)):\n",
        "    print(f\"\\n--- Chunk {i+1} ---\")\n",
        "    print(f\"Original: {original}\")\n",
        "    print(f\"Arabic: {translation}\")\n",
        "    print(\"-\" * 30)\n"
      ],
      "metadata": {
        "id": "i1TukUH2Y1Ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚úèÔ∏è BLEU Score Evaluation\n",
        "\n",
        "To measure the quality of our translations, we'll use the **BLEU score** (Bilingual Evaluation Understudy).\n",
        "\n",
        "---\n",
        "\n",
        "#### üîç What is BLEU?\n",
        "\n",
        "- **BLEU** compares the machine-generated translation with a human reference.\n",
        "- It evaluates the **precision of n-gram overlaps** between the two texts.\n",
        "- **Higher BLEU scores** (closer to 1.0) mean a closer match to the reference translation.\n",
        "- BLEU is widely used for evaluating translation tasks in NLP.\n",
        "\n",
        "---\n",
        "\n",
        "#### üìå Key Points:\n",
        "\n",
        "- A perfect translation gets a BLEU score of 1.0.\n",
        "- A poor translation may get a score closer to 0.\n",
        "- BLEU is **case-sensitive** and often needs **smoothing** for short sentences.\n",
        "\n",
        "---\n",
        "\n",
        "We'll use **NLTK**'s `sentence_bleu()` method and apply a smoothing function to avoid zero scores on short sentences.\n"
      ],
      "metadata": {
        "id": "76VazReHbnjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import BLEU score tools from NLTK\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# Download required tokenizer data for NLTK (used internally by some BLEU functions)\n",
        "nltk.download(\"punkt\")\n"
      ],
      "metadata": {
        "id": "YZiqaGEiY7ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìñ Reference Arabic Translations\n",
        "\n",
        "Here we define **reference translations** for the first few English chunks.  \n",
        "These are **human-curated** Arabic translations that serve as the **ground truth** when evaluating translation quality using BLEU or other metrics.\n",
        "\n",
        "---\n",
        "\n",
        "#### üìù Why Are References Needed?\n",
        "\n",
        "- BLEU score works by comparing model-generated output with a human reference.\n",
        "- Accurate reference translations help measure how close the machine translation is to native-quality output.\n",
        "- These references usually come from professional translators or trusted bilingual speakers.\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚úÖ Notes:\n",
        "- Ensure each reference matches the corresponding English chunk in order.\n",
        "- The structure should be a list of Arabic strings (`sentence_references`), aligned index-wise with your `sentence_chunks`.\n"
      ],
      "metadata": {
        "id": "QQ4jDHB_bvhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_references = [\n",
        "   # 1\n",
        "   'ŸÅŸä 15 ŸÖÿßÿ±ÿ≥ 2024ÿå ÿ™ŸÑŸÇÿ™ ÿ≥ÿßÿ±ÿ© ÿ¨ŸàŸÜÿ≥ŸàŸÜ ÿ±ÿ≥ÿßŸÑÿ© ÿ®ÿ±ŸäÿØ ÿ•ŸÑŸÉÿ™ÿ±ŸàŸÜŸä ŸÖŸÜ ÿ¥ÿ£ŸÜŸáÿß ÿ£ŸÜ ÿ™ÿ∫Ÿäÿ± ÿ≠Ÿäÿßÿ™Ÿáÿß ÿ•ŸÑŸâ ÿßŸÑÿ£ÿ®ÿØ.',\n",
        "\n",
        "   # 2\n",
        "   'ŸÉÿßŸÜ ÿπŸÜŸàÿßŸÜ ÿßŸÑÿ±ÿ≥ÿßŸÑÿ© ŸäŸÇŸàŸÑ: \"ÿ™ŸáÿßŸÜŸäŸÜÿß!',\n",
        "\n",
        "   # 3\n",
        "   'ŸÑŸÇÿØ ÿ™ŸÖ ÿßÿÆÿ™Ÿäÿßÿ±ŸÉ ŸÑÿ®ÿ±ŸÜÿßŸÖÿ¨ TechVenture Accelerator 2024.\" ŸÑŸÖ ÿ™ÿµÿØŸÇ ÿ≥ÿßÿ±ÿ© ÿπŸäŸÜŸäŸáÿß.',\n",
        "\n",
        "   # 4\n",
        "   'ÿ®ÿπÿØ 3.5 ÿ≥ŸÜŸàÿßÿ™ ŸÖŸÜ ÿßŸÑÿπŸÖŸÑ ŸÉŸÖŸáŸÜÿØÿ≥ÿ© ÿ®ÿ±ŸÖÿ¨Ÿäÿßÿ™ ŸÅŸä ÿ¥ÿ±ŸÉÿ© DataCorp Inc.ÿå ŸàŸÉÿ≥ÿ® 95,750 ÿØŸàŸÑÿßÿ± ÿ≥ŸÜŸàŸäÿßŸãÿå ŸÉÿßŸÜÿ™ ÿ™ÿ≠ÿµŸÑ ÿ£ÿÆŸäÿ±ÿßŸã ÿπŸÑŸâ ŸÅÿ±ÿµÿ™Ÿáÿß ŸÅŸä ÿßŸÑÿØŸàÿ±Ÿä ÿßŸÑŸÉÿ®Ÿäÿ±.',\n",
        "\n",
        "   # 5\n",
        "   'ŸÅŸÉÿ±ÿ© ÿ¥ÿ±ŸÉÿ™Ÿáÿß ÿßŸÑŸÜÿßÿ¥ÿ¶ÿ© \"SmartHome AI\" ŸÇÿØ ÿ¨ÿ∞ÿ®ÿ™ ÿßŸÜÿ™ÿ®ÿßŸá ÿßŸÑŸÖÿ≥ÿ™ÿ´ŸÖÿ±ŸäŸÜ.',\n",
        "\n",
        "   # 6\n",
        "   'ÿ£ÿ±ÿ≥ŸÑ ŸÑŸáÿß ÿ£ŸÅÿ∂ŸÑ ÿ£ÿµÿØŸÇÿßÿ¶Ÿáÿß ŸÖÿßŸäŸÉ ÿ±ÿ≥ÿßŸÑÿ© ŸÜÿµŸäÿ© ŸÅŸä ÿßŸÑÿ≥ÿßÿπÿ© 9:30 ÿµÿ®ÿßÿ≠ÿßŸã ŸÇÿßÿ¶ŸÑÿßŸã: \"ÿ≠ÿ∏ÿßŸã ŸÖŸàŸÅŸÇÿßŸã ŸÅŸä ÿßŸÑÿπÿ±ÿ∂ ÿßŸÑÿ™ŸÇÿØŸäŸÖŸä!\"',\n",
        "\n",
        "   # 7\n",
        "   'ŸÑŸÖ ŸäŸÉŸÜ ÿ®ÿ±ŸÜÿßŸÖÿ¨ ÿßŸÑŸÖÿ≥ÿ±Ÿëÿπ ŸÖÿ≤ÿ≠ÿ©.',\n",
        "\n",
        "   # 8\n",
        "   'ŸÉÿßŸÜ ÿπŸÑŸâ ÿ≥ÿßÿ±ÿ© ÿ£ŸÜ ÿ™ŸÇÿØŸÖ ÿπÿ±ÿ∂Ÿáÿß ÿ£ŸÖÿßŸÖ ÿ£ŸÉÿ´ÿ± ŸÖŸÜ 50 ŸÖÿ≥ÿ™ÿ´ŸÖÿ±ÿå ÿ®ŸÖÿß ŸÅŸä ÿ∞ŸÑŸÉ ÿßŸÑŸÖŸÑŸäardŸäÿ± ŸÅŸä ŸÖÿ¨ÿßŸÑ ÿßŸÑÿ™ŸÉŸÜŸàŸÑŸàÿ¨Ÿäÿß ÿ±Ÿàÿ®ÿ±ÿ™ ÿ™ÿ¥ŸÜ ÿßŸÑÿ´ÿßŸÑÿ´ÿå ÿßŸÑÿ∞Ÿä ŸÉÿßŸÜ ŸÖÿπÿ±ŸàŸÅÿßŸã ÿ®ÿµÿ±ÿßŸÖÿ™Ÿá ÿßŸÑÿ¥ÿØŸäÿØÿ©.',\n",
        "\n",
        "   # 9\n",
        "   'ŸÉÿßŸÜÿ™ ÿßŸÑŸÖÿÆÿßÿ∑ÿ± ÿπÿßŸÑŸäÿ©: 2.5 ŸÖŸÑŸäŸàŸÜ ÿØŸàŸÑÿßÿ± ŸÖŸÜ ÿßŸÑÿ™ŸÖŸàŸäŸÑ ÿßŸÑÿ£ŸàŸÑŸä ŸÉÿßŸÜ ÿπŸÑŸâ ÿßŸÑŸÖÿ≠ŸÉ.',\n",
        "\n",
        "   # 10\n",
        "   'ÿ¨ÿßÿ° ÿπÿ±ÿ∂ ÿ≥ÿßÿ±ÿ© ÿßŸÑÿ™ŸÇÿØŸäŸÖŸä ŸÉŸÖÿß ŸäŸÑŸä: \"ÿ≥ŸäÿØÿßÿ™Ÿä Ÿàÿ≥ÿßÿØÿ™Ÿäÿå ÿ™ÿÆŸäŸÑŸàÿß ÿπÿßŸÑŸÖÿßŸã ÿ≠Ÿäÿ´ ŸÖŸÜÿ≤ŸÑŸÉŸÖ Ÿäÿπÿ±ŸÅŸÉŸÖ ÿ£ŸÅÿ∂ŸÑ ŸÖŸÖÿß ÿ™ÿπÿ±ŸÅŸàŸÜ ÿ£ŸÜŸÅÿ≥ŸÉŸÖ.',\n",
        "\n",
        "   # 11\n",
        "   'ÿ∞ŸÉÿßÿ§ŸÜÿß ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä ŸäŸÖŸÉŸÜŸá ÿßŸÑÿ™ŸÜÿ®ÿ§ ÿ®ŸÖŸàÿπÿØ ŸàÿµŸàŸÑŸÉŸÖ ŸÑŸÑŸÖŸÜÿ≤ŸÑ ÿ®ÿØŸÇÿ© 97.3%ÿå Ÿàÿ∂ÿ®ÿ∑ ÿØÿ±ÿ¨ÿ© ÿßŸÑÿ≠ÿ±ÿßÿ±ÿ© ÿ•ŸÑŸâ 72.5 ŸÅŸáÿ±ŸÜŸáÿßŸäÿ™ ÿ®ÿßŸÑÿ∂ÿ®ÿ∑ÿå Ÿàÿ≠ÿ™Ÿâ ÿ∑ŸÑÿ® ÿßŸÑÿ®ŸÇÿßŸÑÿ© ŸÇÿ®ŸÑ ŸÜŸÅÿßÿØŸáÿß.\" ÿ≥ÿßÿØ ÿßŸÑÿµŸÖÿ™ ÿßŸÑÿ™ÿßŸÖ ŸÅŸä ÿßŸÑŸÇÿßÿπÿ©.',\n",
        "\n",
        "   # 12\n",
        "   'ÿ´ŸÖ ŸàŸÇŸÅ ÿ±Ÿàÿ®ÿ±ÿ™ ÿ™ÿ¥ŸÜ.',\n",
        "\n",
        "   # 13\n",
        "   'ŸÇÿßŸÑ: \"Ÿáÿ∞ÿß ÿ£ŸÖÿ± ÿ≥ŸáŸÑ ÿ¨ÿØÿßŸã ŸÑÿ£Ÿä ŸÖŸáŸÜÿØÿ≥ ÿ™ÿπŸÑŸÖ ÿ¢ŸÑŸä ŸÖÿ≠ÿ™ÿ±ŸÖ.\"',\n",
        "\n",
        "   # 14\n",
        "   '\"ŸÖÿß ÿßŸÑÿ∞Ÿä Ÿäÿ¨ÿπŸÑŸÉ ŸÖÿÆÿ™ŸÑŸÅÿ©ÿü\" ÿßŸÜŸÇÿ®ÿ∂ ŸÇŸÑÿ® ÿ≥ÿßÿ±ÿ©.',\n",
        "\n",
        "   # 15\n",
        "   'ÿ¥ÿπÿ±ÿ™ ŸàŸÉÿ£ŸÜŸáÿß ŸÅŸä ŸÖŸàŸÇŸÅ ÿµÿπÿ® ŸÑŸÑÿ∫ÿßŸäÿ©.',\n",
        "\n",
        "   # 16\n",
        "   'ŸÑŸÉŸÜŸáÿß ÿ™ÿ∞ŸÉÿ±ÿ™ ÿ≥ŸÑÿßÿ≠Ÿáÿß ÿßŸÑÿ≥ÿ±Ÿä.',\n",
        "\n",
        "   # 17\n",
        "   '\"ÿ≥ŸäÿØ',\n",
        "\n",
        "   # 18\n",
        "   'ÿ™ÿ¥ŸÜÿå\" ŸÇÿßŸÑÿ™ÿå \"ÿ∞ŸÉÿßÿ§ŸÜÿß ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä ŸÑÿß Ÿäÿ™ÿπŸÑŸÖ ÿßŸÑÿ£ŸÜŸÖÿßÿ∑ ŸÅÿ≠ÿ≥ÿ®.',\n",
        "\n",
        "   # 19\n",
        "   'ÿ•ŸÜŸá ŸäŸÅŸáŸÖ ÿßŸÑÿ≥ŸäÿßŸÇ.',\n",
        "\n",
        "   # 20\n",
        "   'ÿπŸÜÿØŸÖÿß ÿ™ÿ≤Ÿàÿ±ŸÜŸä ÿ¨ÿØÿ™Ÿäÿå ÿßŸÑÿ®ÿßŸÑÿ∫ÿ© ŸÖŸÜ ÿßŸÑÿπŸÖÿ± 84 ÿπÿßŸÖÿßŸãÿå ŸÉŸÑ ŸäŸàŸÖ ÿ£ÿ≠ÿØ ŸÅŸä ÿßŸÑÿ≥ÿßÿπÿ© 2:00 ŸÖÿ≥ÿßÿ°Ÿãÿå Ÿäÿπÿ±ŸÅ ÿßŸÑŸÜÿ∏ÿßŸÖ ÿ£ŸÜ Ÿäÿ≠ÿ∂ÿ± ŸÑŸáÿß ÿ¥ÿßŸä ÿßŸÑÿ®ÿßÿ®ŸàŸÜÿ¨ ÿßŸÑŸÖŸÅÿ∂ŸÑ ŸàŸäÿÆŸÅÿ∂ ŸÖÿ≥ÿ™ŸàŸâ ÿßŸÑŸÖŸàÿ≥ŸäŸÇŸâ.',\n",
        "\n",
        "   # 21\n",
        "   'ÿ•ŸÜŸá ŸÑŸäÿ≥ ÿ∞ŸÉŸäÿßŸã ŸÅÿ≠ÿ≥ÿ® - ÿ®ŸÑ ŸÖÿ±ÿßÿπŸç ÿ£Ÿäÿ∂ÿßŸã.\" ÿ™ŸÑŸäŸÜ ÿ™ÿπÿ®Ÿäÿ± ÿ±Ÿàÿ®ÿ±ÿ™.',\n",
        "\n",
        "   # 22\n",
        "   'ŸÇÿßŸÑ: \"ÿßŸÑÿ¢ŸÜ ÿ£ŸÜÿ™ ÿ™ÿ™ŸÇÿØŸÖŸäŸÜ ÿ®ÿ¥ŸÉŸÑ ŸÖŸÖÿ™ÿßÿ≤.\"',\n",
        "\n",
        "   # 23\n",
        "   '\"ÿ£ÿÆÿ®ÿ±ŸäŸÜŸä ÿßŸÑŸÖÿ≤ŸäÿØ.\"'\n",
        "]"
      ],
      "metadata": {
        "id": "_kR2_1aebpwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üßÆ BLEU Score Results\n",
        "\n",
        "We'll now compute the **BLEU score** for each translation chunk and display the **average BLEU score** for all chunks.\n",
        "\n",
        "---\n",
        "\n",
        "#### üî¢ How This Works:\n",
        "\n",
        "- This compares each **model-generated translation** to its **human reference** using `sentence_bleu()` from NLTK.\n",
        "- BLEU is calculated per chunk, and then the **average BLEU score** is printed.\n",
        "\n",
        "---\n",
        "\n",
        "#### üß™ Key Implementation Details:\n",
        "\n",
        "- `SmoothingFunction().method4`: Used to avoid BLEU = 0 on short or partial outputs by applying smoothing.\n",
        "- `.split()`: Tokenizes both reference and hypothesis sentences by whitespace.\n",
        "- `min(...)`: Ensures we only compare as many pairs as exist in both reference and translation lists.\n",
        "- `bleu_scores`: A list to store all individual scores.\n",
        "- The final average is calculated and printed with 4 decimal points.\n",
        "\n",
        "---\n",
        "\n",
        "#### üìå Output Example:\n",
        "\n",
        "```\n",
        "Chunk 1 BLEU score: 0.6231\n",
        "Chunk 2 BLEU score: 0.5512\n",
        "Chunk 3 BLEU score: 0.7310\n",
        "\n",
        "‚óÜ Average BLEU score: 0.6351\n",
        "```\n"
      ],
      "metadata": {
        "id": "r0U4eg8cdOaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use smoothing method to handle short sentences\n",
        "smoother = SmoothingFunction().method4\n",
        "bleu_scores = []\n",
        "\n",
        "# Compare only available chunks in both lists\n",
        "num_to_compare = min(len(translations), len(sentence_references))\n",
        "\n",
        "print(f\"Calculating BLEU scores for {num_to_compare} chunks...\")\n",
        "\n",
        "for i in range(num_to_compare):\n",
        "    # Tokenize both reference and translation sentences\n",
        "    reference_tokens = [sentence_references[i].split()]\n",
        "    hypothesis_tokens = translations[i].split()\n",
        "\n",
        "    # Calculate BLEU score with smoothing\n",
        "    score = sentence_bleu(reference_tokens, hypothesis_tokens, smoothing_function=smoother)\n",
        "    bleu_scores.append(score)\n",
        "\n",
        "    print(f\"Chunk {i+1} BLEU score: {score:.4f}\")\n",
        "\n",
        "# Display average BLEU score\n",
        "if bleu_scores:\n",
        "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "    print(f\"\\n‚óÜ Average BLEU score: {avg_bleu:.4f}\")\n",
        "else:\n",
        "    print(\"No BLEU scores calculated\")\n"
      ],
      "metadata": {
        "id": "9SOPrMKMbyEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìä BLEU Score Visualization\n",
        "\n",
        "This cell visualizes the BLEU scores computed for each translated chunk using a bar plot.\n",
        "\n",
        "---\n",
        "\n",
        "#### üéØ Why Visualize?\n",
        "\n",
        "- Makes it easy to **compare translation quality across chunks**.\n",
        "- Highlights any **low-quality translations** that might need manual review.\n",
        "- Gives a quick glance at **translation consistency**.\n",
        "\n",
        "---\n",
        "\n",
        "#### üìà Plot Features:\n",
        "\n",
        "- Each bar represents the BLEU score for a chunk.\n",
        "- The **average BLEU score** is shown in the title.\n",
        "- Each bar is labeled with its individual BLEU value (rounded to 3 decimal places).\n",
        "- Uses a light **skyblue** color with some transparency for visual clarity.\n",
        "- Grid lines help align values across bars.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vdenGw_m0e19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if bleu_scores:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    bars = plt.bar(range(1, len(bleu_scores)+1), bleu_scores, color=\"skyblue\", alpha=0.7)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for i, bar in enumerate(bars):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                f'{bleu_scores[i]:.3f}', ha='center', va='bottom')\n",
        "\n",
        "    plt.xlabel(\"Chunk Number\")\n",
        "    plt.ylabel(\"BLEU Score\")\n",
        "    plt.title(f\"BLEU Scores for Translated Chunks (Avg: {avg_bleu:.4f})\")\n",
        "    plt.ylim(0, max(bleu_scores) * 1.2 if bleu_scores else 1)\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No data to plot\")"
      ],
      "metadata": {
        "id": "-wR9IuFJdQ6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìë Sentence-by-Sentence Comparison with BLEU Scores\n",
        "\n",
        "This section provides a **detailed comparison** for each sentence, showing:\n",
        "\n",
        "- The original English chunk\n",
        "- The **model-generated Arabic translation**\n",
        "- The **reference human translation**\n",
        "- The **BLEU score** for each pair\n",
        "\n",
        "---\n",
        "\n",
        "#### üéØ Purpose:\n",
        "\n",
        "- Helps in evaluating the **quality of each translation individually**.\n",
        "- Useful for manual error analysis and debugging.\n",
        "- Enables insight into **which sentences scored well or poorly**, and why."
      ],
      "metadata": {
        "id": "lAUN32gs051L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print visual header for the comparison\n",
        "print(\"=\" * 80)\n",
        "print(\"SENTENCE-BY-SENTENCE COMPARISON WITH BLEU SCORES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Loop through all sentence chunks and compare each\n",
        "for i in range(len(sentence_chunks)):\n",
        "    # Calculate BLEU for this sentence\n",
        "    ref = [sentence_references[i].split()]         # Tokenized reference as a list of lists\n",
        "    hyp = translations[i].split()                  # Tokenized model-generated hypothesis\n",
        "    bleu_score = sentence_bleu(ref, hyp, smoothing_function=smoother)\n",
        "\n",
        "    # Print detailed output\n",
        "    print(f\"\\n--- Sentence {i+1} (BLEU: {bleu_score:.4f}) ---\")\n",
        "    print(f\"English:    {sentence_chunks[i]}\")\n",
        "    print(f\"Generated:  {translations[i]}\")\n",
        "    print(f\"Reference:  {sentence_references[i]}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "wzHjjGuDibqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîÑ Model Comparison: Different Translation Models\n",
        "\n",
        "Let‚Äôs compare the quality of translations generated by **two different models**:\n",
        "\n",
        "- **`Helsinki-NLP/opus-mt-en-ar`**: A strong, traditional model trained specifically for English ‚Üí Arabic translation.\n",
        "- **`facebook/nllb-200-distilled-600M`**: A multilingual model from Meta‚Äôs NLLB project that supports 200+ languages.\n",
        "\n",
        "---\n",
        "\n",
        "#### üß™ Objective:\n",
        "\n",
        "We'll translate the **same English chunks** using both models and compute **BLEU scores** for each to evaluate performance.\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚öôÔ∏è Next Steps:\n",
        "\n",
        "1. Load both models and tokenizers.\n",
        "2. Define a translation function for each.\n",
        "3. Translate all chunks with both models.\n",
        "4. Compute BLEU scores and compare results.\n"
      ],
      "metadata": {
        "id": "hKjTMW2Aels8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Load the multilingual model from Meta's NLLB project\n",
        "nllb_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
        "nllb_tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n"
      ],
      "metadata": {
        "id": "ILDgbVFedv44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üåç NLLB Translation Function\n",
        "\n",
        "This function uses **Meta's NLLB-200 model** (`facebook/nllb-200-distilled-600M`) to translate text from a source language to a target language.\n",
        "\n",
        "---\n",
        "\n",
        "#### üß† How It Works:\n",
        "\n",
        "- Sets the **source language** using `src_lang` (e.g., `\"eng_Latn\"` for English).\n",
        "- Sets the **target language** using `tgt_lang` (e.g., `\"arb_Arab\"` for Arabic).\n",
        "- Uses `forced_bos_token_id` to force decoding into the specified target language.\n",
        "- Generates the translated output using the model and decodes it into readable text.\n",
        "\n",
        "---\n",
        "\n",
        "**Note**: You can translate between any of the 200+ supported languages by changing the codes.\n"
      ],
      "metadata": {
        "id": "ItT-GqVg1xU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_nllb(text, src_lang=\"eng_Latn\", tgt_lang=\"arb_Arab\"):\n",
        "    # Set the source language for the tokenizer\n",
        "    nllb_tokenizer.src_lang = src_lang\n",
        "\n",
        "    # Tokenize the input with padding/truncation\n",
        "    inputs = nllb_tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    # Convert the target language code into a token ID\n",
        "    tgt_token_id = nllb_tokenizer.convert_tokens_to_ids(tgt_lang)\n",
        "\n",
        "    # Force decoder to start generation with the target language\n",
        "    inputs[\"forced_bos_token_id\"] = tgt_token_id\n",
        "\n",
        "    # Generate translation with a token cap\n",
        "    outputs = nllb_model.generate(**inputs, max_new_tokens=200)\n",
        "\n",
        "    # Decode the generated token IDs to readable text\n",
        "    return nllb_tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "kxr3sttien8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîÅ NLLB Translation\n",
        "\n",
        "Translating all English chunks using the `translate_nllb` function and storing the results in `translations_nllb`.\n",
        "\n",
        "A progress bar is shown using `tqdm`.\n"
      ],
      "metadata": {
        "id": "VG5JUHVy2XNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translations_nllb = [translate_nllb(chunk) for chunk in tqdm(sentence_chunks, desc=\"NLLB Translation\")]"
      ],
      "metadata": {
        "id": "TS-fJdRketXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìÉ NLLB Translation Results\n",
        "\n",
        "Displaying each original English chunk alongside its Arabic translation generated by the NLLB model.\n"
      ],
      "metadata": {
        "id": "FbffTA8V2fYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print all NLLB translations alongside original chunks\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"NLLB TRANSLATION RESULTS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for i, (original, translation) in enumerate(zip(sentence_chunks, translations_nllb)):\n",
        "    print(f\"\\n--- Chunk {i+1} ---\")\n",
        "    print(f\"Original: {original}\")\n",
        "    print(f\"Arabic (NLLB): {translation}\")\n",
        "    print(\"-\" * 30)\n"
      ],
      "metadata": {
        "id": "JbGhtEqPfBh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚öñÔ∏è Compare BLEU Scores: Opus-MT vs NLLB\n",
        "\n",
        "Now we‚Äôll compare a specific sentence across:\n",
        "\n",
        "- The original English input\n",
        "- Opus-MT translation\n",
        "- NLLB translation\n",
        "- Reference translation\n",
        "\n",
        "We‚Äôll also analyze token overlap to better understand the lexical similarity between each model‚Äôs output and the reference.\n"
      ],
      "metadata": {
        "id": "AHQ3mUhuexKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === SENTENCE COMPARISON ===\n",
        "print(\"=== SENTENCE COMPARISON ===\")\n",
        "print(\"Original English:\")\n",
        "print(sentence_chunks[2])\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "print(\"Opus Translation:\")\n",
        "print(translations[2])\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "print(\"NLLB Translation:\")\n",
        "print(translations_nllb[2])\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "print(\"Reference:\")\n",
        "print(sentence_references[2])\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# === TOKEN ANALYSIS ===\n",
        "print(\"=== TOKEN ANALYSIS ===\")\n",
        "\n",
        "# Tokenize all sentences by whitespace\n",
        "opus_tokens = translations[2].split()\n",
        "nllb_tokens = translations_nllb[2].split()\n",
        "ref_tokens = sentence_references[2].split()\n",
        "\n",
        "# Print number of words\n",
        "print(f\"Opus tokens: {len(opus_tokens)} words\")\n",
        "print(f\"NLLB tokens: {len(nllb_tokens)} words\")\n",
        "print(f\"Reference tokens: {len(ref_tokens)} words\")\n",
        "\n",
        "# Calculate overlapping tokens with reference\n",
        "opus_matches = set(opus_tokens) & set(ref_tokens)\n",
        "nllb_matches = set(nllb_tokens) & set(ref_tokens)\n",
        "\n",
        "# Print overlap count and examples (up to 10 tokens)\n",
        "print(f\"\\nOpus matches: {len(opus_matches)} words: {list(opus_matches)[:10]}\")\n",
        "print(f\"NLLB matches: {len(nllb_matches)} words: {list(nllb_matches)[:10]}\")\n"
      ],
      "metadata": {
        "id": "FcOUbT1ifvp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìä BLEU Score & Sentence Comparison: Opus-MT vs NLLB\n",
        "\n",
        "For each chunk, we show:\n",
        "- The English source\n",
        "- Translations from both models\n",
        "- Reference translation\n",
        "- BLEU scores for Opus-MT and NLLB\n"
      ],
      "metadata": {
        "id": "2sDFisYZ2_Bz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# Use smoothing to avoid zero BLEU for short sequences\n",
        "smoother = SmoothingFunction().method4\n",
        "\n",
        "# Lists to store scores\n",
        "bleu_opus, bleu_nllb = [], []\n",
        "\n",
        "# Loop through all references\n",
        "for i in range(len(sentence_references)):\n",
        "    ref = [sentence_references[i].split()]         # Human reference tokens\n",
        "    h1 = translations[i].split()                   # Opus-MT tokens\n",
        "    h2 = translations_nllb[i].split()              # NLLB tokens\n",
        "\n",
        "    # Calculate BLEU scores\n",
        "    score_opus = sentence_bleu(ref, h1, smoothing_function=smoother)\n",
        "    score_nllb = sentence_bleu(ref, h2, smoothing_function=smoother)\n",
        "\n",
        "    # Store scores\n",
        "    bleu_opus.append(score_opus)\n",
        "    bleu_nllb.append(score_nllb)\n",
        "\n",
        "    # Display everything per chunk\n",
        "    print(f\"\\n=== Chunk {i+1} ===\")\n",
        "    print(f\"üîπ English:     {sentence_chunks[i]}\")\n",
        "    print(f\"üî∑ Opus-MT:     {translations[i]}\")\n",
        "    print(f\"üü° NLLB:        {translations_nllb[i]}\")\n",
        "    print(f\"‚úÖ Reference:   {sentence_references[i]}\")\n",
        "    print(f\"üìà BLEU (Opus): {score_opus:.4f}\")\n",
        "    print(f\"üìà BLEU (NLLB): {score_nllb:.4f}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print average scores at the end\n",
        "print(f\"\\nüî∑ Avg Opus-MT BLEU: {sum(bleu_opus)/len(bleu_opus):.4f}\")\n",
        "print(f\"üü° Avg NLLB BLEU:    {sum(bleu_nllb)/len(bleu_nllb):.4f}\")\n"
      ],
      "metadata": {
        "id": "BlOkihQgeurN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìä BLEU Score Bar Chart: Opus-MT vs NLLB\n",
        "\n",
        "This plot shows the BLEU scores for each sentence chunk, comparing **Opus-MT** and **NLLB** side-by-side.\n"
      ],
      "metadata": {
        "id": "ioGZsIhn3fxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# X-axis: chunk indices\n",
        "x = range(1, len(bleu_opus) + 1)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot Opus-MT bars shifted left\n",
        "plt.bar(\n",
        "    [i - 0.2 for i in x],\n",
        "    bleu_opus,\n",
        "    width=0.4,\n",
        "    label=\"Opus-MT\",\n",
        "    color=\"skyblue\"\n",
        ")\n",
        "\n",
        "# Plot NLLB bars shifted right\n",
        "plt.bar(\n",
        "    [i + 0.2 for i in x],\n",
        "    bleu_nllb,\n",
        "    width=0.4,\n",
        "    label=\"NLLB\",\n",
        "    color=\"orchid\"\n",
        ")\n",
        "\n",
        "# X-axis labels and styling\n",
        "plt.xticks(x)\n",
        "plt.xlabel(\"Chunk\")\n",
        "plt.ylabel(\"BLEU Score\")\n",
        "plt.title(\"BLEU Comparison: Opus-MT vs NLLB\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ebhJWexJezjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üéµ Bonus Task: Arabic to English Translation (Your Favorite Song)\n",
        "\n",
        "Let's flip the task! Now it's your turn to translate **from Arabic into English** using your favorite Arabic song lyrics.\n",
        "\n",
        "---\n",
        "\n",
        "#### üéØ Your Mission:\n",
        "\n",
        "1. **Choose a short Arabic song or verse** that you love (3‚Äì5 lines).\n",
        "2. **Translate it into English** using one of the following:\n",
        "   - A model (`nllb-200` or `opus-mt-ar-en`)\n",
        "   - Your own understanding\n",
        "   - GPT or any tool you like (just note the method used)\n",
        "3. **Analyze the result**:\n",
        "   - How accurate is the translation?\n",
        "   - What meaning or emotion was lost or kept?\n",
        "   - Which model did better (if you tried more than one)?\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Enjoy translating ‚Äî and bring your favorite music to life in a new language! üé§üåç\n"
      ],
      "metadata": {
        "id": "4tRP6DUxEWEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Arabic song lyrics (multi-line string)\n",
        "arabic_lyrics = \"\"\"\n",
        "ÿ™ŸéÿπŸéŸÑŸéŸëŸÇŸé ŸÇŸéŸÑÿ®Ÿä ÿ∑ŸéŸÅŸÑŸéÿ©Ÿã ÿπŸéÿ±Ÿéÿ®ŸêŸäŸéŸëÿ©Ÿã\n",
        "ÿ™ŸéŸÜŸéÿπŸÖŸè ŸÅŸä ÿßŸÑÿØŸêŸëŸäÿ®Ÿéÿßÿ¨Ÿê ŸàÿßŸÑÿ≠ŸéŸÑŸâ ŸàÿßŸÑÿ≠ŸèŸÑŸéŸÑ\n",
        "ŸÑŸéŸáŸéÿß ŸÖŸèŸÇŸÑŸéÿ©Ÿå ŸÑŸéŸà ÿ£ŸéŸÜŸéŸëŸáŸéÿß ŸÜŸéÿ∏Ÿéÿ±Ÿéÿ™ ÿ®ŸêŸáŸéÿß\n",
        "ÿ•ŸêŸÑŸâ ÿ±ŸéÿßŸáŸêÿ®Ÿç ŸÇŸéÿØ ÿµŸéÿßŸÖŸé ŸÑŸêŸÑŸëŸáŸê Ÿàÿßÿ®ÿ™ŸéŸáŸéŸÑ\n",
        "ŸÑŸéÿ£Ÿéÿµÿ®Ÿéÿ≠Ÿé ŸÖŸéŸÅÿ™ŸèŸàŸÜÿßŸã ŸÖŸèÿπŸéŸÜŸéŸëŸâ ÿ®Ÿêÿ≠Ÿèÿ®ŸêŸëŸáŸéÿß\n",
        "ŸÉŸéÿ£ŸéŸÜ ŸÑŸéŸÖ ŸäŸéÿµŸèŸÖ ŸÑŸêŸÑŸëŸáŸê ŸäŸéŸàŸÖÿßŸã ŸàŸÑŸéŸÖ ŸäŸèÿµŸéŸÑ\n",
        "\"\"\"\n",
        "\n",
        "# Translate using existing function\n",
        "english_translation = translate_nllb(arabic_lyrics, src_lang=\"arb_Arab\", tgt_lang=\"eng_Latn\")\n",
        "\n",
        "# Display result\n",
        "print(\"üé∂ Original Arabic Lyrics:\")\n",
        "print(arabic_lyrics.strip())\n",
        "print(\"\\n Translated English:\")\n",
        "print(english_translation)\n"
      ],
      "metadata": {
        "id": "r7_id4v2FUx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß≠ Next Steps: Choose Your Improvement Track\n",
        "\n",
        "Now that you've completed the base translation notebook, here are **5 optional learning tracks** to help you improve translation quality and deepen your understanding.\n",
        "\n",
        "Choose one (or more) to explore further ‚Äî each has clear goals and outcomes.\n",
        "\n",
        "---\n",
        "\n",
        "#### üìè Track 1: Context-Aware Chunking\n",
        "**Goal:** Improve translation quality by grouping related sentences.\n",
        "\n",
        "**What you'll do:**\n",
        "- Merge related chunks into paragraphs before translating.\n",
        "- Compare BLEU scores before vs. after.\n",
        "\n",
        "**What to expect:**\n",
        "‚úÖ Better handling of pronouns, idioms, and flow  \n",
        "üìâ Slightly slower inference, but more coherent output\n",
        "\n",
        "---\n",
        "\n",
        "#### üéõÔ∏è Track 2: Decoding Parameter Tuning\n",
        "**Goal:** Find the best decoding settings for your model.\n",
        "\n",
        "**What you'll do:**\n",
        "- Experiment with `top_k`, `top_p`, `temperature`, and `num_beams`.\n",
        "- Evaluate results using BLEU and your own judgment.\n",
        "\n",
        "**Suggested ranges:**\n",
        "- `top_k`      | 30‚Äì60\n",
        "- `top_p`      | 0.85‚Äì0.95\n",
        "- `temperature`| 0.7‚Äì1.0\n",
        "- `num_beams`  | 4‚Äì6\n",
        "\n",
        "**What to expect:**\n",
        "üé® More diverse or controlled outputs  \n",
        "üß† Better understanding of sampling vs. beam search\n",
        "\n",
        "---\n",
        "\n",
        "#### üß™ Track 3: Metric Upgrade\n",
        "**Goal:** Evaluate translations using more reliable metrics.\n",
        "\n",
        "**What you'll do:**\n",
        "- Use `sacrebleu`, `chrF`, or `BERTScore`.\n",
        "- Compare how each metric ranks your translations.\n",
        "\n",
        "**What to expect:**\n",
        "‚úÖ Insights into how BLEU can be misleading  \n",
        "üìà More correlation with human judgment (especially for Arabic)\n",
        "\n",
        "---\n",
        "\n",
        "#### ü§ñ Track 4: Try Bigger or Specialized Models\n",
        "**Goal:** Improve quality using larger or better-suited models.\n",
        "\n",
        "**What you'll do:**\n",
        "- Replace your current model with one of the following:\n",
        "  - `facebook/nllb-200-1.3B`\n",
        "  - `facebook/m2m100_1.2B`\n",
        "- OR test the same input using commercial APIs (DeepL, Google Translate, Azure).\n",
        "\n",
        "**What to expect:**\n",
        "üöÄ Higher BLEU scores  \n",
        "üíª Increased memory and runtime requirements  \n",
        "üí° Trade-offs between open-source and commercial solutions\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚úçÔ∏è Track 5: Post-Editing and Retraining\n",
        "**Goal:** Learn how human feedback fits into machine translation workflows.\n",
        "\n",
        "**What you'll do:**\n",
        "- Manually correct the lowest-scoring translations.\n",
        "- Optionally fine-tune a model on the improved samples.\n",
        "\n",
        "**What to expect:**\n",
        "üõ†Ô∏è Clear quality gains with minimal effort  \n",
        "üîÅ Understand real-world translation workflows  \n",
        "üìö Learn how feedback improves models\n",
        "\n",
        "---\n",
        "\n",
        "Choose a track based on your interests and curiosity ‚Äî and don‚Äôt be afraid to mix and match! üöÄ\n"
      ],
      "metadata": {
        "id": "NvoJoRBGA2Wq"
      }
    }
  ]
}