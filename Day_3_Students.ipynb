{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üì¶ Day 3: Building a RAG System (Retrieval-Augmented Generation)\n",
        "\n",
        "Welcome to the third session of the Generative AI workshop!\n",
        "\n",
        "Today we'll learn how to **build a Retrieval-Augmented Generation (RAG) pipeline** using open-source tools. You'll see how to process documents, embed them into a vector store, and query them with a language model to generate intelligent responses grounded in real content.\n",
        "\n",
        "üéØ **Objectives**\n",
        "\n",
        "- Understand the concept and benefits of Retrieval-Augmented Generation (RAG)\n",
        "- Chunk and embed a document using `sentence-transformers`\n",
        "- Store and search document vectors using `ChromaDB`\n",
        "- Query a document using a local language model (`FLAN-T5`)\n",
        "- Build and test a simple QA system over a PDF ‚Äî no API keys required!\n"
      ],
      "metadata": {
        "id": "f3KbeHseh4Xl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Step 1: Install Required Packages\n",
        "\n",
        "We'll begin by installing all the necessary Python libraries for this RAG pipeline:\n",
        "\n",
        "- `chromadb` for vector storage and retrieval\n",
        "- `PyPDF2` for extracting text from PDF documents\n",
        "- `transformers` for loading our language model (FLAN-T5)\n",
        "- `sentence-transformers` for generating text embeddings\n",
        "\n",
        "This may take a minute the first time you run it.\n"
      ],
      "metadata": {
        "id": "F48Ixn9qiuP0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdTUshycXdDe"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb PyPDF2 transformers sentence-transformers --quiet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Step 2: Import Required Libraries\n",
        "\n",
        "Now that we've installed our dependencies, let's import the necessary libraries:\n",
        "\n",
        "- `PyPDF2` to read PDF files\n",
        "- `sentence-transformers` to embed text chunks\n",
        "- `transformers` to load and run our LLM (FLAN-T5)\n",
        "- `chromadb` to store and retrieve vectorized document chunks\n",
        "- `torch` as the backend for running the language model\n"
      ],
      "metadata": {
        "id": "MLeupceli0fY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PyPDF2 import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n"
      ],
      "metadata": {
        "id": "13-zYsnBXhAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìÑ Step 3: Upload and Extract Text from a PDF\n",
        "\n",
        "We'll now upload a PDF file using Colab's file uploader and extract its text content.\n",
        "\n",
        "- This step reads each page of the PDF using `PyPDF2`\n",
        "- It joins the extracted text into a single string\n",
        "- The resulting `full_text` variable will be used for chunking and embedding in the next steps\n"
      ],
      "metadata": {
        "id": "chihMEL8i9f8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# TODO: Upload a PDF file from your computer\n",
        "uploaded = files.____()  # Hint: What method allows users to upload files?\n",
        "\n",
        "# TODO: Get the filename of the uploaded file\n",
        "filename = next(iter(____))  # Hint: What variable contains the uploaded files?\n",
        "\n",
        "# TODO: Create a PDF reader object\n",
        "reader = PdfReader(____)  # Hint: What variable contains the filename?\n",
        "\n",
        "# Extract all text from the PDF\n",
        "# TODO: Extract text from each page and join into a single string\n",
        "full_text = \"\\n\".join([page.____() for page in reader.____ if page.extract_text()])\n",
        "# Hint: What method extracts text from a page? What attribute contains all the pages?\n",
        "\n",
        "print(f\"‚úÖ PDF uploaded and processed!\")\n",
        "print(f\"üìÑ Filename: {filename}\")\n",
        "print(f\"üìù Total text length: {len(full_text)} characters\")\n",
        "\n",
        "# üí° LEARNING NOTES:\n",
        "# - This step reads each page of the PDF using PyPDF2\n",
        "# - It joins the extracted text into a single string\n",
        "# - The resulting full_text variable will be used for chunking and embedding in the next steps\n",
        "# - We filter out empty pages to avoid processing blank content"
      ],
      "metadata": {
        "id": "SCfZ7K9yZADy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üëÄ Optional: View Extracted Text\n",
        "\n",
        "Let‚Äôs preview the extracted text from the PDF to ensure it was loaded correctly.\n",
        "\n",
        "This is helpful for:\n",
        "- Verifying that the PDF contains valid text (not just images)\n",
        "- Understanding what content the model will later use for answering questions\n"
      ],
      "metadata": {
        "id": "iv2_oa4JkIHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Display the first 1000 characters of the extracted text\n",
        "print(____[:____])  # Hint: What variable contains our text and how many characters should we show?\n",
        "\n",
        "# üí° This is helpful for:\n",
        "# - Verifying that the PDF contains valid text (not just images)\n",
        "# - Understanding what content the model will later use for answering questions\n",
        "# - Checking if the text extraction worked properly"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JxQucCSSkJjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÇÔ∏è Step 4: Chunk the Text\n",
        "\n",
        "To make the text manageable for embedding and retrieval, we'll break the PDF content into smaller chunks.\n",
        "\n",
        "- This function splits the text into sentences using regular expressions\n",
        "- It groups sentences together until a character limit (e.g., 300) is reached\n",
        "- The result is a list of `chunks`, each suitable for embedding in the next step\n"
      ],
      "metadata": {
        "id": "jkvzjDBtjGMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple chunking by sentence\n",
        "import re\n",
        "\n",
        "def chunk_text(text, max_length=300):\n",
        "    # TODO: Split text into sentences using regular expressions\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', ____)  # Hint: What variable contains our extracted text?\n",
        "\n",
        "    # TODO: Initialize empty lists for chunks and current chunk\n",
        "    chunks, chunk = ____, ____  # Hint: What data structures should hold multiple chunks and one chunk? List and String\n",
        "\n",
        "    # TODO: Loop through each sentence\n",
        "    for sentence in ____:  # Hint: What variable contains our sentences?\n",
        "        # TODO: Check if adding this sentence would exceed max_length\n",
        "        if len(____) + len(____) <= ____:  # Hint: What variables represent current chunk, new sentence, and limit?\n",
        "            # TODO: Add sentence to current chunk with space\n",
        "            chunk += ____ + \" \"  # Hint: What sentence are we adding?\n",
        "        else:\n",
        "            # TODO: Save current chunk and start a new one\n",
        "            chunks.append(chunk.____())  # Hint: What method removes extra whitespace?\n",
        "            chunk = ____ + \" \"  # Hint: What sentence starts the new chunk?\n",
        "\n",
        "    # TODO: Don't forget the last chunk if it has content\n",
        "    if ____:  # Hint: What variable might have remaining content?\n",
        "        chunks.append(chunk.strip())\n",
        "\n",
        "    return ____  # Hint: What should this function return?\n",
        "\n",
        "# TODO: Apply chunking to our extracted text\n",
        "chunks = chunk_text(____)  # Hint: What variable contains our full PDF text?\n",
        "\n",
        "print(f\"‚úÖ Text chunked successfully!\")\n",
        "print(f\"üìä Total chunks created: {len(chunks)}\")\n",
        "print(f\"üìè Average chunk length: {sum(len(c) for c in chunks) / len(chunks):.0f} characters\")\n",
        "print(f\"üîç First chunk preview:\")\n",
        "print(chunks[0])\n",
        "\n",
        "# üí° LEARNING NOTES:\n",
        "# - This function splits text into sentences using regular expressions\n",
        "# - It groups sentences together until a character limit (e.g., 300) is reached\n",
        "# - The result is a list of chunks, each suitable for embedding in the next step"
      ],
      "metadata": {
        "id": "HKvHQLRBZCV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìö Optional: Inspect the Chunks\n",
        "\n",
        "Let‚Äôs inspect a few individual chunks to understand how the original text was segmented.\n",
        "\n",
        "This helps you:\n",
        "- See how the chunking logic grouped sentences together\n",
        "- Verify whether the chunks are clean and meaningful for embedding\n"
      ],
      "metadata": {
        "id": "lhBCo3Wokgee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Loop through the first 3 chunks\n",
        "for i in range(____):  # Hint: How many chunks do we want to see?\n",
        "    print(f\"--- Chunk {____} ---\")  # Hint: What variable represents the current chunk number? i + 1?\n",
        "    print(____[____])  # Hint: What list contains our chunks and what index are we at?\n",
        "    print()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YHBPN2sjkhUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß¨ Step 5: Generate Embeddings\n",
        "\n",
        "We‚Äôll now convert each text chunk into a numerical vector using a pre-trained sentence embedding model.\n",
        "\n",
        "- We're using the `all-MiniLM-L6-v2` model from `sentence-transformers`\n",
        "- These embeddings will later be stored in a vector database for retrieval\n",
        "\n",
        "Each chunk is now represented in a way that a machine learning model can understand semantically.\n"
      ],
      "metadata": {
        "id": "EUf6gKZ5jMZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Initialize a sentence transformer model for creating embeddings\n",
        "embedder = SentenceTransformer(\"____\")  # Hint: What's the model name for all-MiniLM-L6-v2?\n",
        "\n",
        "# TODO: Convert all chunks into embedding vectors\n",
        "embeddings = embedder.____(____)  # Hint: What method creates embeddings and what variable contains our chunks?\n",
        "\n",
        "print(f\"‚úÖ Embeddings created successfully!\")\n",
        "print(f\"üìä Number of embeddings: {len(embeddings)}\")\n",
        "print(f\"üìè Embedding dimension: {embeddings.shape[1]}\")\n",
        "print(f\"üîç First embedding preview (first 10 values):\")\n",
        "print(embeddings[0][:10])"
      ],
      "metadata": {
        "id": "4NjToc0uZlUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîé Optional: Inspect Embeddings\n",
        "\n",
        "Let‚Äôs take a quick look at the generated embeddings.\n",
        "\n",
        "- Each embedding is a high-dimensional vector representing the meaning of a chunk\n",
        "- These vectors are what the model uses to retrieve relevant information later\n",
        "\n",
        "Note: Embeddings are large arrays of numbers, so we‚Äôll only display the first one for illustration.\n"
      ],
      "metadata": {
        "id": "P4Wuid6ClhqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Print information about the first embedding\n",
        "print(f\"Embedding for Chunk 1 (dimension: {len(____[____])}:\")  # Hint: What array contains embeddings and what index is the first?\n",
        "print(____[____])  # Hint: What array and what index for the first embedding?"
      ],
      "metadata": {
        "id": "Rix46NfElilx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üóÇÔ∏è Step 6: Store Chunks in ChromaDB\n",
        "\n",
        "Now we‚Äôll store the chunks and their corresponding embeddings in a ChromaDB collection.\n",
        "\n",
        "- ChromaDB is an efficient local vector database\n",
        "- We create a collection named `\"test\"` (or reuse it if it already exists)\n",
        "- Each chunk is added along with its embedding and a unique ID\n",
        "\n",
        "This setup allows us to later search for relevant chunks based on user questions.\n"
      ],
      "metadata": {
        "id": "KKxyfgmtjUry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Create a ChromaDB client with anonymized telemetry disabled\n",
        "chroma_client = chromadb.Client(Settings(anonymized_telemetry=____))  # Hint: Should telemetry be enabled? True or False?\n",
        "\n",
        "# TODO: Create a collection to store our documents and embeddings\n",
        "collection = chroma_client.create_collection(name=\"____\", get_or_create=____)  # Hint: What should we name our collection and should we create if it exists?\n",
        "\n",
        "# TODO: Add documents and embeddings to the collection\n",
        "for i, (chunk, emb) in enumerate(zip(____, ____)):  # Hint: What two lists should we iterate through together?\n",
        "    collection.add(\n",
        "        documents=[____],  # Hint: What text chunk are we adding?\n",
        "        embeddings=[____.tolist()],  # Hint: What embedding (converted to list) are we adding?\n",
        "        ids=[str(____)]  # Hint: What should be the unique ID for this document?\n",
        "    )\n",
        "\n",
        "print(f\"‚úÖ Vector database created successfully!\")\n",
        "print(f\"üìä Total documents stored: {collection.count()}\")"
      ],
      "metadata": {
        "id": "tHb88axoZngR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìã Optional: Preview Stored Chunks in ChromaDB\n",
        "\n",
        "Let‚Äôs confirm that the chunks and embeddings were properly added to the ChromaDB collection.\n",
        "\n",
        "This quick check allows us to:\n",
        "- View some of the stored chunk texts\n",
        "- Ensure each one has a unique ID\n"
      ],
      "metadata": {
        "id": "C-bYPRd_lxrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Retrieve documents from the collection\n",
        "results = collection.get(include=[\"____\"])  # Hint: What type of data do we want to retrieve from the collection?\n",
        "\n",
        "# TODO: Display the first 3 documents\n",
        "for i in range(min(____, len(results[\"____\"]))):  # Hint: How many documents to show and what key contains the documents?\n",
        "    print(f\"üìÑ Chunk ID: {results['____'][____]}\")  # Hint: What key contains IDs and what index are we at?\n",
        "    print(results[\"____\"][____])  # Hint: What key contains documents and what index are we at?\n",
        "    print(\"____\" * 80)  # Hint: What character should create a separator line?"
      ],
      "metadata": {
        "id": "Qw7bKLV1lzLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü§ñ Step 7: Load the Language Model (FLAN-T5)\n",
        "\n",
        "We‚Äôll now load a lightweight instruction-tuned language model to generate answers based on retrieved context.\n",
        "\n",
        "- `google/flan-t5-base` is a small and efficient model suitable for Q&A tasks\n",
        "- We load both the tokenizer and the model using Hugging Face Transformers\n",
        "\n",
        "This model will take the retrieved document chunks and generate context-aware answers to user questions.\n"
      ],
      "metadata": {
        "id": "kYhRcE0AjZsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Load the tokenizer for the T5 model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"____\")  # Hint: What's the model name for google/flan-t5-base?\n",
        "\n",
        "# TODO: Load the T5 model for sequence-to-sequence generation\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"____\")  # Hint: Should we use the same name?"
      ],
      "metadata": {
        "id": "xqC0i6MSZtQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ùì Step 8: Define a Question-Answering Function\n",
        "\n",
        "This function allows us to query the document using natural language and receive an answer generated by the language model.\n",
        "\n",
        "Here‚Äôs how it works:\n",
        "\n",
        "- It first embeds the user's question using the same embedding model as before\n",
        "- It then queries the ChromaDB collection to retrieve the most relevant text chunks\n",
        "- These chunks are used as context in a prompt passed to the `flan-t5-base` model\n",
        "- The model generates an answer based on the context and the question\n",
        "\n",
        "You can now ask the model questions about the uploaded PDF!\n"
      ],
      "metadata": {
        "id": "JB9iTnC2jhM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_question(query):\n",
        "    # TODO: Convert the query into an embedding vector\n",
        "    query_vec = embedder.____([____])[____]  # Hint: What method creates embeddings? What should we encode? What index for first result?\n",
        "\n",
        "    # TODO: Search for similar documents in the vector database\n",
        "    results = collection.query(query_embeddings=[____.tolist()], n_results=____)  # Hint: What vector to search with? How many similar chunks to retrieve? Bonus: What if we are able to add a threshold?\n",
        "\n",
        "    # TODO: Combine retrieved documents into context\n",
        "    context = \" \".join(results[\"____\"][____])  # Hint: What key contains the retrieved documents? What index for our query results?\n",
        "\n",
        "    # TODO: Create a prompt that includes context and question\n",
        "    instruction = \"You are a helpful assistant. Use the context to answer the question.\"\n",
        "    prompt = (\n",
        "        f\"{instruction}\\n\\n\"\n",
        "        f\"Context:\\n{____}\\n\\n\"  # Hint: What variable contains our retrieved context?\n",
        "        f\"Question: {____}\\n\\n\"  # Hint: What variable contains the user's question?\n",
        "        \"Answer:\"\n",
        "    )\n",
        "\n",
        "    # Display the components before generating answer\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üîç QUERY:\")\n",
        "    print(f\"'{query}'\")\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"üìã INSTRUCTION:\")\n",
        "    print(instruction)\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"üìÑ RETRIEVED CONTEXT:\")\n",
        "    print(context)\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"ü§ñ GENERATED ANSWER:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # TODO: Tokenize the prompt for the model\n",
        "    inputs = tokenizer(____, return_tensors=\"____\")  # Hint: What should we tokenize? What tensor format does PyTorch use? (\"pt\")\n",
        "\n",
        "    # TODO: Generate an answer using the model\n",
        "    outputs = model.generate(**____, max_new_tokens=____)  # Hint: What inputs should we pass? What's a reasonable token limit for answers?\n",
        "\n",
        "    # TODO: Decode and return the generated answer\n",
        "    full_response = tokenizer.decode(____[____], skip_special_tokens=____)  # Hint: What outputs to decode? What index for first result? Should we skip special tokens?\n",
        "\n",
        "    # Extract only the answer part (after \"Answer:\")\n",
        "    if \"Answer:\" in full_response:\n",
        "        answer_start = full_response.find(\"Answer:\") + len(\"Answer:\")\n",
        "        answer = full_response[answer_start:].lstrip()  # Use lstrip() instead of strip() to preserve trailing whitespace\n",
        "    else:\n",
        "        answer = full_response.strip()  # Fallback if \"Answer:\" not found\n",
        "\n",
        "    print(answer)\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "# üí° LEARNING NOTES:\n",
        "# - This function implements the complete RAG pipeline\n",
        "# - It retrieves relevant context based on query similarity\n",
        "# - It augments the prompt with retrieved information\n",
        "# - The model generates answers using both the question and context\n",
        "# - Now includes debug output showing all components of the RAG process"
      ],
      "metadata": {
        "id": "sFYzu8AiZupm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ñ∂Ô∏è Step 9: Ask a Question!\n",
        "\n",
        "Let‚Äôs test the full pipeline by asking a question about the uploaded PDF.\n",
        "\n",
        "- This example asks the model to generate a bullet-point summary\n",
        "- You can replace the prompt with any question relevant to the document\n",
        "\n",
        "Try experimenting with different question styles to explore the model's capabilities!\n"
      ],
      "metadata": {
        "id": "yjlg0nfbjmls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Ask a question about the document content\n",
        "ask_question(\"____\")  # Hint: Write a question that would require information from your uploaded PDF\n",
        "\n",
        "# üí° Try different types of questions:\n",
        "# - Factual questions about specific content\n",
        "# - Summary requests\n",
        "# - Questions that require combining information from multiple chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0Tc5lxAZy_e",
        "outputId": "b5c3e3ce-b816-4398-ecd5-0cc248b749b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Developed Low-Resource Languag e Translation Models for Amharic and Afan Oromo , achieving a BLEU scor e of 25 in Amharic and 20 in Af an Or omo, a significant impro vement given the limited resources. Projects E-Commerce Recommendation and Search Engine ( Jiji Ethiopia) Dec 2024 ‚Ä¢ Fine-tuned a custom X\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Advanced RAG Experiments\n",
        "## For Students Who Want to Go Further!\n",
        "\n",
        "Congratulations! You've built a complete RAG system. Now it's time to become a **real RAG researcher** and explore what makes these systems work better.\n",
        "\n",
        "---\n",
        "\n",
        "## üî¨ **Choose Your Experiment Track**\n",
        "\n",
        "### üß© **Track 1: Chunking Strategy Optimization**\n",
        "**The Question**: How does text splitting affect answer quality?\n",
        "\n",
        "**Experiments to Try:**\n",
        "- **Chunk size comparison**: Test 100, 300, 500, 1000 character chunks\n",
        "- **Overlap experiments**: Add 50-100 character overlap between chunks\n",
        "- **Smart boundaries**: Split by paragraphs vs. sentences vs. fixed length\n",
        "- **Hybrid approaches**: Combine multiple splitting strategies\n",
        "\n",
        "**Success Metrics**: Answer quality, retrieval accuracy, response coherence\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **Track 2: Embedding Model Showdown**\n",
        "**The Question**: Which embedding model gives the best retrieval results?\n",
        "\n",
        "**Models to Compare:**\n",
        "- `all-MiniLM-L6-v2` (what we used - fast and small)\n",
        "- `all-mpnet-base-v2` (larger, potentially better quality)\n",
        "- `sentence-transformers/all-MiniLM-L12-v2` (larger variant)\n",
        "- Domain-specific models for your document type\n",
        "\n",
        "**Success Metrics**: Retrieval precision, answer relevance, speed comparison\n",
        "\n",
        "---\n",
        "\n",
        "### üîç **Track 3: Retrieval Strategy Enhancement**\n",
        "**The Question**: How many chunks should we retrieve and how should we rank them?\n",
        "\n",
        "**Experiments to Try:**\n",
        "- **Retrieval count**: Test 1, 3, 5, 10 retrieved chunks\n",
        "- **Similarity thresholds**: Only use chunks above 0.5, 0.7, 0.8 similarity\n",
        "- **Re-ranking**: Use different similarity metrics\n",
        "- **Context limits**: How much context can the model handle effectively?\n",
        "\n",
        "**Success Metrics**: Answer completeness, hallucination reduction, context utilization\n",
        "\n",
        "---\n",
        "\n",
        "### üìö **Track 4: Multi-Document Mastery**\n",
        "**The Question**: How well does RAG work with multiple different documents?\n",
        "\n",
        "**Experiments to Try:**\n",
        "- Upload 2-3 different PDFs and ask cross-document questions\n",
        "- Test document type mixing (PDFs + text files + web content)\n",
        "- Source attribution: Can you track which document answered what?\n",
        "- Conflicting information: How does the system handle contradictions?\n",
        "\n",
        "**Success Metrics**: Cross-document reasoning, source accuracy, conflict resolution\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö° **Track 5: Real-World Application**\n",
        "**The Question**: Can you build something actually useful?\n",
        "\n",
        "**Project Ideas:**\n",
        "- **Study Assistant**: Upload your course materials, create a personal tutor\n",
        "- **Research Helper**: Upload papers from your field, ask comparative questions\n",
        "- **Policy Bot**: Upload company/school policies, create an internal help system\n",
        "- **Personal Knowledge Base**: Upload your notes, papers, articles\n",
        "\n",
        "**Success Metrics**: Practical utility, user satisfaction, real-world accuracy\n",
        "\n",
        "---\n",
        "\n",
        "### üìä **Track 6: Evaluation & Quality Analysis**\n",
        "**The Question**: How do we measure if our RAG system is actually good?\n",
        "\n",
        "**Evaluation Methods to Build:**\n",
        "- **Answer quality rubric**: Rate responses on accuracy, relevance, completeness\n",
        "- **Retrieval evaluation**: Check if the right chunks were found\n",
        "- **Speed benchmarking**: Measure response times across configurations\n",
        "- **Hallucination detection**: Identify when the model makes things up\n",
        "\n",
        "**Success Metrics**: Systematic quality measurement, performance optimization\n",
        "\n",
        "---\n",
        "\n",
        "## üìù **Documentation Tips**\n",
        "\n",
        "As you experiment, keep track of:\n",
        "- ‚úÖ **What you tried** (specific configurations, parameters)\n",
        "- ‚úÖ **What worked** (successful approaches and why)\n",
        "- ‚úÖ **What didn't work** (failures teach us too!)\n",
        "- ‚úÖ **Surprising discoveries** (unexpected results often lead to breakthroughs)\n",
        "- ‚úÖ **Practical insights** (what would you use in a real project?)\n",
        "\n",
        "---\n",
        "\n",
        "## ü§ù **Collaboration Encouraged!**\n",
        "\n",
        "- **Team up** with classmates to tackle different tracks\n",
        "- **Share findings** - compare results across different approaches\n",
        "- **Peer review** each other's experiments\n",
        "- **Present discoveries** to the class\n",
        "\n",
        "---\n",
        "\n",
        "## üåü **Remember**\n",
        "\n",
        "> *\"The best way to understand RAG is not just to build it, but to break it, improve it, and push its boundaries.\"*\n",
        "\n",
        "**Every expert started as a curious experimenter. Every breakthrough began with someone asking \"What if...?\"**\n",
        "\n",
        "Ready to become a RAG researcher? Pick your track and start experimenting! üöÄ"
      ],
      "metadata": {
        "id": "HcfiAFsnqk50"
      }
    }
  ]
}